{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Read the source file of raw commit data\n",
    "pred_commits = pd.read_csv('C:/Users/aveli/Downloads/elasticray.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features from raw commit data\n",
    "\n",
    "pred_commits['total_changed'] = pred_commits['lines_added']+pred_commits['lines_removed']\n",
    "pred_commits['size'].loc[pred_commits['size'] == 0] = pred_commits['total_changed']\n",
    "pred_commits['ratio_changed'] = pred_commits['total_changed']/pred_commits['size']\n",
    "pred_commits['rated_complexity'] = pred_commits['ratio_changed']*pred_commits['complexity']\n",
    "pred_commits['total_dmm_size'] = pred_commits['total_changed']*pred_commits['dmm_unit_size']\n",
    "pred_commits['total_dmm_unit_complexity'] = pred_commits['total_changed']*pred_commits['dmm_unit_complexity']\n",
    "pred_commits['total_dmm_unit_interfacing'] = pred_commits['total_changed']*pred_commits['dmm_unit_interfacing']\n",
    "\n",
    "# We picked the sqrt of no_of_mod_files to reduce weightage of this feature\n",
    "pred_commits['scaled_rated_complexity'] = pred_commits['rated_complexity'] * (pred_commits['no._of_mod_files'] ** 0.5)\n",
    "\n",
    "\n",
    "pred_ml_commits = pred_commits[['hash','Author','Committer','committed_date','total_changed','rated_complexity',\n",
    "                               'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity']]\n",
    "\n",
    "# Resetting the frame's index. It is required to retain the integrity of the frame\n",
    "pred_ml_commits = pred_ml_commits.reset_index().drop(columns = 'index')\n",
    "\n",
    "# Author column needs to be dropped before converting the all the fields into numeric types\n",
    "pred_ml_commits_na = pred_ml_commits.drop(columns = ['Author','hash','Committer','committed_date'])\n",
    "\n",
    "# Converting the fields to numeric types, filling the NaNs with zeros\n",
    "pred_ml_commits_numeric = pred_ml_commits_na.apply(pd.to_numeric,errors ='coerce').fillna(0)\n",
    "\n",
    "# Adding teh Author column back\n",
    "pred_ml_commits_numeric['Author'] = pred_ml_commits['Author']\n",
    "pred_ml_commits_numeric['hash'] = pred_ml_commits['hash']\n",
    "pred_ml_commits_numeric['Committer'] = pred_ml_commits['Committer']\n",
    "pred_ml_commits_numeric['committed_date'] = pred_ml_commits['committed_date']\n",
    "pred_ml_commits_numeric_na = pred_ml_commits_numeric.drop(columns=['Author','hash','committed_date','Committer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "ml_commits_nout = pred_ml_commits_numeric\n",
    "\n",
    "# Removing the text columns\n",
    "pred_ml_commits_numeric_all = ml_commits_nout.drop(columns=['Author','hash','committed_date','Committer'])\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(pred_ml_commits_numeric_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pickled model\n",
    "filename = 'C:/Users/aveli/Downloads/finalized_model.sav'\n",
    "xboost_model = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the cluster using the model\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "predicted_clusters = xboost_model.predict(pred_ml_commits_numeric_na)\n",
    "pred_ml_commits_numeric['predicted_cluster'] = predicted_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ml_commits_numeric.to_csv('C:/Users/aveli/Downloads/elasticray_predicted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the previously saved gmm model.\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "vfilename = 'C:/Users/aveli/Downloads/gmm_cluster_model.sav'\n",
    "mix = pickle.load(open(vfilename, 'rb'))\n",
    "\n",
    "# As per our logic from https://github.com/kcramakrishna/cg/issues/10\n",
    "# We need P(x)\n",
    "cluster_frame = pd.DataFrame(data_scaled)\n",
    "\n",
    "gmm_hash_clusters = mix.predict(cluster_frame)\n",
    "gmm_centroids = mix.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the values of inverted scaling of centroids for sanity\n",
    "real_centroids = scaler.inverse_transform(gmm_centroids)\n",
    "\n",
    "# Write these to dataframe\n",
    "real_centroids_dataFrame = pd.DataFrame(real_centroids, columns=['total_changed','rated_complexity',\n",
    "                            'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity'])\n",
    "\n",
    "# Add a column for summing all coloumns (https://github.com/kcramakrishna/cg/issues/10)\n",
    "real_centroids_dataFrame['Sum_centroids'] = real_centroids_dataFrame.sum(axis = 1)\n",
    "real_centroids_dataFrame['original_cluster_labels'] = real_centroids_dataFrame.index\n",
    "real_centroids_dataFrame.to_csv('C:/Users/aveli/Downloads/totalCommits_centroids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_map={}\n",
    "for i in range(real_centroids_dataFrame.shape[0]):\n",
    "    centroid_map[real_centroids_dataFrame['original_cluster_labels'].values[i]]=real_centroids_dataFrame['Sum_centroids'].values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# To store probabilities for predicted cluster labels\n",
    "probability_for_labels = np.zeros((len(predicted_clusters),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of belonging to each cluster\n",
    "member_probs = mix.predict_proba(cluster_frame)\n",
    "for i in range(len(predicted_clusters)):\n",
    "    probability_for_labels[i] = member_probs[i,predicted_clusters[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_commits_nout['Cluster'] = predicted_clusters\n",
    "ml_commits_nout['probablities'] = probability_for_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_commits_nout['sum_centroid']=np.arange(0.0,ml_commits_nout.shape[0],1.0)\n",
    "for i in range(ml_commits_nout.shape[0]):\n",
    "    ml_commits_nout['sum_centroid'].values[i]=centroid_map[ml_commits_nout['Cluster'].values[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_commits_nout['mod_score'] = ml_commits_nout['sum_centroid'] * ml_commits_nout['probablities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_authors = {}\n",
    "for idx, word in enumerate(ml_commits_nout['Author']):\n",
    "    centroid_authors[word] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (a, b) in enumerate(zip(ml_commits_nout['Author'], ml_commits_nout['mod_score'])):\n",
    "    print(idx, a, b)\n",
    "    centroid_authors[a] = centroid_authors[a] + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_authors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
