{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DEBUG flag with values from '0' to '5'. Default is '0' which is OFF. \n",
    "# Use this cautiously - we are not validating for this\n",
    "\n",
    "DEBUG = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cuda - nvidia-smi if you have it installed\n",
    "# Add a flag with Default as False. Don't change this unless you have cuda installed.\n",
    "nvidia_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Read the source file of raw commit data\n",
    "target_repo_dir = '/home/kc/Projects/data_files/target_repo_data/'\n",
    "target_repo_raw_data_file = 'anonymised_elasticray.git.csv'\n",
    "target_repo_commits = pd.read_csv(target_repo_dir+target_repo_raw_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the features from raw commit data\n",
    "def create_ml_frame(pred_commits,ext):\n",
    "    pred_commits = pred_commits[pred_commits['file_ext']==ext]\n",
    "    \n",
    "    pred_commits['total_changed'] = pred_commits['number_lines_added']+pred_commits['number_lines_removed']\n",
    "    pred_commits['feature_total_changed'] = (pred_commits['total_changed'] ** 0.7)\n",
    "\n",
    "    pred_commits['n_functions_add_del'] = abs(pred_commits['number_functions_before'] - pred_commits['number_functions_after'])\n",
    "    pred_commits['feature_add_del_functions'] = (pred_commits['n_functions_add_del'] ** 1.3) * (pred_commits['total_changed'] ** 0.5)  \n",
    "\n",
    "    \n",
    "    pred_commits['feature_changed_functions'] = (pred_commits['number_functions_edited'] ** 1.1) * (pred_commits['total_changed'] ** 0.5)  \n",
    "  \n",
    "    \n",
    "    pred_commits['feature_dmm_size'] = pred_commits['dmm_unit_size'] * (((pred_commits['n_functions_add_del'] ** 1.3) * (pred_commits['total_changed'] ** 0.3)) + ((pred_commits['number_functions_edited'] ** 1.1) * (pred_commits['total_changed'] ** 0.3))) \n",
    "    pred_commits['feature_dmm_unit_complexity'] = pred_commits['dmm_unit_complexity'] * (((pred_commits['n_functions_add_del'] ** 1.3) * (pred_commits['total_changed'] ** 0.3)) + ((pred_commits['number_functions_edited'] ** 1.1) * (pred_commits['total_changed'] ** 0.3))) \n",
    "    pred_commits['feature_dmm_unit_interfacing'] = pred_commits['dmm_unit_interfacing'] * (((pred_commits['n_functions_add_del'] ** 1.3) * (pred_commits['total_changed'] ** 0.3)) + ((pred_commits['number_functions_edited'] ** 1.1) * (pred_commits['total_changed'] ** 0.3))) \n",
    "\n",
    "\n",
    "\n",
    "    pred_ml_commits = pred_commits[['hash','Author_encrypted','Committer_encrypted','committed_date', \\\n",
    "                                    'Email_encrypted', 'feature_total_changed',\n",
    "                                    'feature_add_del_functions', 'feature_changed_functions',\n",
    "                                    'feature_dmm_unit_complexity',\n",
    "                                    'feature_dmm_size','feature_dmm_unit_interfacing']]\n",
    "\n",
    "    # Resetting the frame's index. It is required to retain the integrity of the frame\n",
    "    pred_ml_commits = pred_ml_commits.reset_index().drop(columns = 'index')\n",
    "\n",
    "    # Author/text column needs to be dropped before converting the all the fields into numeric types\n",
    "    pred_ml_commits_na = pred_ml_commits.drop(columns = \\\n",
    "                        ['Email_encrypted', 'Author_encrypted','hash','Committer_encrypted','committed_date'])\n",
    "\n",
    "    # Converting the fields to numeric types, filling the NaNs with zeros\n",
    "    pred_ml_commits_numeric = pred_ml_commits_na.apply(pd.to_numeric,errors ='coerce').fillna(0)\n",
    "\n",
    "    # Adding teh Author/text column back\n",
    "    pred_ml_commits_all_coloumns = pred_ml_commits_numeric.copy()\n",
    "    pred_ml_commits_all_coloumns['Author_encrypted'] = pred_ml_commits['Author_encrypted']\n",
    "    pred_ml_commits_all_coloumns['hash'] = pred_ml_commits['hash']\n",
    "    pred_ml_commits_all_coloumns['Committer_encrypted'] = pred_ml_commits['Committer_encrypted']\n",
    "    pred_ml_commits_all_coloumns['committed_date'] = pred_ml_commits['committed_date']\n",
    "    pred_ml_commits_all_coloumns['Email_encrypted'] = pred_ml_commits['Email_encrypted']\n",
    "\n",
    "    return pred_ml_commits_numeric, pred_ml_commits_all_coloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using ML models from CPU training. i.e. if you do NOT have GPU\n",
    "# Run this cell only if nvidia_cuda == False\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import glob\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Get the list of file extension from the target repo data\n",
    "target_repo_file_exts = target_repo_commits['file_ext'].unique()\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Folder having the GMM pickle files\n",
    "gmm_models_folder = '/home/kc/Projects/data_files/sav_files/gmm_sav/'\n",
    "# Get the file names of saved GMM models. Get the file_ext from the file names\n",
    "a = glob.glob(gmm_models_folder+'*cpu*.sav')\n",
    "gmm_model_files = [os.path.basename(f) for f in a]\n",
    "file_ext_models = [x.split('_')[0] for x in gmm_model_files]\n",
    "\n",
    "\n",
    "# Folder having the xgboost trained classifiers\n",
    "xgboost_models_folder = '/home/kc/Projects/data_files/sav_files/xgboost_sav/'\n",
    "\n",
    "# Folder for storing results\n",
    "predictions_folder = '/home/kc/Projects/data_files/predictions/'\n",
    "\n",
    "# Remove any previous files. This is probably not needed\n",
    "# files = glob.glob(predictions_folder+'*.csv')\n",
    "# for f in files:\n",
    "#    os.remove(f)\n",
    "\n",
    "## Process the target repos mods against our trained files    \n",
    "for file_ext in target_repo_file_exts:\n",
    "    \n",
    "    if DEBUG >= 3:\n",
    "        print('Processing file extension: ', file_ext)\n",
    "        store_start = datetime.now()\n",
    "        print('starting at: ', store_start)\n",
    "    \n",
    "    # Prepare the features from raw data \n",
    "    target_repo_data_frame_numeric, target_repo_data_frame_all_coloumns = create_ml_frame(target_repo_commits, file_ext)\n",
    "    \n",
    "    ## Ensure that we have models for this file extension\n",
    "    if file_ext in file_ext_models:\n",
    "        xgboost_model_file = xgboost_models_folder+file_ext+'_cpu_xgboost_model.sav'\n",
    "        xboost_model = pickle.load(open(xgboost_model_file, 'rb'))\n",
    "        \n",
    "        # Use the xgboost model to predict the cluster\n",
    "        predicted_clusters = xboost_model.predict(target_repo_data_frame_numeric)\n",
    "        target_repo_data_frame_all_coloumns['predicted_cluster'] = predicted_clusters\n",
    "\n",
    "        ## Now use the GMM pickled models to calculate the probability of the mod belonging to predicted cluster\n",
    "        # First get the relevant GMM pickel file for this file type/extension\n",
    "        gmm_model_file = gmm_models_folder+file_ext+'_cpu_gmm_model_pickle.sav'\n",
    "        mix = pickle.load(open(gmm_model_file, 'rb'))\n",
    "        #pred_ml_commits_numeric_all = target_repo_data_frame_all_coloumns.drop(columns=['Author','hash','committed_date','Committer'])\n",
    "        \n",
    "        # Scale the data for GMM processing\n",
    "        data_scaled = scaler.fit_transform(target_repo_data_frame_numeric)\n",
    "        \n",
    "        # Put this in a pandas frame\n",
    "        cluster_frame = pd.DataFrame(data_scaled)\n",
    "        \n",
    "        # Not sure why we are doing this. I think this is redundant.\n",
    "        # gmm_hash_clusters = mix.predict(cluster_frame)\n",
    "        \n",
    "        # Get the 'real world' value of the centroids. We need these to calculate the 'score' of each mod. \n",
    "        gmm_centroids = mix.means_\n",
    "        real_centroids = scaler.inverse_transform(gmm_centroids)\n",
    "\n",
    "        # Write these to dataframe\n",
    "        real_centroids_dataFrame = pd.DataFrame(real_centroids, columns=['feature_total_changed',\n",
    "                                    'feature_add_del_functions', 'feature_changed_functions', \n",
    "                                    'feature_dmm_unit_complexity',\n",
    "                                    'feature_dmm_size','feature_dmm_unit_interfacing'])\n",
    "                                                \n",
    "        # Add a column for summing all coloumns (https://github.com/kcramakrishna/cg/issues/10)\n",
    "        # This is basically assigning a 'real world value' to each centroid i.e. cluster\n",
    "        real_centroids_dataFrame['Sum_centroids'] = real_centroids_dataFrame.sum(axis = 1)\n",
    "        real_centroids_dataFrame['original_cluster_labels'] = real_centroids_dataFrame.index\n",
    "        \n",
    "        # Now we need to map the cluster labels to the 'sum of centroids' for that cluster \n",
    "        centroid_map={}\n",
    "        for i in range(real_centroids_dataFrame.shape[0]):\n",
    "            centroid_map[real_centroids_dataFrame['original_cluster_labels'].values[i]]=real_centroids_dataFrame['Sum_centroids'].values[i]\n",
    "        \n",
    "        # Initialise a coloumn for holding the probabilities of the prediction\n",
    "        probability_for_labels = np.zeros((len(predicted_clusters),1))\n",
    "        \n",
    "        # xgboost Gave the prediction, From GMM, get the probability of this prediction\n",
    "        # Need to understand the below lines in more depth\n",
    "        member_probs = mix.predict_proba(cluster_frame)\n",
    "        for i in range(len(predicted_clusters)):\n",
    "            probability_for_labels[i] = member_probs[i,predicted_clusters[i]]\n",
    "        \n",
    "        # Add the probabilities coloumn to the data Frame\n",
    "        target_repo_data_frame_all_coloumns['probablities'] = probability_for_labels\n",
    "        \n",
    "        # Look up the Sum of Centroids for each cluster for each mod and add it to the row.\n",
    "        target_repo_data_frame_all_coloumns['sum_centroid']=np.arange(0.0,target_repo_data_frame_all_coloumns.shape[0],1.0)\n",
    "        for i in range(target_repo_data_frame_all_coloumns.shape[0]):\n",
    "            target_repo_data_frame_all_coloumns['sum_centroid'].values[i]=centroid_map[target_repo_data_frame_all_coloumns['predicted_cluster'].values[i]]\n",
    "        \n",
    "        # Finally calculate the score for each mod in the target repo\n",
    "        target_repo_data_frame_all_coloumns['mod_score'] = target_repo_data_frame_all_coloumns['sum_centroid'] * target_repo_data_frame_all_coloumns['probablities']\n",
    "        \n",
    "        # Append these results to target_predictions file\n",
    "        with open(predictions_folder+'scores_'+target_repo_raw_data_file, 'a') as predictions_file:\n",
    "            target_repo_data_frame_all_coloumns.to_csv(predictions_file, mode='a', \\\n",
    "                                                       header=predictions_file.tell()==0)\n",
    "    else:\n",
    "        target_repo_data_frame_all_coloumns['predicted_cluster'] = 'No Model found'\n",
    "        target_repo_data_frame_all_coloumns['sum_centroid'] = 0\n",
    "        target_repo_data_frame_all_coloumns['probablities'] = 0\n",
    "        target_repo_data_frame_all_coloumns['mod_score'] = 0\n",
    "        with open(predictions_folder+'cpu_scores_'+target_repo_raw_data_file, 'a') as predictions_file:\n",
    "            target_repo_data_frame_all_coloumns.to_csv(predictions_file, mode='a', \\\n",
    "                                                       header=predictions_file.tell()==0)\n",
    "    if DEBUG >=2:\n",
    "            print(predictions_folder+'scores_'+target_repo_raw_data_file)    \n",
    "    \n",
    "    if DEBUG >=3:\n",
    "            store_end = datetime.now()\n",
    "            print('processing complete: ', store_end)\n",
    "            print('time taken: ', (store_end - store_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need a GPU implementation for GMM. Not easy to find or to run. Most are old. \n",
    "# Finally found something which was working. Downloaded it and including this into the notebook.\n",
    "# Note that this code uses diagonal covariance. Full covariance would have been better but we\n",
    "#           can make do with diagonal for now.\n",
    "# https://mg.readthedocs.io/importing-local-python-modules-from-jupyter-notebooks/sys-path-in-notebook/path-notebook.html\n",
    "# https://github.com/ldeecke/gmm-torch\n",
    "# \n",
    "\n",
    "if nvidia_cuda == True:\n",
    "    import os\n",
    "    import sys\n",
    "    sys.path.insert(0, os.path.abspath('/home/kc/Projects/gpu_sklearn_tensor/gmm-torch/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions using ML models from GPU training. \n",
    "# Run this cell only if nvidia_cuda == True\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import glob\n",
    "from os.path import isfile, join\n",
    "from os import listdir\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from gmm import GaussianMixture\n",
    "\n",
    "# Setting cuda device, Not required unsless you have more than one GPU/cuda on your system\n",
    "#torch.cuda.set_device('cuda:0')\n",
    "\n",
    "# Get the list of file extension from the target repo data\n",
    "target_repo_file_exts = target_repo_commits['file_ext'].unique()\n",
    "\n",
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Folder having the GMM pickle files\n",
    "gmm_models_folder = '/home/kc/Projects/data_files/sav_files/gmm_sav/'\n",
    "# Get the file names of saved GMM models. Get the file_ext from the file names\n",
    "a = glob.glob(gmm_models_folder+'*gpu*.sav')\n",
    "gmm_model_files = [os.path.basename(f) for f in a]\n",
    "file_ext_models = [x.split('_')[0] for x in gmm_model_files]\n",
    "\n",
    "# Folder having the xgboost trained classifiers\n",
    "xgboost_models_folder = '/home/kc/Projects/data_files/sav_files/xgboost_sav/'\n",
    "\n",
    "# Folder for storing results\n",
    "predictions_folder = '/home/kc/Projects/data_files/predictions/'\n",
    "\n",
    "# Remove any previous files. This is probably not needed\n",
    "# files = glob.glob(predictions_folder+'*.csv')\n",
    "# for f in files:\n",
    "#    os.remove(f)\n",
    "\n",
    "## Process the target repos mods against our trained files    \n",
    "for file_ext in target_repo_file_exts:\n",
    "    \n",
    "    if DEBUG >= 3:\n",
    "        print('Processing file extension: ', file_ext)\n",
    "        store_start = datetime.now()\n",
    "        print('starting at: ', store_start)\n",
    "    \n",
    "    # Prepare the features from raw data \n",
    "    target_repo_data_frame_numeric, target_repo_data_frame_all_coloumns = create_ml_frame(target_repo_commits, file_ext)\n",
    "    \n",
    "    ## Ensure that we have models for this file extension\n",
    "    if file_ext in file_ext_models:\n",
    "        xgboost_model_file = xgboost_models_folder+file_ext+'_gpu_xgboost_model.sav'\n",
    "        xboost_model = pickle.load(open(xgboost_model_file, 'rb'))\n",
    "        \n",
    "        # Use the xgboost model to predict the cluster\n",
    "        predicted_clusters = xboost_model.predict(target_repo_data_frame_numeric)\n",
    "        target_repo_data_frame_all_coloumns['predicted_cluster'] = predicted_clusters\n",
    "\n",
    "        ## Now use the GMM pickled models to calculate the probability of the mod belonging to predicted cluster\n",
    "        # First get the relevant GMM pickel file for this file type/extension\n",
    "        gmm_model_file = gmm_models_folder+file_ext+'_gpu_gmm_model_pickle.sav'\n",
    "        mix = pickle.load(open(gmm_model_file, 'rb'))\n",
    "        \n",
    "        # Scale the data for GMM processing\n",
    "        data_scaled = scaler.fit_transform(target_repo_data_frame_numeric)\n",
    "        # There is some error when we have lesser than 3 rows. This is a bug which we need to fix\n",
    "        #    Until we fix this bug, we'll skip processing this file_ext if we don't have enough rows\n",
    "        if data_scaled.shape[0] <= 2:\n",
    "            continue\n",
    "            \n",
    "        # Put this in a pandas frame\n",
    "        cluster_frame = pd.DataFrame(data_scaled)\n",
    "        \n",
    "        # Get the 'real world' value of the centroids. We need these to calculate the 'score' of each mod. \n",
    "        means_gpu = mix.mu\n",
    "        means_cpu = means_gpu.to('cpu')\n",
    "        gmm_centroids = means_cpu.numpy()[0,:]\n",
    "        real_centroids = scaler.inverse_transform(gmm_centroids)\n",
    "\n",
    "        # Write these to dataframe\n",
    "        real_centroids_dataFrame = pd.DataFrame(real_centroids, columns=['feature_total_changed',\n",
    "                                    'feature_add_del_functions', 'feature_changed_functions', \n",
    "                                    'feature_dmm_unit_complexity',\n",
    "                                    'feature_dmm_size','feature_dmm_unit_interfacing'])\n",
    "                                                \n",
    "        # Add a column for summing all coloumns (https://github.com/kcramakrishna/cg/issues/10)\n",
    "        # This is basically assigning a 'real world value' to each centroid i.e. cluster\n",
    "        real_centroids_dataFrame['Sum_centroids'] = real_centroids_dataFrame.sum(axis = 1)\n",
    "        real_centroids_dataFrame['original_cluster_labels'] = real_centroids_dataFrame.index\n",
    "        \n",
    "        # Now we need to map the cluster labels to the 'sum of centroids' for that cluster \n",
    "        centroid_map={}\n",
    "        for i in range(real_centroids_dataFrame.shape[0]):\n",
    "            centroid_map[real_centroids_dataFrame['original_cluster_labels'].values[i]]=real_centroids_dataFrame['Sum_centroids'].values[i]\n",
    "        \n",
    "        # Initialise a coloumn for holding the probabilities of the prediction\n",
    "        probability_for_labels = np.zeros((len(predicted_clusters),1))\n",
    "        \n",
    "        # Our GPU-GMM algo requires data as tensors.\n",
    "        # Convert the pandas DF into a tensor and then move it to GPU to feed to GPU-GMM algo.\n",
    "        scaled_file_ext_tensor_cpu = torch.tensor(list(data_scaled))\n",
    "        scaled_file_ext_tensor = scaled_file_ext_tensor_cpu.to(device='cuda')\n",
    "\n",
    "        # xgboost Gave the prediction, From GMM, get the probability of this prediction\n",
    "        membership_tensor_gpu = mix.predict_proba(scaled_file_ext_tensor)\n",
    "        \n",
    "        # Move the predicted data back from GPU to CPU. Then convert tensor to DF/list.\n",
    "        membership_tensor_cpu = membership_tensor_gpu.to('cpu')\n",
    "        member_probs = membership_tensor_cpu.numpy()\n",
    "\n",
    "        # Create coloumn with probabilities of of the predicted cluster\n",
    "        for i in range(len(predicted_clusters)):\n",
    "            probability_for_labels[i] = member_probs[i,predicted_clusters[i]]\n",
    "\n",
    "        # Add the probabilities coloumn to the data Frame\n",
    "        target_repo_data_frame_all_coloumns['probablities'] = probability_for_labels\n",
    "\n",
    "        # Look up the Sum of Centroids for each cluster for each mod and add it to the row.\n",
    "        target_repo_data_frame_all_coloumns['sum_centroid']=np.arange(0.0,target_repo_data_frame_all_coloumns.shape[0],1.0)\n",
    "        for i in range(target_repo_data_frame_all_coloumns.shape[0]):\n",
    "            target_repo_data_frame_all_coloumns['sum_centroid'].values[i]=centroid_map[target_repo_data_frame_all_coloumns['predicted_cluster'].values[i]]\n",
    "\n",
    "        # Finally calculate the score for each mod in the target repo\n",
    "        target_repo_data_frame_all_coloumns['mod_score'] = target_repo_data_frame_all_coloumns['sum_centroid'] * target_repo_data_frame_all_coloumns['probablities']\n",
    "\n",
    "        # Append these results to target_predictions file\n",
    "        with open(predictions_folder+'gpu_scores_'+target_repo_raw_data_file, 'a') as predictions_file:\n",
    "            target_repo_data_frame_all_coloumns.to_csv(predictions_file, mode='a', \\\n",
    "                                                   header=predictions_file.tell()==0)\n",
    "    else:\n",
    "        target_repo_data_frame_all_coloumns['predicted_cluster'] = 'No Model found'\n",
    "        target_repo_data_frame_all_coloumns['sum_centroid'] = 0\n",
    "        target_repo_data_frame_all_coloumns['probablities'] = 0\n",
    "        target_repo_data_frame_all_coloumns['mod_score'] = 0\n",
    "        with open(predictions_folder+'gpu_scores_'+target_repo_raw_data_file, 'a') as predictions_file:\n",
    "            target_repo_data_frame_all_coloumns.to_csv(predictions_file, mode='a', \\\n",
    "                                                       header=predictions_file.tell()==0)\n",
    "    if DEBUG >=2:\n",
    "            print(predictions_folder+'gpu_scores_'+target_repo_raw_data_file)    \n",
    "    \n",
    "    if DEBUG >=3:\n",
    "            store_end = datetime.now()\n",
    "            print('processing complete: ', store_end)\n",
    "            print('time taken: ', (store_end - store_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "14Jan21_cuda_python_env",
   "language": "python",
   "name": "14jan21_cuda_python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
