{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DEBUG flag with values from '0' to '5'. Default is '0' which is OFF. \n",
    "# Use this cautiously - we are not validating for this\n",
    "\n",
    "DEBUG = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to find buggy commits. \n",
    "# Finding buggy commits entails using 'git blame' extensively. This takes a LOT of time. \n",
    "# We have not yet fine tuned the logic to find 'buggy' commits. We should disable this to save time.\n",
    "# Default is 'False'. \n",
    "# For now, do not set this to 'True'. \n",
    "findBuggyFlag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import git\n",
    "import os\n",
    "from git import Repo\n",
    "import sys\n",
    "\n",
    "def create_components(git_url,esurl,localdir):\n",
    "    # if no url supplied for Elastic, assume the localhost\n",
    "    if esurl =='':\n",
    "        try:\n",
    "            es = elasticsearch.Elasticsearch(['http://localhost:9200/'], maxsize=500, block=False)\n",
    "        except:\n",
    "            print('Elasticsearch not running at localhost:9200')\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        try:\n",
    "            es = elasticsearch.Elasticsearch([esurl], maxsize=500, block=False)\n",
    "        except:\n",
    "            print('Elasticsearch not running at the given URL. For default localhost, do not provide the argument')\n",
    "            sys.exit(1)\n",
    "        \n",
    "    # Get the default commit index name\n",
    "    es_index_raw = str.split(git_url,'/')[-1].split('.')[0]+'_'+'index'\n",
    "    \n",
    "    # Get the default blame index name\n",
    "    es_blame_index_raw = str.split(git_url,'/')[-1].split('.')[0]+'_'+'blame'+'_'+'index'\n",
    "    es_index = es_index_raw.lower()\n",
    "    es_blame_index = es_blame_index_raw.lower()\n",
    "        \n",
    "    # If local Repo path is not supplied, create default path in '/tmp'\n",
    "    if localdir == '':\n",
    "        if sys.platform == 'linux':\n",
    "            local_dir ='/tmp/'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "            clone_dir = '/tmp'\n",
    "        else:\n",
    "            local_dir ='C:\\\\Downloads'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "            clone_dir = 'C:\\\\Downloads'\n",
    "    else:\n",
    "        local_dir = localdir+'/'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "        clone_dir = localdir\n",
    "    \n",
    "    # Check if the local Repo already exists\n",
    "    if os.path.isdir(local_dir):\n",
    "        # Load the local Repo\n",
    "        try:\n",
    "            repo = git.Repo(local_dir)   \n",
    "        # Get the latest commit object in the local Repo\n",
    "            local_commit = repo.commit()     \n",
    "        except:\n",
    "            print('No valid Repo found at the location. \\\n",
    "                    If unsure, remove the directory and try without local dir argument')\n",
    "            sys.exit(1)\n",
    "                       # latest local commit \n",
    "        \n",
    "        # Get the latest commit object in the remote Repo\n",
    "        remote = git.remote.Remote(repo, 'origin')      # remote repo\n",
    "        info = remote.fetch()[0]                        # fetch changes\n",
    "        remote_commit = info.commit  \n",
    "        \n",
    "        # If latest commit in local and remote differ then refresh the local Repo\n",
    "        if (local_commit.hexsha == remote_commit.hexsha ):\n",
    "            print('No changes in the Repo...')\n",
    "        else:    \n",
    "            repo = git.Repo(local_dir) \n",
    "            o = repo.remotes.origin\n",
    "            o.pull()\n",
    "            # Analyse and store additional commit data\n",
    "            store_commit_data(local_dir,es,es_index,es_blame_index,local_commit.hexsha,remote_commit.hexsha )\n",
    "    else:\n",
    "        # If no local Repo exists, clone the Repo\n",
    "        try:\n",
    "            if sys.platform == 'linux':\n",
    "                git.Git(clone_dir).clone(git_url)\n",
    "            else:\n",
    "                git.Git('C:\\\\Downloads').clone(git_url)\n",
    "        except:\n",
    "            print('Not able to clone the Repo. \\\n",
    "                    If there is a non Git directory with the same name, delete it and then try')\n",
    "            sys.exit(1)\n",
    "        \n",
    "        # Delete the elastic indices, if exist\n",
    "        es.indices.delete(index=es_index, ignore=[400, 404])\n",
    "        es.indices.delete(index=es_blame_index, ignore=[400, 404])\n",
    "        \n",
    "        # Create new elastic indices\n",
    "        es.indices.create(es_index)\n",
    "        es.indices.create(es_blame_index)\n",
    "        \n",
    "        # Call the function to store the necessary commit data\n",
    "        store_commit_data(local_dir,es,es_index,es_blame_index,'None','None')\n",
    "\n",
    "    return es,es_index,es_blame_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_blame(commit, q_blamelist,eachline, local_dir, repo, npath):\n",
    "    blamelist = []\n",
    "    repo_blame = repo.blame(commit.hash,npath,eachline)\n",
    "    \n",
    "    # Git Blame of a line can produce multiple records with each record representing a past modification\n",
    "    for blame_record in repo_blame:\n",
    "        # Git Blame produces duplicate records (Don't know why). \n",
    "        # Attempt to ignore duplicated by comparting the current record with the previous record\n",
    "        # Also Git Blame produces record of the same commit hash, which can be ignored\n",
    "        if str(commit.hash) !=str(blame_record[0]): #and (str(blame_record[0]) != prev_record):\n",
    "            # Building Blame tuple for each Blame record\n",
    "            blame_doc = {'orig_hash':commit.hash,'blame_hash':str(blame_record[0]),\n",
    "                                            'file':npath}    \n",
    "            # Loading blame data into the list\n",
    "            blamelist.append(blame_doc)\n",
    "    \n",
    "    # Put the blamelist into the queue.\n",
    "    q_blamelist.put(blamelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to processes each commit\n",
    "\n",
    "import concurrent.futures\n",
    "import elasticsearch\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import queue\n",
    "import pandas as pd\n",
    "from enum import Enum\n",
    "\n",
    "def process_commit(commit, repo, es, es_index, es_blame_index, local_dir):\n",
    "\n",
    "    # Creating empty lists for carrying commit data\n",
    "    doclist = []\n",
    "    blamelist = []\n",
    "    \n",
    "    # Create queues for commit data and blame data\n",
    "    q_blamelist = queue.Queue()\n",
    "    \n",
    "    for mod in commit.modifications:    \n",
    "        commit_data = {'hash':commit.hash,'Author':commit.author.name,'Email':commit.author.email,\n",
    "                       'message':commit.msg,'authored_date':commit.author_date,\n",
    "                       'Committer':commit.committer.name,'committed_date':commit.committer_date,\n",
    "                       'number_of_branches':len(commit.branches), 'in_main_branch':commit.in_main_branch,\n",
    "                       'merge_commit?':commit.merge,\n",
    "                       'number_of_mod_files':len(commit.modifications),\n",
    "                       'file_name':mod.filename, \n",
    "                       'file_change_type_name':mod.change_type.name, \n",
    "                       'file_change_type_value':mod.change_type.value,\n",
    "                       'file_old_path':mod.old_path, 'file_new_path':mod.new_path,\n",
    "                       'number_functions_before': len(mod.methods_before), \n",
    "                       'number_functions_after': len(mod.methods),\n",
    "                       'number_functions_edited': len(mod.changed_methods), #Existing methods changed.\n",
    "                       'number_lines_added':mod.added,'number_lines_removed': mod.removed,\n",
    "                       'file_number_loc':mod.nloc, 'language_supported': mod.language_supported,                         \n",
    "                       # Can we get number of lines which are comments? \n",
    "                       #   Else,We may not need the below variable 'size'. \n",
    "                       'file_size': 0 if mod.source_code is None else len(mod.source_code.splitlines()), \n",
    "                       'dmm_unit_size':commit.dmm_unit_size,\n",
    "                       'dmm_unit_complexity':commit.dmm_unit_complexity,\n",
    "                       'dmm_unit_interfacing':commit.dmm_unit_interfacing,\n",
    "                       'file_complexity': mod.complexity,\n",
    "                       'tokens':mod.token_count # We need to get exact details.\n",
    "                      }\n",
    "        \n",
    "        # We actually need to identify and save file extension here but for now, \n",
    "        #         we are doing this in 'get_latest_commits' below. We should move that code here.\n",
    "        \n",
    "        \n",
    "        # loading each commit tuple into the list\n",
    "        doclist.append(commit_data)\n",
    "        \n",
    "        if (findBuggyFlag == True):\n",
    "            # Each modification object contains diff_parsed tag with \"added\" and \"deleted\" properties. \n",
    "            alines = mod.diff_parsed['added']\n",
    "            \n",
    "            # \"added\" property is a tuple list with line number and actual line text. \n",
    "            # List of text lines is extracted\n",
    "            addedlines = [x[1] if len(alines)>0 else 'None' for x in alines ]\n",
    "                    \n",
    "            # Each modification object contains diff_parsed tag with \"added\" and \"deleted\" properties. \n",
    "            blines = mod.diff_parsed['deleted']\n",
    "            # \"deleted\" property is a tuple list with line number and actual line text. \n",
    "            # List of text lines is extracted\n",
    "            deletedlines = [x[1] if len(blines)>0 else 'None' for x in blines]\n",
    "            \n",
    "            no_of_mod_files = len(commit.modifications)\n",
    "            lines_added = mod.added\n",
    "            count = 0\n",
    "\n",
    "            # Absolute path of the file in the cloned repo. \n",
    "            # This is required to validate that the file has not been deleted in the subsequent commits\n",
    "            newfilepath = local_dir+'/'+str(mod.new_path)\n",
    "\n",
    "            # For bug fix commits, retrieving the blame data. \n",
    "            # Using Regex on Commit messages to identify bug fix commits\n",
    "            # Have lightweight threads process each modification(mod)\n",
    "            # Remember that each modification is a single line in the commit and we can have many\n",
    "            # Since most of the work is I/O bound, processors should be mostly idle, \n",
    "            #       we can load up with a LOT of threads - set he max threads to 200 \n",
    "            if len(re.findall(r\"\\bbug\\b|\\bBug\\b|\\bFix\\b|\\bfix\\b\",commit.msg))>0 \\\n",
    "                    and os.path.isfile(newfilepath) \\\n",
    "                    and no_of_mod_files <15 and lines_added < 1000:\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=200) as executor:\n",
    "                    futures = [executor.submit(process_blame, commit, q_blamelist,eachline, \\\n",
    "                                               local_dir, repo, newfilepath) \\\n",
    "                                               for eachline in addedlines]\n",
    "\n",
    "            # Wait for all threads to complete\n",
    "            executor.shutdown(wait=True)\n",
    "     \n",
    "    # Changed the method to assert the empty queue\n",
    "    while not q_blamelist.empty():\n",
    "        blamelist.append(q_blamelist.get())    \n",
    "\n",
    "    \n",
    "    # Each thread produces a list of tuples (in this case list of one tuple)\n",
    "    # When data from queue is appended to a list, it produces list of lists instead of list of tuples\n",
    "    # It is required to flatten the list of lists into list of tuples. \n",
    "    # Bulkloading to elastic won't work otherwise\n",
    "    blamelist = [item for sublist in blamelist for item in sublist]\n",
    "    \n",
    "    # using elasticsearch.py's helper tools to bulk load into elasticsearch's commit index           \n",
    "    helpers.bulk(es,doclist,index=es_index,doc_type ='commit_data',request_timeout = 2000)\n",
    "    \n",
    "    # Since Git Blame produces duplicate data, getting only unique records\n",
    "    #blamelist_fil = [i for n, i in enumerate(blamelist) if i not in blamelist[n + 1:]]\n",
    "    blame_df = pd.DataFrame(blamelist)\n",
    "    blame_df_clean = blame_df.drop_duplicates()\n",
    "\n",
    "    blamelist_fil = []\n",
    "    \n",
    "    df_iter = blame_df_clean.iterrows()\n",
    "    for index, document in df_iter:\n",
    "        blamelist_fil.append(document.to_dict())\n",
    "\n",
    "    # using elasticsearch.py's helper tools to bulk load into elasticsearch's blame index\n",
    "    helpers.bulk(es,blamelist_fil,index=es_blame_index,doc_type ='blame',request_timeout = 2000)\n",
    "\n",
    "    # delete our lists\n",
    "    del doclist\n",
    "    del blamelist\n",
    "    del blamelist_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyse all commits (and import to elastic)\n",
    "\n",
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "from elasticsearch import helpers\n",
    "import pandas as pd\n",
    "from pydriller import RepositoryMining\n",
    "import git\n",
    "from git import Repo\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "\n",
    "def store_commit_data(local_dir,es,es_index,es_blame_index,local_commit,remote_commit):\n",
    "\n",
    "    if DEBUG >= 1:\n",
    "        store_start = datetime.now()\n",
    "        print('starting store_commit', store_start)\n",
    "    \n",
    "    repo = Repo(local_dir)\n",
    "\n",
    "    # Create Multithreading pool to use full CPU\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    \n",
    "    # If the Repo has just been cloned, the program will traverse the whole Repo\n",
    "    if(local_commit == 'None'):\n",
    "        [pool.apply_async(process_commit(commit, repo, es,es_index,es_blame_index, local_dir)) for commit in \\\n",
    "         RepositoryMining(local_dir).traverse_commits()]\n",
    "        \n",
    "    else:\n",
    "        [pool.apply_async(process_commit(commit, repo, es,es_index,es_blame_index, local_dir)) for commit in \\\n",
    "         RepositoryMining(local_dir,from_commit = local_commit, to_commit = remote_commit).traverse_commits()]\n",
    "\n",
    "    # Close Multiprocessing pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    if DEBUG >= 1:\n",
    "        store_end = datetime.now()\n",
    "        print('exiting store_commit', store_end)\n",
    "        print('time taken by store_commit', (store_end - store_start))\n",
    "    \n",
    "    # Very important to explicitly refresh the Elastic indices as they are not automatically done.\n",
    "    es.indices.refresh([es_blame_index,es_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realised that git blame and most of our code is heavily constrained by disk speed. \n",
    "#       We can create and use an in memory file system. In Linux we can use /tmp \n",
    "from pathlib import Path\n",
    "\n",
    "#Set tmpfs if you have tmpfs. Else leave this as '/tmp' /tmp is usually in RAM \n",
    "tmpfs = '/tmp'\n",
    "\n",
    "if (tmpfs == ''):\n",
    "    Home_address = str(Path.home())\n",
    "else:\n",
    "    #Set Home_address to the tmpfs \n",
    "    Home_address = tmpfs\n",
    "\n",
    "localdir = Home_address + '/cg_Repos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import re\n",
    "\n",
    "\n",
    "def get_latest_commits(es_instance,commit_index,blame_index):\n",
    "        # Assigning Elastic instance, Commit Elastic Index and Blame Elastic Index to variables\n",
    "        es = es_instance\n",
    "        es_ma_index = commit_index\n",
    "        es_bl_index = blame_index\n",
    "        # Using Elasticsearch DSL function to get the data of Commit index\n",
    "        blame_es_data = Search(using=es, index=es_bl_index)\n",
    "        # Loading data into a dictionary\n",
    "        blame_dict = [hit.to_dict() for hit in blame_es_data.scan()]\n",
    "        # Using Elasticsearch DSL function to get the data of Blame index\n",
    "        commit_es_data = Search(using=es, index=es_ma_index)\n",
    "        # Loading data into a dictionary\n",
    "        commit_dict = [hit.to_dict() for hit in commit_es_data.scan()]\n",
    "        # Creating pandas dataframe for commit data\n",
    "        commit_frame = pd.DataFrame(commit_dict)\n",
    "        # Creating pandas dataframe for blame data\n",
    "        blame_frame = pd.DataFrame(blame_dict)\n",
    "        # Getting the blame row count. If the frame is empty, it means all the records are clean\n",
    "        blame_count = blame_frame.shape[0]\n",
    "        #print(blame_frame.columns)\n",
    "        if blame_count>0:\n",
    "            blame_frame['file'] = blame_frame['file'].apply(lambda x:x.split('/')[-1])\n",
    "            # Adding a column to Blame frame indicating that the row represents a Buggy commit\n",
    "            blame_frame['type'] = 'Buggy'\n",
    "            # Combining Commit frmae with Blame frame. \n",
    "            # An additional column called 'type' gets added to the Commit frame.\n",
    "            comb_frame = pd.merge(commit_frame,blame_frame,how='left', left_on = ['hash','file_name'], \\\n",
    "                                  right_on = ['blame_hash','file'])     \n",
    "        else:\n",
    "            # If the Blame frame is empty, no need to merge.\n",
    "            comb_frame=commit_frame\n",
    "            comb_frame['type'] = 'Clean'\n",
    "            \n",
    "        # When merging happnes and 'type' column gets added to the main Commit frame, \n",
    "        # The rows that are not part of Blame frame are filled with 'Nan'.\n",
    "        # Here, all the NaNs from 'type' column are replaced with 'Clean' label.\n",
    "        # Effectively, Each commit file (one Commit can contain more than one file) is categorised as \n",
    "        #                 either Buggy or Clean.\n",
    "        comb_frame['type'] = comb_frame['type'].fillna('Clean')\n",
    "        \n",
    "        # Cleaning and retaining the required columns\n",
    "        comb_frame_refined = comb_frame[['hash','Author','Email',\n",
    "                                           'message','authored_date',\n",
    "                                           'Committer','committed_date',\n",
    "                                           'number_of_branches', 'in_main_branch',\n",
    "                                           'merge_commit?',\n",
    "                                           'number_of_mod_files',\n",
    "                                           'file_name', \n",
    "                                           'file_change_type_name', \n",
    "                                           'file_change_type_value',\n",
    "                                           'file_old_path', 'file_new_path',\n",
    "                                           'number_functions_before', \n",
    "                                           'number_functions_after',\n",
    "                                           'number_functions_edited',\n",
    "                                           'number_lines_added','number_lines_removed',\n",
    "                                           'file_number_loc', 'language_supported',                         \n",
    "                                           'file_size', \n",
    "                                           'dmm_unit_size',\n",
    "                                           'dmm_unit_complexity',\n",
    "                                           'dmm_unit_interfacing',\n",
    "                                           'file_complexity',\n",
    "                                           'tokens',\n",
    "                                           'type'\n",
    "                                        ]]\n",
    "        \n",
    "        # Create a coloumn 'file_ext' which is the file 'type'\n",
    "        comb_frame_refined['file_ext'] = comb_frame_refined['file_new_path'].\\\n",
    "                                                    apply(lambda x:pathlib.Path(str(x)).suffix).\\\n",
    "                                                    apply(lambda x:re.split(r\"[^a-zA-Z0-9\\s\\++\\_\\-]\",x)[-1])\n",
    "\n",
    "        # For files without any extension, mark 'file_ext' as \"NoExt\" \n",
    "        comb_frame_refined.file_ext = comb_frame_refined.file_ext.replace(r'^\\s*$', 'NoExt', regex=True)\n",
    "\n",
    "        # Sorting the frame by committed date\n",
    "        comb_frame_refined = comb_frame_refined.drop_duplicates().sort_values('committed_date', ascending=False)\n",
    "        \n",
    "        return comb_frame_refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from target repos for predictions \n",
    "# Processing data for Git Repo. \n",
    "p1 = create_components('git://localhost/elasticray.git','',localdir)\n",
    "\n",
    "# Getting the commit data as pandas frames\n",
    "p1_commits = get_latest_commits(p1[0],p1[1],p1[2])\n",
    "\n",
    "# Storing these to file on hard disk\n",
    "p1_commits.to_csv('/home/kc/Projects/data_files/target_repo_data/elasticray.git.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to anonymise all sensitive data to protect identities and IP.\n",
    "# Decided to use widely accepted one way hashing algo - SHA 256\n",
    "# https://www.geeksforgeeks.org/sha-in-python/\n",
    "# Users to use this code to encrypt/anonymise the data before uploading for our predictions.\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "# Read the source file of raw commit data\n",
    "target_repo_dir = '/home/kc/Projects/data_files/target_repo_data/'\n",
    "target_repo_raw_data_file = 'elasticray.git.csv'\n",
    "target_repo_commits = pd.read_csv(target_repo_dir+target_repo_raw_data_file)\n",
    "\n",
    "for i in range(len(target_repo_commits)) : \n",
    "    # Encrypt Author\n",
    "    clear_text = target_repo_commits.loc[i, 'Author']\n",
    "    hashed = hashlib.sha256(str(clear_text).encode()).hexdigest()\n",
    "    target_repo_commits.loc[i, 'Author_encrypted'] = hashed\n",
    "    \n",
    "    # Encrypt Email    \n",
    "    clear_text = target_repo_commits.loc[i, 'Email']\n",
    "    hashed = hashlib.sha256(str(clear_text).encode()).hexdigest()\n",
    "    target_repo_commits.loc[i, 'Email_encrypted'] = hashed\n",
    "    if DEBUG >=1:\n",
    "        print('hash, email: ', hashed, clear_text)\n",
    "    \n",
    "    # Encrypt Committer\n",
    "    clear_text = target_repo_commits.loc[i, 'Committer']\n",
    "    hashed = hashlib.sha256(str(clear_text).encode()).hexdigest()\n",
    "    target_repo_commits.loc[i, 'Committer_encrypted'] = hashed\n",
    "\n",
    "    # Encrypt file_name\n",
    "    clear_text = target_repo_commits.loc[i, 'file_name']\n",
    "    hashed = hashlib.sha256(str(clear_text).encode()).hexdigest()\n",
    "    target_repo_commits.loc[i, 'file_name_encrypted'] = hashed\n",
    "    \n",
    "    # Encrypt file_old_path\n",
    "    clear_text = target_repo_commits.loc[i, 'file_old_path']\n",
    "    hashed = hashlib.sha256(str(clear_text).encode()).hexdigest()\n",
    "    target_repo_commits.loc[i, 'file_old_path_encrypted'] = hashed\n",
    "\n",
    "    # Encrypt file_new_path\n",
    "    clear_text = target_repo_commits.loc[i, 'file_new_path']\n",
    "    hashed = hashlib.sha256(str(clear_text).encode()).hexdigest()\n",
    "    target_repo_commits.loc[i, 'file_new_path_encrypted'] = hashed\n",
    "    \n",
    "# Create a dictionary for email-ids. We need this later to decrypt the predicitions file.\n",
    "#      We can ignore the other encrypted fileds for now.\n",
    "email_hash_dict = {}\n",
    "for author_email in target_repo_commits['Email'].unique().tolist():\n",
    "    # First hash the email\n",
    "    hashed_email = hashlib.sha256(str(author_email).encode()).hexdigest()\n",
    "    # Now add the hash and corresponding email to dictionary \n",
    "    email_hash_dict[hashed_email] = author_email\n",
    "    \n",
    "# Pickle this dictionary and write to file for future use\n",
    "email_hash_dict_file = target_repo_dir+target_repo_raw_data_file+'.email_dict.pickle'\n",
    "email_hash_dict_file_handler = open(email_hash_dict_file, 'wb')\n",
    "pickle.dump(email_hash_dict, email_hash_dict_file_handler)\n",
    "email_hash_dict_file_handler.close()\n",
    "\n",
    "# Drop the clear text columns \n",
    "target_repo_commits.drop(columns = \\\n",
    "            ['Author', 'Email', 'Committer', 'file_name', 'file_old_path', 'file_new_path'], inplace=True)\n",
    "\n",
    "# Write it out to the file. This is the file that is to be uploaded for scoring and prediction.\n",
    "target_repo_commits.to_csv(target_repo_dir+'anonymised_'+target_repo_raw_data_file)\n",
    "if DEBUG >= 1:\n",
    "    print('File to be uploaded for predictions: ', target_repo_dir+'anonymised_'+target_repo_raw_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this block of code *AFTER* you download/receive the scores/predictions file\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "\n",
    "# Read the file to be decrypted\n",
    "predictions_dir = '/home/kc/Projects/data_files/predictions/'\n",
    "anonymised_predictions_file = 'scores_anonymised_elasticray.git.csv'\n",
    "anonymised_predictions_data = pd.read_csv(predictions_dir+anonymised_predictions_file)\n",
    "\n",
    "# Read the source file of raw commit data\n",
    "target_repo_dir = '/home/kc/Projects/data_files/target_repo_data/'\n",
    "target_repo_raw_data_file = 'elasticray.git.csv'\n",
    "target_repo_commits = pd.read_csv(target_repo_dir+target_repo_raw_data_file)\n",
    "\n",
    "# Add code here to check that shapes of these 2 data frames match.\n",
    "#\n",
    "#\n",
    "\n",
    "# Read saved dictionary file and recreate the dictionary\n",
    "email_hash_dict_file = target_repo_dir+target_repo_raw_data_file+'.email_dict.pickle'\n",
    "email_hash_dict_file_handler = open(email_hash_dict_file, 'rb')\n",
    "email_hash_dict = pickle.load(email_hash_dict_file_handler)\n",
    "\n",
    "# Put back the original values for the anonymised data\n",
    "dev_predictions_file = predictions_dir+'dev_scores_'+target_repo_raw_data_file\n",
    "\n",
    "predictions_data = anonymised_predictions_data.copy()\n",
    "# Iterate through each row to put back the emails\n",
    "for i in range(len(predictions_data)) : \n",
    "    hashed_email = anonymised_predictions_data.loc[i, 'Email_encrypted']\n",
    "    if DEBUG >=1:\n",
    "        print('hash, email: ', hashed_email, email_hash_dict.get(hashed_email))\n",
    "    predictions_data.loc[i, 'Email'] = email_hash_dict.get(hashed_email)\n",
    "\n",
    "# Write this out to a file\n",
    "predictions_data.to_csv(dev_predictions_file)\n",
    "\n",
    "if DEBUG >= 1:\n",
    "    print('File containing scores of each mod(modification): ', dev_predictions_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_python_env",
   "language": "python",
   "name": "cuda_python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
