{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import git\n",
    "import os\n",
    "from git import Repo\n",
    "import sys\n",
    "\n",
    "def create_components(git_url,esurl,localdir):\n",
    "    # if no url supplied for Elastic, assume the localhost\n",
    "    if esurl =='':\n",
    "        try:\n",
    "            es = elasticsearch.Elasticsearch(['http://localhost:9200/'], maxsize=500, block=False)\n",
    "        except:\n",
    "            print('Elasticsearch not running at localhost:9200')\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        try:\n",
    "            es = elasticsearch.Elasticsearch([esurl], maxsize=500, block=False)\n",
    "        except:\n",
    "            print('Elasticsearch not running at the given URL. For default localhost, do not provide the argument')\n",
    "            sys.exit(1)\n",
    "        \n",
    "    # Get the default commit index name\n",
    "    es_index_raw = str.split(git_url,'/')[-1].split('.')[0]+'_'+'index'\n",
    "    # Get the default blame index name\n",
    "    es_blame_index_raw = str.split(git_url,'/')[-1].split('.')[0]+'_'+'blame'+'_'+'index'\n",
    "    es_index = es_index_raw.lower()\n",
    "    es_blame_index = es_blame_index_raw.lower()\n",
    "    # Create elasticsearch instance\n",
    "    \n",
    "    # If local Repo path is not supplied, create default path in '/tmp'\n",
    "    if localdir == '':\n",
    "        if sys.platform == 'linux':\n",
    "            local_dir ='/tmp/'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "            clone_dir = '/tmp'\n",
    "        else:\n",
    "            local_dir ='C:\\\\Downloads'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "            clone_dir = 'C:\\\\Downloads'\n",
    "    else:\n",
    "        local_dir = localdir+'/'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "        clone_dir = localdir\n",
    "    # Check if the local Repo already exists\n",
    "    if os.path.isdir(local_dir):\n",
    "        # Load the local Repo\n",
    "        try:\n",
    "            repo = git.Repo(local_dir)   \n",
    "        # Get the latest commit object in the local Repo\n",
    "            local_commit = repo.commit()     \n",
    "        except:\n",
    "            print('No valid Repo found at the location. If unsure, remove the directory and try without local dir argument')\n",
    "            sys.exit(1)\n",
    "                       # latest local commit \n",
    "        \n",
    "        # Get the latest commit object in the remote Repo\n",
    "        remote = git.remote.Remote(repo, 'origin')      # remote repo\n",
    "        info = remote.fetch()[0]                        # fetch changes\n",
    "        remote_commit = info.commit  \n",
    "        \n",
    "        # If latest commit in local and remote differ then refresh the local Repo\n",
    "        if (local_commit.hexsha == remote_commit.hexsha ):\n",
    "            print('No changes in the Repo...')\n",
    "            # kc - but still run it to force processing - remove below line when we move this to production\n",
    "            #store_commit_data(local_dir,es,es_index,es_blame_index,local_commit.hexsha,remote_commit.hexsha )\n",
    "        else:    \n",
    "            repo = git.Repo(local_dir) \n",
    "            o = repo.remotes.origin\n",
    "            o.pull()\n",
    "            # Analyse and store additional commit data\n",
    "            store_commit_data(local_dir,es,es_index,es_blame_index,local_commit.hexsha,remote_commit.hexsha )\n",
    "    else:\n",
    "        # If no local Repo exists, clone the Repo\n",
    "        try:\n",
    "            if sys.platform == 'linux':\n",
    "                git.Git(clone_dir).clone(git_url)\n",
    "            else:\n",
    "                git.Git('C:\\\\Downloads').clone(git_url)\n",
    "        except:\n",
    "            print('Not able to clone the Repo. If there is a non Git directory with the  same name, delete it and then try')\n",
    "            sys.exit(1)\n",
    "        # Delete the elastic indices, if exist\n",
    "        es.indices.delete(index=es_index, ignore=[400, 404])\n",
    "        es.indices.delete(index=es_blame_index, ignore=[400, 404])\n",
    "        # Create new elastic indices\n",
    "        es.indices.create(es_index)\n",
    "        es.indices.create(es_blame_index)\n",
    "        # Call the function to store the necessary commit data\n",
    "        store_commit_data(local_dir,es,es_index,es_blame_index,'None','None')\n",
    "\n",
    "    return es,es_index,es_blame_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert records to Elastic\n",
    "# This is useless for now but we can decide to use it along with\n",
    "#        elastic connection pools. \n",
    "#        We can avoid bulk import with this function\n",
    "\n",
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "\n",
    "def store_to_elastic(elastic_object, index_name, document_type, record):\n",
    "    try:\n",
    "        outcome = elastic_object.index(index=index_name, doc_type =document_type, body=record)\n",
    "    except Exception as ex:\n",
    "        print('Error in indexing data')\n",
    "        print(str(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process individual modification(mod) in commit\n",
    "\n",
    "import re\n",
    "\n",
    "def process_mod(commit, q_doclist, q_blamelist, local_dir, repo, mod):\n",
    "    \n",
    "            # Creating empty lists for carrying commit data\n",
    "            doclist = []\n",
    "            blamelist = []\n",
    "            #print('In Process Mod')\n",
    "            \n",
    "            #Building Commit tuples\n",
    "            commit_data = {'hash':commit.hash,'Author':commit.author.name,'Email':commit.author.email,\n",
    "                               'message':commit.msg,'authored_date':commit.author_date,\n",
    "                               'Committer':commit.committer.name,'committed_date':commit.committer_date,\n",
    "                               'no._of_branches':len(commit.branches),'merge_commit?':commit.merge,\n",
    "                               'no._of_mod_files':len(commit.modifications),'dmm_unit_size':commit.dmm_unit_size,\n",
    "                               'dmm_unit_complexity':commit.dmm_unit_complexity,'dmm_unit_interfacing':commit.dmm_unit_interfacing,\n",
    "                               'file_name':mod.filename, 'file_path':mod.new_path, 'complexity': mod.complexity, 'functions': len(mod.methods),\n",
    "                               'lines_added':mod.added,'lines_removed': mod.removed,'loc':mod.nloc,'size': 0 if mod.source_code is None else len(mod.source_code.splitlines()),'tokens':mod.token_count\n",
    "                                       \n",
    "                                        }\n",
    "            # loading each commit tuple into the list\n",
    "            doclist.append(commit_data)\n",
    "            #print(\"doclistlength: \"+str(len(doclist)))\n",
    "            # Each modification object contains diff_parsed tag with \"added\" and \"deleted\" properties. \n",
    "            alines = mod.diff_parsed['added']\n",
    "            # \"added\" property is a tuple list with line number and actual line text. List of text lines is extracted\n",
    "            addedlines = [x[1] if len(alines)>0 else 'None' for x in alines ]\n",
    "                    \n",
    "            # Each modification object contains diff_parsed tag with \"added\" and \"deleted\" properties. \n",
    "            blines = mod.diff_parsed['deleted']\n",
    "            # \"deleted\" property is a tuple list with line number and actual line text. List of text lines is extracted\n",
    "            deletedlines = [x[1] if len(blines)>0 else 'None' for x in blines]\n",
    "            count = 0\n",
    "                    \n",
    "            # Absolute path of the file in the cloned repo. This is required to validate that the file has not been deleted in the subsequent commits\n",
    "            newfilepath = local_dir+'/'+str(mod.new_path)\n",
    "            # For bug fix commits, retrieving the blame data. Using Regex on Commit messages to identify bug fix commits\n",
    "            if len(re.findall(r\"\\bbug\\b|\\bBug\\b|\\bFix\\b|\\bfix\\b\",commit.msg))>0 and os.path.isfile(newfilepath):# & len(addedlines)>0:\n",
    "                # Running Git Blame on each added line.\n",
    "                for eachline in addedlines:\n",
    "                    repo_blame = repo.blame(commit.hash,mod.new_path,eachline)\n",
    "                    # Git Blame of a line can produce multiple records with each record representing a past modification\n",
    "                    for blame_record in repo_blame:\n",
    "                        # Git Blame produces duplicate records (Don't know why). Attempt to ignore duplicated by comparting the current record with the previous record\n",
    "                        # Also Git Blame produces record of the same commit hash, which can be ignored\n",
    "                        #prev_record = ''\n",
    "                        if str(commit.hash) !=str(blame_record[0]): #and (str(blame_record[0]) != prev_record):\n",
    "                        # Building Blame tuple for each Blame record\n",
    "                            blame_doc = {'orig_hash':commit.hash,'blame_hash':str(blame_record[0]),\n",
    "                                            'file':mod.new_path}    \n",
    "                            # Loading blame data into the list\n",
    "                            blamelist.append(blame_doc)\n",
    "                            #print(\"blamelistlength: \"+str(len(blamelist)))\n",
    "                            #prev_record = blame_record[0]\n",
    "\n",
    "            # Send these to 'process_commit'\n",
    "            \n",
    "            #q_doclist.put(doclist)\n",
    "            \n",
    "            q_blamelist.put(blamelist)\n",
    "           \n",
    "            # delete our lists\n",
    "            del doclist\n",
    "            del blamelist\n",
    "            del blamelist_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_blame(commit, q_blamelist,eachline, local_dir, repo, npath):\n",
    "    blamelist = []\n",
    "    repo_blame = repo.blame(commit.hash,npath,eachline)\n",
    "                    # Git Blame of a line can produce multiple records with each record representing a past modification\n",
    "    for blame_record in repo_blame:\n",
    "                        # Git Blame produces duplicate records (Don't know why). Attempt to ignore duplicated by comparting the current record with the previous record\n",
    "                        # Also Git Blame produces record of the same commit hash, which can be ignored\n",
    "                        #prev_record = ''\n",
    "        if str(commit.hash) !=str(blame_record[0]): #and (str(blame_record[0]) != prev_record):\n",
    "                        # Building Blame tuple for each Blame record\n",
    "            blame_doc = {'orig_hash':commit.hash,'blame_hash':str(blame_record[0]),\n",
    "                                            'file':npath}    \n",
    "                            # Loading blame data into the list\n",
    "            blamelist.append(blame_doc)\n",
    "                            #print(\"blamelistlength: \"+str(len(blamelist)))\n",
    "                     #prev_record = blame_record[0]\n",
    "    q_blamelist.put(blamelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to processes each commit\n",
    "\n",
    "import concurrent.futures\n",
    "import elasticsearch\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import queue\n",
    "import pandas as pd\n",
    "\n",
    "def process_commit(commit, repo, es, es_index, es_blame_index, local_dir):\n",
    "\n",
    "    # Creating empty lists for carrying commit data\n",
    "    doclist = []\n",
    "    blamelist = []\n",
    "    \n",
    "    # Create queues for commit data and blame data\n",
    "    #q_doclist = queue.Queue()\n",
    "    q_blamelist = queue.Queue()\n",
    "    for mod in commit.modifications:\n",
    "        commit_data = {'hash':commit.hash,'Author':commit.author.name,'Email':commit.author.email,\n",
    "                               'message':commit.msg,'authored_date':commit.author_date,\n",
    "                               'Committer':commit.committer.name,'committed_date':commit.committer_date,\n",
    "                               'no._of_branches':len(commit.branches),'merge_commit?':commit.merge,\n",
    "                               'no._of_mod_files':len(commit.modifications),'dmm_unit_size':commit.dmm_unit_size,\n",
    "                               'dmm_unit_complexity':commit.dmm_unit_complexity,'dmm_unit_interfacing':commit.dmm_unit_interfacing,\n",
    "                               'file_name':mod.filename, 'file_path':mod.new_path, 'complexity': mod.complexity, 'functions': len(mod.methods),\n",
    "                               'lines_added':mod.added,'lines_removed': mod.removed,'loc':mod.nloc,'size': 0 if mod.source_code is None else len(mod.source_code.splitlines()),'tokens':mod.token_count\n",
    "                                       \n",
    "                                        }\n",
    "            # loading each commit tuple into the list\n",
    "        doclist.append(commit_data)\n",
    "            #print(\"doclistlength: \"+str(len(doclist)))\n",
    "            # Each modification object contains diff_parsed tag with \"added\" and \"deleted\" properties. \n",
    "        alines = mod.diff_parsed['added']\n",
    "            # \"added\" property is a tuple list with line number and actual line text. List of text lines is extracted\n",
    "        addedlines = [x[1] if len(alines)>0 else 'None' for x in alines ]\n",
    "                    \n",
    "            # Each modification object contains diff_parsed tag with \"added\" and \"deleted\" properties. \n",
    "        blines = mod.diff_parsed['deleted']\n",
    "            # \"deleted\" property is a tuple list with line number and actual line text. List of text lines is extracted\n",
    "        deletedlines = [x[1] if len(blines)>0 else 'None' for x in blines]\n",
    "        no_of_mod_files = len(commit.modifications)\n",
    "        lines_added = mod.added\n",
    "        count = 0\n",
    "                    \n",
    "            # Absolute path of the file in the cloned repo. This is required to validate that the file has not been deleted in the subsequent commits\n",
    "        newfilepath = local_dir+'/'+str(mod.new_path)\n",
    "            # For bug fix commits, retrieving the blame data. Using Regex on Commit messages to identify bug fix commits\n",
    "    # Have lightweight threads process each modification(mod)\n",
    "    # Remember that each modification is a single line in the commit and we can have many\n",
    "    # Since most of the work is I/O bound, processors should be mostly idle, \n",
    "    #       we can load up with a LOT of threads - set he max threads to 200 \n",
    "        if len(re.findall(r\"\\bbug\\b|\\bBug\\b|\\bFix\\b|\\bfix\\b\",commit.msg))>0 and os.path.isfile(newfilepath) and no_of_mod_files <15 and lines_added < 1000:# and no_of_mod_files <15 and lines_added < 1000:# & len(addedlines)>0:\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=200) as executor:\n",
    "                futures = [executor.submit(process_blame, commit, q_blamelist,eachline, local_dir, repo, newfilepath) for eachline in addedlines]\n",
    "\n",
    "        # Wait for all threads to complete\n",
    "            executor.shutdown(wait=True)\n",
    "\n",
    "\n",
    "     \n",
    "    # Vamsi - Changed the method to assert the empty queue\n",
    "    while not q_blamelist.empty():\n",
    "    #while (q_blamelist.empty == False):\n",
    "        blamelist.append(q_blamelist.get())    \n",
    "\n",
    "    \n",
    "    # Each thread produces a list of tuples (in this case list of one tuple)\n",
    "    # When data from queue is appended to a list, it produces list of lists instead of list of tuples\n",
    "    # It is required to flatten the list of lists into list of tuples. Bulkloading to elastic won't work otherwise\n",
    "    #doclist = [item for sublist in doclist for item in sublist]\n",
    "    \n",
    "    \n",
    "    # Each thread produces a list of tuples (in this case list of one tuple)\n",
    "    # When data from queue is appended to a list, it produces list of lists instead of list of tuples\n",
    "    # It is required to flatten the list of lists into list of tuples. Bulkloading to elastic won't work otherwise\n",
    "    blamelist = [item for sublist in blamelist for item in sublist]\n",
    "    #doclist = doclist[0]\n",
    "    # using elasticsearch.py's helper tools to bulk load into elasticsearch's commit index           \n",
    "    helpers.bulk(es,doclist,index=es_index,doc_type ='commit_data',request_timeout = 2000)\n",
    "    # Since Git Blame produces duplicate data, getting only unique records\n",
    "    #blamelist_fil = [i for n, i in enumerate(blamelist) if i not in blamelist[n + 1:]]\n",
    "    blame_df = pd.DataFrame(blamelist)\n",
    "    #print(blame_df.shape)\n",
    "    \n",
    "    blame_df_clean = blame_df.drop_duplicates()\n",
    "    #print(blame_df_clean.shape)\n",
    "    # using elasticsearch.py's helper tools to bulk load into elasticsearch's blame index\n",
    "    #print('blamelist_fil: '+str(len(blamelist_fil)))\n",
    "    #helpers.bulk(es,blamelist_fil,index=es_blame_index,doc_type ='blame',request_timeout = 2000)\n",
    "    blamelist_fil = []\n",
    "    df_iter = blame_df_clean.iterrows()\n",
    "    for index, document in df_iter:\n",
    "        blamelist_fil.append(document.to_dict())\n",
    "    #print(blame_df_clean.shape)        \n",
    "    #print(len(blamelist_fil))\n",
    "    #helpers.bulk(es, doc_generator(blame_df_clean,es_blame_index))\n",
    "    # using elasticsearch.py's helper tools to bulk load into elasticsearch's blame index\n",
    "    helpers.bulk(es,blamelist_fil,index=es_blame_index,doc_type ='blame',request_timeout = 2000)\n",
    "    # delete our lists\n",
    "    del doclist\n",
    "    del blamelist\n",
    "    del blamelist_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyse all commits (and import to elastic)\n",
    "\n",
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "from elasticsearch import helpers\n",
    "import pandas as pd\n",
    "from pydriller import RepositoryMining\n",
    "import git\n",
    "from git import Repo\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "\n",
    "def store_commit_data(local_dir,es,es_index,es_blame_index,local_commit,remote_commit):\n",
    "\n",
    "    store_start = datetime.now()\n",
    "    print('starting store_commit', store_start)\n",
    "    \n",
    "    repo = Repo(local_dir)\n",
    "\n",
    "    # Create Multithreading pool to use full CPU\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    \n",
    "    # If the Repo has just been cloned, the program will traverse the whole Repo\n",
    "    if(local_commit == 'None'):\n",
    "        [pool.apply_async(process_commit(commit, repo, es,es_index,es_blame_index, local_dir)) for commit in RepositoryMining(local_dir).traverse_commits()]\n",
    "        \n",
    "    else:\n",
    "        [pool.apply_async(process_commit(commit, repo, es,es_index,es_blame_index, local_dir)) for commit in RepositoryMining(local_dir,from_commit = local_commit, to_commit = remote_commit).traverse_commits()]\n",
    "\n",
    "    # Close Multiprocessing pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    store_end = datetime.now()\n",
    "    print('exiting store_commit', store_end)\n",
    "    print('time taken by store_commit', (store_end - store_start))\n",
    "    \n",
    "    # Very important to explicitly refresh the Elastic indices as they are not automatically done.\n",
    "    es.indices.refresh([es_blame_index,es_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realised that git blame and most of our code is heavily constrained by disk speed. \n",
    "#       We can create and use an in memory file system. In Linux we can use /tmp \n",
    "from pathlib import Path\n",
    "\n",
    "#Set tmpfs if you have tmpfs. Else leave this as '/tmp' /tmp is usually in RAM \n",
    "tmpfs = '/tmp'\n",
    "\n",
    "if (tmpfs == ''):\n",
    "    Home_address = str(Path.home())\n",
    "else:\n",
    "    #Set Home_address to the tmpfs \n",
    "    Home_address = tmpfs\n",
    " \n",
    "print(Home_address)\n",
    "localdir = Home_address + '/cg_Repos'\n",
    "print(localdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the base method to process and load the commit data. See \"create_components\" method for detail\n",
    "#localdir = '/home/vamsi'\n",
    "#p1 = create_components('https://github.com/R-Knowsys/elasticray.git','','')\n",
    "p2 = create_components('https://github.com/chaoss/grimoirelab.git','','')\n",
    "#p3 = create_components('https://github.com/microsoft/vscode.git','',localdir)\n",
    "#p4 = create_components('https://github.com/tensorflow/tensorflow.git','',localdir)    \n",
    "#p5 = create_components('https://github.com/facebook/react-native.git','',localdir)\n",
    "#p6 = create_components('https://github.com/kcramakrishna/Digital_Assistant_Client.git','',localdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import pandas as pd\n",
    "\n",
    "def get_user_data(es_instance,commit_index,blame_index):\n",
    "        # Assigning Elastic instance, Commit Elastic Index and Blame Elastic Index to variables\n",
    "        es = es_instance\n",
    "        es_ma_index = commit_index\n",
    "        es_bl_index = blame_index\n",
    "        # Using Elasticsearch DSL function to get the data of Commit index\n",
    "        blame_es_data = Search(using=es, index=es_bl_index)\n",
    "        # Loading data into a dictionary\n",
    "        blame_dict = [hit.to_dict() for hit in blame_es_data.scan()]\n",
    "        # Using Elasticsearch DSL function to get the data of Blame index\n",
    "        commit_es_data = Search(using=es, index=es_ma_index)\n",
    "        # Loading data into a dictionary\n",
    "        commit_dict = [hit.to_dict() for hit in commit_es_data.scan()]\n",
    "        # Creating pandas dataframe for commit data\n",
    "        commit_frame = pd.DataFrame(commit_dict)\n",
    "        # Creating pandas dataframe for blame data\n",
    "        blame_frame = pd.DataFrame(blame_dict)\n",
    "        blame_frame['file'] = blame_frame['file'].apply(lambda x:x.split('/')[-1])\n",
    "        # Getting the blame row count. If the frmae is empty, it means all the records are clean\n",
    "        blame_count = blame_frame.shape[0]\n",
    "        \n",
    "        if blame_count>0:\n",
    "            # Adding a column to Blame frame indicating that the row represents a Buggy commit\n",
    "            blame_frame['type'] = 'Buggy'\n",
    "            # Combining Commit frmae with Blame frame. An additional column called 'type' gets added to the Commit frame.\n",
    "            comb_frame = pd.merge(commit_frame,blame_frame,how='left',left_on = ['hash','file_name'],right_on = ['blame_hash','file'])\n",
    "        else:\n",
    "            # If the Blame frame is empty, no need to merge.\n",
    "            comb_frame=commit_frame\n",
    "        # When merging happnes and 'type' column gets added to the main Commit frame, The rows that are not part of Blame frame are filled with 'Nan'.\n",
    "        # Here, all the NaNs fro 'type' column are replaced with 'Clean' label.\n",
    "        # Effectively, Each commit file (one Commit can contain more than one file) is categorised as either Buggy or Clean.\n",
    "        comb_frame['type'] = comb_frame['type'].fillna('Clean')\n",
    "        \n",
    "        # Cleaning and retaining the required columns\n",
    "        comb_frame_refined = comb_frame[['hash', 'Author','Committer', 'Email', 'message',                               'committed_date', 'no._of_branches', 'merge_commit?',\n",
    "                                        'no._of_mod_files', 'dmm_unit_size', 'dmm_unit_complexity','dmm_unit_interfacing',\n",
    "                                        'file_path', 'complexity','functions', 'lines_added', 'lines_removed', 'size', 'tokens',\n",
    "                                        'type']]\n",
    "        # Commit hash raw value is very long. Cutting the value into first ten chars \n",
    "        # Assumption is that the first ten chars rednder necessary uniqueness. May need to revisit later\n",
    "        #comb_frame_refined['hash'] = comb_frame_refined['hash'].str.slice(0,10)\n",
    "\n",
    "        # Changing the type from string to Data. Used Pacific time zone. Heard the pacific coast is beautiful\n",
    "        commit_frame['committed_date'] = commit_frame['committed_date'].astype('str').apply(lambda x: pd.to_datetime(x).tz_convert('US/Pacific'))\n",
    "        \n",
    "        # Sorting the frame by committes date\n",
    "        comb_frame_refined = comb_frame_refined.sort_values('committed_date', ascending=False)\n",
    "        \n",
    "        #------ Attempting to score developers with a simple/simplistic calculation -----/\n",
    "        \n",
    "        # Replacing all NaN fields with zero\n",
    "        comb_frame_refined = comb_frame_refined.fillna(0)\n",
    "        # Adding commit count (actually commit file count) for each other column\n",
    "        comb_frame_refined['commit_count'] = comb_frame_refined.groupby('Author').hash.transform('nunique')\n",
    "        # Adding up three dmm values creating a new column\n",
    "        comb_frame_refined['riskfreescore'] = comb_frame_refined['dmm_unit_size']+comb_frame_refined['dmm_unit_complexity']+comb_frame_refined['dmm_unit_interfacing']\n",
    "        # Adding up \"Lines Added\" and \"Lines Removed\" and creating a new column\n",
    "        comb_frame_refined['lineschanged'] = comb_frame_refined['lines_added']+comb_frame_refined['lines_removed']\n",
    "        # Simple calculation to come up with a score for each row\n",
    "        comb_frame_refined['score'] = comb_frame_refined.apply(lambda x:((x.riskfreescore)*10+(x.lineschanged)+(x.complexity)*10)/10,axis =1)\n",
    "        # Filtering out the necessary columns and creating a new frame\n",
    "        dev_score = comb_frame_refined[['Author','commit_count','score','type']]\n",
    "        # Score is scaled based on the type of commit file\n",
    "        dev_score['scaled_score'] = dev_score.apply(lambda x: x.score if x.type == 'Clean' else x.score/2,axis=1)\n",
    "        # Retaining the necessary columns\n",
    "        dev_score_refined = dev_score[['Author','commit_count','scaled_score']]\n",
    "        # Calculating the Average score for the Author. A simple arithmetic mean.\n",
    "        dev_score_refined['average_score'] = dev_score_refined.groupby('Author').scaled_score.transform('mean').round()\n",
    "        dev_score_final = dev_score_refined[['Author','commit_count','average_score']].drop_duplicates().sort_values('average_score', ascending=False).reset_index().drop(columns = 'index') \n",
    "        return dev_score_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_user_data(p1[0],p1[1],p1[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import pandas as pd\n",
    "\n",
    "def get_latest_commits(es_instance,commit_index,blame_index):\n",
    "        # Assigning Elastic instance, Commit Elastic Index and Blame Elastic Index to variables\n",
    "        es = es_instance\n",
    "        es_ma_index = commit_index\n",
    "        es_bl_index = blame_index\n",
    "        # Using Elasticsearch DSL function to get the data of Commit index\n",
    "        blame_es_data = Search(using=es, index=es_bl_index)\n",
    "        # Loading data into a dictionary\n",
    "        blame_dict = [hit.to_dict() for hit in blame_es_data.scan()]\n",
    "        # Using Elasticsearch DSL function to get the data of Blame index\n",
    "        commit_es_data = Search(using=es, index=es_ma_index)\n",
    "        # Loading data into a dictionary\n",
    "        commit_dict = [hit.to_dict() for hit in commit_es_data.scan()]\n",
    "        # Creating pandas dataframe for commit data\n",
    "        commit_frame = pd.DataFrame(commit_dict)\n",
    "        # Creating pandas dataframe for blame data\n",
    "        blame_frame = pd.DataFrame(blame_dict)\n",
    "        blame_frame['file'] = blame_frame['file'].apply(lambda x:x.split('/')[-1])\n",
    "        # Getting the blame row count. If the frmae is empty, it means all the records are clean\n",
    "        blame_count = blame_frame.shape[0]\n",
    "        #print(blame_frame.columns)\n",
    "        \n",
    "        if blame_count>0:\n",
    "            # Adding a column to Blame frame indicating that the row represents a Buggy commit\n",
    "            blame_frame['type'] = 'Buggy'\n",
    "            # Combining Commit frmae with Blame frame. An additional column called 'type' gets added to the Commit frame.\n",
    "            #comb_frame = pd.merge(commit_frame,blame_frame,how='left',left_on = ['hash','file_path'],right_on = ['blame_hash','file'])\n",
    "            comb_frame = pd.merge(commit_frame,blame_frame,how='left',left_on = ['hash','file_name'],right_on = ['blame_hash','file'])\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            # If the Blame frame is empty, no need to merge.\n",
    "            comb_frame=commit_frame\n",
    "        # When merging happnes and 'type' column gets added to the main Commit frame, The rows that are not part of Blame frame are filled with 'Nan'.\n",
    "        # Here, all the NaNs fro 'type' column are replaced with 'Clean' label.\n",
    "        # Effectively, Each commit file (one Commit can contain more than one file) is categorised as either Buggy or Clean.\n",
    "        print(comb_frame[comb_frame['type']=='Buggy'].shape)\n",
    "        comb_frame['type'] = comb_frame['type'].fillna('Clean')\n",
    "        \n",
    "        # Cleaning and retaining the required columns\n",
    "        comb_frame_refined = comb_frame[['hash', 'Author','Committer', 'Email', 'message',                               'committed_date', 'no._of_branches', 'merge_commit?',\n",
    "                                        'no._of_mod_files', 'dmm_unit_size', 'dmm_unit_complexity','dmm_unit_interfacing',\n",
    "                                        'file_path', 'complexity','functions', 'lines_added', 'lines_removed', 'size', 'tokens',\n",
    "                                        'type']]\n",
    "        # Commit hash raw value is very long. Cutting the value into first ten chars \n",
    "        # Assumption is that the first ten chars rednder necessary uniqueness. May need to revisit later\n",
    "        #comb_frame_refined['hash'] = comb_frame_refined['hash'].str.slice(0,10)\n",
    "\n",
    "        # Changing the type from string to Data. Used Pacific time zone. Heard the pacific coast is beautiful\n",
    "        commit_frame['committed_date'] = commit_frame['committed_date'].astype('str').apply(lambda x: pd.to_datetime(x).tz_convert('US/Pacific'))\n",
    "        # Sorting the frame by committes date\n",
    "        comb_frame_refined = comb_frame_refined.drop_duplicates().sort_values('committed_date', ascending=False)\n",
    "        \n",
    "        return comb_frame_refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "se = get_latest_commits(p2[0],p2[1],p2[2])\n",
    "#get_latest_commits(p6[0],p6[1],p6[2])\n",
    "se[se['type']=='Buggy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following cells contain the code catering for K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data for few Git Repos. Look into the above method definitions for detail\n",
    "#p1 = create_components('https://github.com/R-Knowsys/elasticray.git','',localdir)\n",
    "#p2 = create_components('git://localhost/grimoirelab.git','',localdir)\n",
    "#p3 = create_components('git://localhost/vscode.git','',localdir)\n",
    "#p4 = create_components('https://github.com/tensorflow/tensorflow.git','',localdir)    \n",
    "#p5 = create_components('https://github.com/facebook/react-native.git','',localdir)\n",
    "#p6 = create_components('https://github.com/kcramakrishna/Digital_Assistant_Client.git','',localdir)\n",
    "#p7 = create_components('git://localhost/tensorflow.git','',localdir)\n",
    "#p8 = create_components('git://localhost/Python.git','',localdir)\n",
    "#p9 = create_components('git://localhost/keras.git','',localdir)\n",
    "#p10 = create_components('git://localhost/requests.git','',localdir)\n",
    "p11 = create_components('git://localhost/scrapy.git','',localdir)\n",
    "#p12 = create_components('git://localhost/flask.git','',localdir)\n",
    "\n",
    "\n",
    "# Getting the commit data as pandas frames\n",
    "#p1_commits = get_latest_commits(p1[0],p1[1],p1[2])\n",
    "#p2_commits = get_latest_commits(p2[0],p2[1],p2[2])\n",
    "#p3_commits = get_latest_commits(p3[0],p3[1],p3[2])\n",
    "#p4_commits = get_latest_commits(p4[0],p4[1],p4[2])\n",
    "#p6_commits = get_latest_commits(p6[0],p6[1],p6[2])\n",
    "#p7_commits = get_latest_commits(p7[0],p7[1],p7[2])\n",
    "#p8_commits = get_latest_commits(p8[0],p8[1],p8[2])\n",
    "#p9_commits = get_latest_commits(p9[0],p9[1],p9[2])\n",
    "#p10_commits = get_latest_commits(p10[0],p10[1],p10[2])\n",
    "p11_commits = get_latest_commits(p11[0],p11[1],p11[2])\n",
    "#p12_commits = get_latest_commits(p12[0],p12[1],p12[2])\n",
    "\n",
    "\n",
    "# Combining commit data from two Git Repos. Add as many Repos as you like\n",
    "#total_commits = p2_commits.append(p4_commits)\n",
    "#total_commits = p11_commits\n",
    "#print(total_commits.shape)\n",
    "p11_commits.to_csv('scrapy.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
