{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to find buggy commits. \n",
    "# Finding buggy commits entails using 'git blame' extensively. This takes a LOT of time. \n",
    "# We have not yet fine tuned the logic to find 'buggy' commits. We should disable this to save time.\n",
    "# Default is 'False'. \n",
    "# For now, do not set this to 'True'. \n",
    "findBuggyFlag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import git\n",
    "import os\n",
    "from git import Repo\n",
    "import sys\n",
    "\n",
    "def create_components(git_url,esurl,localdir):\n",
    "    # if no url supplied for Elastic, assume the localhost\n",
    "    if esurl =='':\n",
    "        try:\n",
    "            es = elasticsearch.Elasticsearch(['http://localhost:9200/'], maxsize=500, block=False)\n",
    "        except:\n",
    "            print('Elasticsearch not running at localhost:9200')\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        try:\n",
    "            es = elasticsearch.Elasticsearch([esurl], maxsize=500, block=False)\n",
    "        except:\n",
    "            print('Elasticsearch not running at the given URL. For default localhost, do not provide the argument')\n",
    "            sys.exit(1)\n",
    "        \n",
    "    # Get the default commit index name\n",
    "    es_index_raw = str.split(git_url,'/')[-1].split('.')[0]+'_'+'index'\n",
    "    \n",
    "    # Get the default blame index name\n",
    "    es_blame_index_raw = str.split(git_url,'/')[-1].split('.')[0]+'_'+'blame'+'_'+'index'\n",
    "    es_index = es_index_raw.lower()\n",
    "    es_blame_index = es_blame_index_raw.lower()\n",
    "        \n",
    "    # If local Repo path is not supplied, create default path in '/tmp'\n",
    "    if localdir == '':\n",
    "        if sys.platform == 'linux':\n",
    "            local_dir ='/tmp/'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "            clone_dir = '/tmp'\n",
    "        else:\n",
    "            local_dir ='C:\\\\Downloads'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "            clone_dir = 'C:\\\\Downloads'\n",
    "    else:\n",
    "        local_dir = localdir+'/'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "        clone_dir = localdir\n",
    "    \n",
    "    # Check if the local Repo already exists\n",
    "    if os.path.isdir(local_dir):\n",
    "        # Load the local Repo\n",
    "        try:\n",
    "            repo = git.Repo(local_dir)   \n",
    "        # Get the latest commit object in the local Repo\n",
    "            local_commit = repo.commit()     \n",
    "        except:\n",
    "            print('No valid Repo found at the location. \\\n",
    "                    If unsure, remove the directory and try without local dir argument')\n",
    "            sys.exit(1)\n",
    "                       # latest local commit \n",
    "        \n",
    "        # Get the latest commit object in the remote Repo\n",
    "        remote = git.remote.Remote(repo, 'origin')      # remote repo\n",
    "        info = remote.fetch()[0]                        # fetch changes\n",
    "        remote_commit = info.commit  \n",
    "        \n",
    "        # If latest commit in local and remote differ then refresh the local Repo\n",
    "        if (local_commit.hexsha == remote_commit.hexsha ):\n",
    "            print('No changes in the Repo...')\n",
    "        else:    \n",
    "            repo = git.Repo(local_dir) \n",
    "            o = repo.remotes.origin\n",
    "            o.pull()\n",
    "            # Analyse and store additional commit data\n",
    "            store_commit_data(local_dir,es,es_index,es_blame_index,local_commit.hexsha,remote_commit.hexsha )\n",
    "    else:\n",
    "        # If no local Repo exists, clone the Repo\n",
    "        try:\n",
    "            if sys.platform == 'linux':\n",
    "                git.Git(clone_dir).clone(git_url)\n",
    "            else:\n",
    "                git.Git('C:\\\\Downloads').clone(git_url)\n",
    "        except:\n",
    "            print('Not able to clone the Repo. \\\n",
    "                    If there is a non Git directory with the same name, delete it and then try')\n",
    "            sys.exit(1)\n",
    "        \n",
    "        # Delete the elastic indices, if exist\n",
    "        es.indices.delete(index=es_index, ignore=[400, 404])\n",
    "        es.indices.delete(index=es_blame_index, ignore=[400, 404])\n",
    "        \n",
    "        # Create new elastic indices\n",
    "        es.indices.create(es_index)\n",
    "        es.indices.create(es_blame_index)\n",
    "        \n",
    "        # Call the function to store the necessary commit data\n",
    "        store_commit_data(local_dir,es,es_index,es_blame_index,'None','None')\n",
    "\n",
    "    return es,es_index,es_blame_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_blame(commit, q_blamelist,eachline, local_dir, repo, npath):\n",
    "    blamelist = []\n",
    "    repo_blame = repo.blame(commit.hash,npath,eachline)\n",
    "    \n",
    "    # Git Blame of a line can produce multiple records with each record representing a past modification\n",
    "    for blame_record in repo_blame:\n",
    "        # Git Blame produces duplicate records (Don't know why). \n",
    "        # Attempt to ignore duplicated by comparting the current record with the previous record\n",
    "        # Also Git Blame produces record of the same commit hash, which can be ignored\n",
    "        if str(commit.hash) !=str(blame_record[0]): #and (str(blame_record[0]) != prev_record):\n",
    "            # Building Blame tuple for each Blame record\n",
    "            blame_doc = {'orig_hash':commit.hash,'blame_hash':str(blame_record[0]),\n",
    "                                            'file':npath}    \n",
    "            # Loading blame data into the list\n",
    "            blamelist.append(blame_doc)\n",
    "    \n",
    "    # Put the blamelist into the queue.\n",
    "    q_blamelist.put(blamelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to processes each commit\n",
    "\n",
    "import concurrent.futures\n",
    "import elasticsearch\n",
    "from elasticsearch import helpers\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import queue\n",
    "import pandas as pd\n",
    "\n",
    "def process_commit(commit, repo, es, es_index, es_blame_index, local_dir):\n",
    "\n",
    "    # Creating empty lists for carrying commit data\n",
    "    doclist = []\n",
    "    blamelist = []\n",
    "    \n",
    "    # Create queues for commit data and blame data\n",
    "    q_blamelist = queue.Queue()\n",
    "    for mod in commit.modifications:\n",
    "        commit_data = {'hash':commit.hash,'Author':commit.author.name,'Email':commit.author.email,\n",
    "                               'message':commit.msg,'authored_date':commit.author_date,\n",
    "                               'Committer':commit.committer.name,'committed_date':commit.committer_date,\n",
    "                               'no._of_branches':len(commit.branches),'merge_commit?':commit.merge,\n",
    "                               'no._of_mod_files':len(commit.modifications),'dmm_unit_size':commit.dmm_unit_size,\n",
    "                               'dmm_unit_complexity':commit.dmm_unit_complexity,\n",
    "                               'dmm_unit_interfacing':commit.dmm_unit_interfacing,\n",
    "                               'file_name':mod.filename, 'file_path':mod.new_path, 'complexity': mod.complexity, \n",
    "                               'functions': len(mod.methods),\n",
    "                               'lines_added':mod.added,'lines_removed': mod.removed,\n",
    "                               'loc':mod.nloc,\n",
    "                               'size': 0 if mod.source_code is None else len(mod.source_code.splitlines()),\n",
    "                               'tokens':mod.token_count        \n",
    "                        }\n",
    "        \n",
    "        # loading each commit tuple into the list\n",
    "        doclist.append(commit_data)\n",
    "        \n",
    "        if (findBuggyFlag == True):\n",
    "            # Each modification object contains diff_parsed tag with \"added\" and \"deleted\" properties. \n",
    "            alines = mod.diff_parsed['added']\n",
    "            \n",
    "            # \"added\" property is a tuple list with line number and actual line text. \n",
    "            # List of text lines is extracted\n",
    "            addedlines = [x[1] if len(alines)>0 else 'None' for x in alines ]\n",
    "                    \n",
    "            # Each modification object contains diff_parsed tag with \"added\" and \"deleted\" properties. \n",
    "            blines = mod.diff_parsed['deleted']\n",
    "            # \"deleted\" property is a tuple list with line number and actual line text. \n",
    "            # List of text lines is extracted\n",
    "            deletedlines = [x[1] if len(blines)>0 else 'None' for x in blines]\n",
    "            \n",
    "            no_of_mod_files = len(commit.modifications)\n",
    "            lines_added = mod.added\n",
    "            count = 0\n",
    "\n",
    "            # Absolute path of the file in the cloned repo. \n",
    "            # This is required to validate that the file has not been deleted in the subsequent commits\n",
    "            newfilepath = local_dir+'/'+str(mod.new_path)\n",
    "\n",
    "            # For bug fix commits, retrieving the blame data. \n",
    "            # Using Regex on Commit messages to identify bug fix commits\n",
    "            # Have lightweight threads process each modification(mod)\n",
    "            # Remember that each modification is a single line in the commit and we can have many\n",
    "            # Since most of the work is I/O bound, processors should be mostly idle, \n",
    "            #       we can load up with a LOT of threads - set he max threads to 200 \n",
    "            if len(re.findall(r\"\\bbug\\b|\\bBug\\b|\\bFix\\b|\\bfix\\b\",commit.msg))>0 and os.path.isfile(newfilepath) \\\n",
    "                    and no_of_mod_files <15 and lines_added < 1000:\n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=200) as executor:\n",
    "                    futures = [executor.submit(process_blame, commit, q_blamelist,eachline, local_dir, repo, \\\n",
    "                                               newfilepath) for eachline in addedlines]\n",
    "\n",
    "            # Wait for all threads to complete\n",
    "            executor.shutdown(wait=True)\n",
    "     \n",
    "    # Changed the method to assert the empty queue\n",
    "    while not q_blamelist.empty():\n",
    "        blamelist.append(q_blamelist.get())    \n",
    "\n",
    "    \n",
    "    # Each thread produces a list of tuples (in this case list of one tuple)\n",
    "    # When data from queue is appended to a list, it produces list of lists instead of list of tuples\n",
    "    # It is required to flatten the list of lists into list of tuples. \n",
    "    # Bulkloading to elastic won't work otherwise\n",
    "    blamelist = [item for sublist in blamelist for item in sublist]\n",
    "    \n",
    "    # using elasticsearch.py's helper tools to bulk load into elasticsearch's commit index           \n",
    "    helpers.bulk(es,doclist,index=es_index,doc_type ='commit_data',request_timeout = 2000)\n",
    "    \n",
    "    # Since Git Blame produces duplicate data, getting only unique records\n",
    "    #blamelist_fil = [i for n, i in enumerate(blamelist) if i not in blamelist[n + 1:]]\n",
    "    blame_df = pd.DataFrame(blamelist)\n",
    "    blame_df_clean = blame_df.drop_duplicates()\n",
    "\n",
    "    blamelist_fil = []\n",
    "    \n",
    "    df_iter = blame_df_clean.iterrows()\n",
    "    for index, document in df_iter:\n",
    "        blamelist_fil.append(document.to_dict())\n",
    "\n",
    "    # using elasticsearch.py's helper tools to bulk load into elasticsearch's blame index\n",
    "    helpers.bulk(es,blamelist_fil,index=es_blame_index,doc_type ='blame',request_timeout = 2000)\n",
    "\n",
    "    # delete our lists\n",
    "    del doclist\n",
    "    del blamelist\n",
    "    del blamelist_fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyse all commits (and import to elastic)\n",
    "\n",
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "from elasticsearch import helpers\n",
    "import pandas as pd\n",
    "from pydriller import RepositoryMining\n",
    "import git\n",
    "from git import Repo\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "from datetime import datetime\n",
    "\n",
    "def store_commit_data(local_dir,es,es_index,es_blame_index,local_commit,remote_commit):\n",
    "\n",
    "    store_start = datetime.now()\n",
    "    print('starting store_commit', store_start)\n",
    "    \n",
    "    repo = Repo(local_dir)\n",
    "\n",
    "    # Create Multithreading pool to use full CPU\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    \n",
    "    # If the Repo has just been cloned, the program will traverse the whole Repo\n",
    "    if(local_commit == 'None'):\n",
    "        [pool.apply_async(process_commit(commit, repo, es,es_index,es_blame_index, local_dir)) for commit in \\\n",
    "         RepositoryMining(local_dir).traverse_commits()]\n",
    "        \n",
    "    else:\n",
    "        [pool.apply_async(process_commit(commit, repo, es,es_index,es_blame_index, local_dir)) for commit in \\\n",
    "         RepositoryMining(local_dir,from_commit = local_commit, to_commit = remote_commit).traverse_commits()]\n",
    "\n",
    "    # Close Multiprocessing pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    store_end = datetime.now()\n",
    "    print('exiting store_commit', store_end)\n",
    "    print('time taken by store_commit', (store_end - store_start))\n",
    "    \n",
    "    # Very important to explicitly refresh the Elastic indices as they are not automatically done.\n",
    "    es.indices.refresh([es_blame_index,es_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realised that git blame and most of our code is heavily constrained by disk speed. \n",
    "#       We can create and use an in memory file system. In Linux we can use /tmp \n",
    "from pathlib import Path\n",
    "\n",
    "#Set tmpfs if you have tmpfs. Else leave this as '/tmp' /tmp is usually in RAM \n",
    "tmpfs = '/tmp'\n",
    "\n",
    "if (tmpfs == ''):\n",
    "    Home_address = str(Path.home())\n",
    "else:\n",
    "    #Set Home_address to the tmpfs \n",
    "    Home_address = tmpfs\n",
    " \n",
    "print(Home_address)\n",
    "localdir = Home_address + '/cg_Repos'\n",
    "print(localdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import pandas as pd\n",
    "\n",
    "def get_latest_commits(es_instance,commit_index,blame_index):\n",
    "        # Assigning Elastic instance, Commit Elastic Index and Blame Elastic Index to variables\n",
    "        es = es_instance\n",
    "        es_ma_index = commit_index\n",
    "        es_bl_index = blame_index\n",
    "        # Using Elasticsearch DSL function to get the data of Commit index\n",
    "        blame_es_data = Search(using=es, index=es_bl_index)\n",
    "        # Loading data into a dictionary\n",
    "        blame_dict = [hit.to_dict() for hit in blame_es_data.scan()]\n",
    "        # Using Elasticsearch DSL function to get the data of Blame index\n",
    "        commit_es_data = Search(using=es, index=es_ma_index)\n",
    "        # Loading data into a dictionary\n",
    "        commit_dict = [hit.to_dict() for hit in commit_es_data.scan()]\n",
    "        # Creating pandas dataframe for commit data\n",
    "        commit_frame = pd.DataFrame(commit_dict)\n",
    "        # Creating pandas dataframe for blame data\n",
    "        blame_frame = pd.DataFrame(blame_dict)\n",
    "        # Getting the blame row count. If the frame is empty, it means all the records are clean\n",
    "        blame_count = blame_frame.shape[0]\n",
    "        #print(blame_frame.columns)\n",
    "        if blame_count>0:\n",
    "            blame_frame['file'] = blame_frame['file'].apply(lambda x:x.split('/')[-1])\n",
    "            # Adding a column to Blame frame indicating that the row represents a Buggy commit\n",
    "            blame_frame['type'] = 'Buggy'\n",
    "            # Combining Commit frmae with Blame frame. \n",
    "            # An additional column called 'type' gets added to the Commit frame.\n",
    "            comb_frame = pd.merge(commit_frame,blame_frame,how='left', left_on = ['hash','file_name'], \\\n",
    "                                  right_on = ['blame_hash','file'])     \n",
    "        else:\n",
    "            # If the Blame frame is empty, no need to merge.\n",
    "            comb_frame=commit_frame\n",
    "            comb_frame['type'] = 'Clean'\n",
    "            \n",
    "        # When merging happnes and 'type' column gets added to the main Commit frame, \n",
    "        # The rows that are not part of Blame frame are filled with 'Nan'.\n",
    "        # Here, all the NaNs from 'type' column are replaced with 'Clean' label.\n",
    "        # Effectively, Each commit file (one Commit can contain more than one file) is categorised as \n",
    "        #                 either Buggy or Clean.\n",
    "        comb_frame['type'] = comb_frame['type'].fillna('Clean')\n",
    "        \n",
    "        # Cleaning and retaining the required columns\n",
    "        comb_frame_refined = comb_frame[['hash', 'Author','Committer', 'Email', 'message',\n",
    "                                         'committed_date', 'no._of_branches', 'merge_commit?',\n",
    "                                        'no._of_mod_files', 'dmm_unit_size', 'dmm_unit_complexity',\n",
    "                                         'dmm_unit_interfacing',\n",
    "                                        'file_path', 'complexity','functions', 'lines_added', 'lines_removed', \n",
    "                                         'size', 'tokens',\n",
    "                                        'type']]\n",
    "        \n",
    "        # Changing the type from string to Data. Used Pacific time zone. Heard the pacific coast is beautiful\n",
    "        # @Vamsi: Maybe we should use GMT instead opf pacific coast ?\n",
    "        commit_frame['committed_date'] = \\\n",
    "        commit_frame['committed_date'].astype('str').apply(lambda x: pd.to_datetime(x).tz_convert('US/Pacific'))\n",
    "        \n",
    "        # Sorting the frame by committes date\n",
    "        comb_frame_refined = comb_frame_refined.drop_duplicates().sort_values('committed_date', ascending=False)\n",
    "        \n",
    "        return comb_frame_refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data for Git Repo. \n",
    "p1 = create_components('git://localhost/elasticray.git','',localdir)\n",
    "\n",
    "# Getting the commit data as pandas frames\n",
    "p1_commits = get_latest_commits(p1[0],p1[1],p1[2])\n",
    "\n",
    "# Storing these to file on hard disk\n",
    "p1_commits.to_csv('/home/kc/Projects/data_files/elasticray.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cg_h2o4gpu",
   "language": "python",
   "name": "cg_h2o4gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
