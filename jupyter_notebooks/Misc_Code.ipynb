{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 5, init='k-means++',max_iter = 20)\n",
    "s = kmeans.fit(data_scaled)\n",
    "s_centroids = s.cluster_centers_\n",
    "\n",
    "plt.scatter(data_scaled[:,0], data_scaled[:,1], s=50, alpha=0.5)\n",
    "plt.scatter(s_centroids[:, 0], s_centroids[:,1], c='red', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimentionality Reduction before training with Kmeans.\n",
    "# Dimentionality Reduction is the technique used to identify the most influential features for machine learning\n",
    "# The input data in this case contains just ten features. Hence, this may not be required\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assigning the input frame to a simpler variable\n",
    "df = ml_commits_numeric_na\n",
    "# Creating the instance of Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "# Transforming the input data using Standard Scaler\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "# Creating the instance of PCA. PCA is a popular tool for the purpose of Dimentionality Reduction\n",
    "pca = PCA()\n",
    "# Processing the scaled data using PCA\n",
    "pca.fit(data_scaled)\n",
    "# Printing the variance ratio for each feature. FYI, each column of the frame represents a feature\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the variance ratios for each feature\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(range(1,7),pca.explained_variance_ratio_.cumsum(),marker = 'o',linestyle = '--')\n",
    "plt.title('Explained Variance by Component')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulatve Explained Variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above plot, it is clear that first three features have major impact on variance. And the last five have no imapct at all.\n",
    "# Hence, we can try Kmeans with first three features\n",
    "pca = PCA(n_components = 6)\n",
    "pca.fit(data_scaled)\n",
    "scores_pca = pca.transform(data_scaled)\n",
    "print(scores_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the input frame with PCA processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "df = ml_commits_nout_numeric\n",
    "scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 6)\n",
    "pca.fit(data_scaled)\n",
    "scores_pca = pca.transform(data_scaled)\n",
    "kmeans = KMeans(n_clusters = 5, init='random',max_iter = 10)\n",
    "q = kmeans.fit(scores_pca)\n",
    "\n",
    "plt.scatter(scores_pca[:,0], scores_pca[:,1], s=50, alpha=0.5)\n",
    "plt.scatter(q_centroids[:, 0], q_centroids[:, 1], c='red', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to try out various input models\n",
    "# Includes measurement of quality of Clustering through silhouette_scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# X can be assigned to any input frame.\n",
    "X = data_normalized\n",
    "# If you want to try out Scaled frame, replace the above line with the following\n",
    "# X = data_scaled\n",
    "# For PCA frame, replace with the following line\n",
    "# X = scores_pca\n",
    "range_n_clusters = [2, 3, 4, 5, 6,8]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10,max_iter = 50,init = 'random')\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values =  sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    # In the below line, two features, X[:,4] and X[:,6] are selected for plotting\n",
    "    # Those features can be replaced with any other two features.\n",
    "    # Keep in mind that, the frame contains ten features, from X[:,0] to X[:,9]\n",
    "    # In the case of PCA, there are only three features selected after the dimentionality reduction.\n",
    "    # Hence, if the X is assigned to scores_pca, you can select from X[:,0] to X[:2]\n",
    "    ax2.scatter(X[:, 4], X[:, 6], marker='.', s=150, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    # The centers should have same positions as features. \n",
    "    ax2.scatter(centers[:,4], centers[:,6], marker='o',\n",
    "                c=\"red\", alpha=1, s=300, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[4], c[6], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to try out various input models\n",
    "# This is same as the above except that the input is limited to just two features\n",
    "# The puprose here is to understand how cluster behaves if we have same number of training features as plotted\n",
    "# Includes measurement of quality of Clustering through silhouette_scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# X can be assigned to any input frame.\n",
    "X = data_normalized[:,[3,5]]\n",
    "# If you want to try out Scaled frame, replace the above line with the following\n",
    "# X = data_scaled[:,[3,5]]\n",
    "# For PCA frame, replace with the following line\n",
    "# X = scores_pca[:,[0,1]]\n",
    "range_n_clusters = [2, 3, 4, 5, 6,8]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10,max_iter = 50,init = 'random')\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values =  sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    # Here, we can only plot X[:, 0], X[:, 1] as the training was done on just two features\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=150, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    # The centers should have same positions as features. \n",
    "    ax2.scatter(centers[:,0], centers[:,1], marker='o',\n",
    "                c=\"red\", alpha=1, s=300, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "data_scaled = scaler.fit_transform(ml_commits_all_na)\n",
    "num_cols = len(scores_pca[0,:])\n",
    "for i in range(num_cols):\n",
    "    col = scores_pca[:,i]\n",
    "    col_stats = ss.describe(col)\n",
    "    print(col_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import pandas as pd\n",
    "#p1[0],p1[1],p1[2]\n",
    "es = p1[0]\n",
    "es_ma_index = p1[1]\n",
    "es_bl_index =p1[2]\n",
    "# Using Elasticsearch DSL function to get the data of Commit index\n",
    "#print(es,es_ma_index,es_bl_index)\n",
    "blame_es_data = Search(using=es, index=es_bl_index)\n",
    "        # Loading data into a dictionary\n",
    "blame_dict = [hit.to_dict() for hit in blame_es_data.scan()]\n",
    "print(len(blame_dict))\n",
    "        # Using Elasticsearch DSL function to get the data of Blame index\n",
    "commit_es_data = Search(using=es, index=es_ma_index)\n",
    "        # Loading data into a dictionary\n",
    "commit_dict = [hit.to_dict() for hit in commit_es_data.scan()]\n",
    "print(len(commit_dict))\n",
    "        # Creating pandas dataframe for commit data\n",
    "commit_frame = pd.DataFrame(commit_dict)\n",
    "        # Creating pandas dataframe for blame data\n",
    "blame_frame = pd.DataFrame(blame_dict)\n",
    "        # Getting the blame row count. If the frmae is empty, it means all the records are clean\n",
    "blame_count = blame_frame.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h2o4gpu",
   "language": "python",
   "name": "h2o4gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
