{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting features from well known public repos for training.\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "training_data_files_path = '/home/kc/Projects/data_files/Training_data_from_public_git/'                     \n",
    "\n",
    "git_repo_folder = '/home/kc/Projects/git-server/'\n",
    "public_git_repos = [file for file in os.listdir(git_repo_folder) if file.endswith(\".git\")]\n",
    "\n",
    "#print(len(public_git_repos))\n",
    "# Split it into 4 sublists - we will run 4 notebooks to speed up the process\n",
    "public_git_repos_1 = public_git_repos[0:6]\n",
    "public_git_repos_2 = public_git_repos[7:14]\n",
    "public_git_repos_3 = public_git_repos[15:21]\n",
    "public_git_repos_4 = public_git_repos[22:28]\n",
    "public_git_repos_5 = public_git_repos[29:35]\n",
    "public_git_repos_6 = public_git_repos[36:42]\n",
    "public_git_repos_7 = public_git_repos[43:44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for repo in public_git_repos_2:\n",
    "    if DEBUG >= 1:\n",
    "        print('Starting mining for: ', repo)\n",
    "    p1 = create_components('git://localhost/'+repo,'',localdir)\n",
    "    p1_commits = get_latest_commits(p1[0],p1[1],p1[2])\n",
    "    p1_commits.to_csv(training_data_files_path+repo+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 5, init='k-means++',max_iter = 20)\n",
    "s = kmeans.fit(data_scaled)\n",
    "s_centroids = s.cluster_centers_\n",
    "\n",
    "plt.scatter(data_scaled[:,0], data_scaled[:,1], s=50, alpha=0.5)\n",
    "plt.scatter(s_centroids[:, 0], s_centroids[:,1], c='red', s=50)\n",
    "plt.show()\n",
    "# atulya test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimentionality Reduction before training with Kmeans.\n",
    "# Dimentionality Reduction is the technique used to identify the most influential features for machine learning\n",
    "# The input data in this case contains just ten features. Hence, this may not be required\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assigning the input frame to a simpler variable\n",
    "df = ml_commits_numeric_na\n",
    "# Creating the instance of Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "# Transforming the input data using Standard Scaler\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "# Creating the instance of PCA. PCA is a popular tool for the purpose of Dimentionality Reduction\n",
    "pca = PCA()\n",
    "# Processing the scaled data using PCA\n",
    "pca.fit(data_scaled)\n",
    "# Printing the variance ratio for each feature. FYI, each column of the frame represents a feature\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the variance ratios for each feature\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(range(1,7),pca.explained_variance_ratio_.cumsum(),marker = 'o',linestyle = '--')\n",
    "plt.title('Explained Variance by Component')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulatve Explained Variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above plot, it is clear that first three features have major impact on variance. And the last five have no imapct at all.\n",
    "# Hence, we can try Kmeans with first three features\n",
    "pca = PCA(n_components = 6)\n",
    "pca.fit(data_scaled)\n",
    "scores_pca = pca.transform(data_scaled)\n",
    "print(scores_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the input frame with PCA processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "df = ml_commits_nout_numeric\n",
    "scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 6)\n",
    "pca.fit(data_scaled)\n",
    "scores_pca = pca.transform(data_scaled)\n",
    "kmeans = KMeans(n_clusters = 5, init='random',max_iter = 10)\n",
    "q = kmeans.fit(scores_pca)\n",
    "\n",
    "plt.scatter(scores_pca[:,0], scores_pca[:,1], s=50, alpha=0.5)\n",
    "plt.scatter(q_centroids[:, 0], q_centroids[:, 1], c='red', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to try out various input models\n",
    "# Includes measurement of quality of Clustering through silhouette_scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# X can be assigned to any input frame.\n",
    "X = data_normalized\n",
    "# If you want to try out Scaled frame, replace the above line with the following\n",
    "# X = data_scaled\n",
    "# For PCA frame, replace with the following line\n",
    "# X = scores_pca\n",
    "range_n_clusters = [2, 3, 4, 5, 6,8]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10,max_iter = 50,init = 'random')\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values =  sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    # In the below line, two features, X[:,4] and X[:,6] are selected for plotting\n",
    "    # Those features can be replaced with any other two features.\n",
    "    # Keep in mind that, the frame contains ten features, from X[:,0] to X[:,9]\n",
    "    # In the case of PCA, there are only three features selected after the dimentionality reduction.\n",
    "    # Hence, if the X is assigned to scores_pca, you can select from X[:,0] to X[:2]\n",
    "    ax2.scatter(X[:, 4], X[:, 6], marker='.', s=150, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    # The centers should have same positions as features. \n",
    "    ax2.scatter(centers[:,4], centers[:,6], marker='o',\n",
    "                c=\"red\", alpha=1, s=300, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[4], c[6], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to try out various input models\n",
    "# This is same as the above except that the input is limited to just two features\n",
    "# The puprose here is to understand how cluster behaves if we have same number of training features as plotted\n",
    "# Includes measurement of quality of Clustering through silhouette_scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# X can be assigned to any input frame.\n",
    "X = data_normalized[:,[3,5]]\n",
    "# If you want to try out Scaled frame, replace the above line with the following\n",
    "# X = data_scaled[:,[3,5]]\n",
    "# For PCA frame, replace with the following line\n",
    "# X = scores_pca[:,[0,1]]\n",
    "range_n_clusters = [2, 3, 4, 5, 6,8]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10,max_iter = 50,init = 'random')\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values =  sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    # Here, we can only plot X[:, 0], X[:, 1] as the training was done on just two features\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=150, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    # The centers should have same positions as features. \n",
    "    ax2.scatter(centers[:,0], centers[:,1], marker='o',\n",
    "                c=\"red\", alpha=1, s=300, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "data_scaled = scaler.fit_transform(ml_commits_all_na)\n",
    "num_cols = len(scores_pca[0,:])\n",
    "for i in range(num_cols):\n",
    "    col = scores_pca[:,i]\n",
    "    col_stats = ss.describe(col)\n",
    "    print(col_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import pandas as pd\n",
    "#p1[0],p1[1],p1[2]\n",
    "es = p1[0]\n",
    "es_ma_index = p1[1]\n",
    "es_bl_index =p1[2]\n",
    "# Using Elasticsearch DSL function to get the data of Commit index\n",
    "#print(es,es_ma_index,es_bl_index)\n",
    "blame_es_data = Search(using=es, index=es_bl_index)\n",
    "        # Loading data into a dictionary\n",
    "blame_dict = [hit.to_dict() for hit in blame_es_data.scan()]\n",
    "print(len(blame_dict))\n",
    "        # Using Elasticsearch DSL function to get the data of Blame index\n",
    "commit_es_data = Search(using=es, index=es_ma_index)\n",
    "        # Loading data into a dictionary\n",
    "commit_dict = [hit.to_dict() for hit in commit_es_data.scan()]\n",
    "print(len(commit_dict))\n",
    "        # Creating pandas dataframe for commit data\n",
    "commit_frame = pd.DataFrame(commit_dict)\n",
    "        # Creating pandas dataframe for blame data\n",
    "blame_frame = pd.DataFrame(blame_dict)\n",
    "        # Getting the blame row count. If the frmae is empty, it means all the records are clean\n",
    "blame_count = blame_frame.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/shreyasbapat/89c6d6e09ff3f763e21ea68f98d74f84\n",
    "# This script outputs relevant system environment info\n",
    "# Run it with `python collect_env.py`.\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import re\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "except (ImportError, NameError, AttributeError):\n",
    "    TORCH_AVAILABLE = False\n",
    "\n",
    "PY3 = sys.version_info >= (3, 0)\n",
    "\n",
    "# System Environment Information\n",
    "SystemEnv = namedtuple('SystemEnv', [\n",
    "    'torch_version',\n",
    "    'is_debug_build',\n",
    "    'cuda_compiled_version',\n",
    "    'gcc_version',\n",
    "    'cmake_version',\n",
    "    'os',\n",
    "    'python_version',\n",
    "    'is_cuda_available',\n",
    "    'cuda_runtime_version',\n",
    "    'nvidia_driver_version',\n",
    "    'nvidia_gpu_models',\n",
    "    'cudnn_version',\n",
    "    'pip_version',  # 'pip' or 'pip3'\n",
    "    'pip_packages',\n",
    "    'conda_packages',\n",
    "])\n",
    "\n",
    "\n",
    "def run(command):\n",
    "    \"\"\"Returns (return-code, stdout, stderr)\"\"\"\n",
    "    p = subprocess.Popen(command, stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.PIPE, shell=True)\n",
    "    output, err = p.communicate()\n",
    "    rc = p.returncode\n",
    "    if PY3:\n",
    "        output = output.decode(\"utf-8\")\n",
    "        err = err.decode(\"utf-8\")\n",
    "    return rc, output.strip(), err.strip()\n",
    "\n",
    "\n",
    "def run_and_read_all(run_lambda, command):\n",
    "    \"\"\"Runs command using run_lambda; reads and returns entire output if rc is 0\"\"\"\n",
    "    rc, out, _ = run_lambda(command)\n",
    "    if rc is not 0:\n",
    "        return None\n",
    "    return out\n",
    "\n",
    "\n",
    "def run_and_parse_first_match(run_lambda, command, regex):\n",
    "    \"\"\"Runs command using run_lambda, returns the first regex match if it exists\"\"\"\n",
    "    rc, out, _ = run_lambda(command)\n",
    "    if rc is not 0:\n",
    "        return None\n",
    "    match = re.search(regex, out)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "\n",
    "def get_conda_packages(run_lambda):\n",
    "    if get_platform() == 'win32':\n",
    "        grep_cmd = r'findstr /R \"torch soumith mkl magma\"'\n",
    "    else:\n",
    "        grep_cmd = r'grep \"torch\\|soumith\\|mkl\\|magma\"'\n",
    "    out = run_and_read_all(run_lambda, 'conda list | ' + grep_cmd)\n",
    "    if out is None:\n",
    "        return out\n",
    "    # Comment starting at beginning of line\n",
    "    comment_regex = re.compile(r'^#.*\\n')\n",
    "    return re.sub(comment_regex, '', out)\n",
    "\n",
    "\n",
    "def get_gcc_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'gcc --version', r'gcc (.*)')\n",
    "\n",
    "\n",
    "def get_cmake_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'cmake --version', r'cmake (.*)')\n",
    "\n",
    "\n",
    "def get_nvidia_driver_version(run_lambda):\n",
    "    smi = get_nvidia_smi()\n",
    "    return run_and_parse_first_match(run_lambda, smi, r'Driver Version: (.*?) ')\n",
    "\n",
    "\n",
    "def get_gpu_info(run_lambda):\n",
    "    smi = get_nvidia_smi()\n",
    "    uuid_regex = re.compile(r' \\(UUID: .+?\\)')\n",
    "    rc, out, _ = run_lambda(smi + ' -L')\n",
    "    if rc is not 0:\n",
    "        return None\n",
    "    # Anonymize GPUs by removing their UUID\n",
    "    return re.sub(uuid_regex, '', out)\n",
    "\n",
    "\n",
    "def get_running_cuda_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'nvcc --version', r'V(.*)$')\n",
    "\n",
    "\n",
    "def get_cudnn_version(run_lambda):\n",
    "    \"\"\"This will return a list of libcudnn.so; it's hard to tell which one is being used\"\"\"\n",
    "    if get_platform() == 'win32':\n",
    "        cudnn_cmd = 'where /R \"%CUDA_PATH%\\\\bin\" cudnn*.dll'\n",
    "    else:\n",
    "        cudnn_cmd = 'find /usr/local /usr/lib -type f -name \"libcudnn*\" 2> /dev/null'\n",
    "    rc, out, _ = run_lambda(cudnn_cmd)\n",
    "    # find will return 1 if there are permission errors or if not found\n",
    "    if len(out) == 0:\n",
    "        return None\n",
    "    if rc != 1 and rc != 0:\n",
    "        return None\n",
    "    # Alphabetize the result because the order is non-deterministic otherwise\n",
    "    result = '\\n'.join(sorted(out.split('\\n')))\n",
    "    return 'Probably one of the following:\\n{}'.format(result)\n",
    "\n",
    "\n",
    "def get_nvidia_smi():\n",
    "    smi = 'nvidia-smi'\n",
    "    if get_platform() == 'win32':\n",
    "        smi = '\"C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVSMI\\\\%s\"' % smi\n",
    "    return smi\n",
    "\n",
    "\n",
    "def get_platform():\n",
    "    if sys.platform.startswith('linux'):\n",
    "        return 'linux'\n",
    "    elif sys.platform.startswith('win32'):\n",
    "        return 'win32'\n",
    "    elif sys.platform.startswith('cygwin'):\n",
    "        return 'cygwin'\n",
    "    elif sys.platform.startswith('darwin'):\n",
    "        return 'darwin'\n",
    "    else:\n",
    "        return sys.platform\n",
    "\n",
    "\n",
    "def get_mac_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'sw_vers -productVersion', r'(.*)')\n",
    "\n",
    "\n",
    "def get_windows_version(run_lambda):\n",
    "    return run_and_read_all(run_lambda, 'wmic os get Caption | findstr /v Caption')\n",
    "\n",
    "\n",
    "def get_lsb_version(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'lsb_release -a', r'Description:\\t(.*)')\n",
    "\n",
    "\n",
    "def check_release_file(run_lambda):\n",
    "    return run_and_parse_first_match(run_lambda, 'cat /etc/*-release',\n",
    "                                     r'PRETTY_NAME=\"(.*)\"')\n",
    "\n",
    "\n",
    "def get_os(run_lambda):\n",
    "    platform = get_platform()\n",
    "\n",
    "    if platform is 'win32' or platform is 'cygwin':\n",
    "        return get_windows_version(run_lambda)\n",
    "\n",
    "    if platform == 'darwin':\n",
    "        version = get_mac_version(run_lambda)\n",
    "        if version is None:\n",
    "            return None\n",
    "        return 'Mac OSX {}'.format(version)\n",
    "\n",
    "    if platform == 'linux':\n",
    "        # Ubuntu/Debian based\n",
    "        desc = get_lsb_version(run_lambda)\n",
    "        if desc is not None:\n",
    "            return desc\n",
    "\n",
    "        # Try reading /etc/*-release\n",
    "        desc = check_release_file(run_lambda)\n",
    "        if desc is not None:\n",
    "            return desc\n",
    "\n",
    "        return platform\n",
    "\n",
    "    # Unknown platform\n",
    "    return platform\n",
    "\n",
    "\n",
    "def get_pip_packages(run_lambda):\n",
    "    # People generally have `pip` as `pip` or `pip3`\n",
    "    def run_with_pip(pip):\n",
    "        if get_platform() == 'win32':\n",
    "            grep_cmd = r'findstr /R \"numpy torch\"'\n",
    "        else:\n",
    "            grep_cmd = r'grep \"torch\\|numpy\"'\n",
    "        return run_and_read_all(run_lambda, pip + ' list --format=legacy | ' + grep_cmd)\n",
    "\n",
    "    if not PY3:\n",
    "        return 'pip', run_with_pip('pip')\n",
    "\n",
    "    # Try to figure out if the user is running pip or pip3.\n",
    "    out2 = run_with_pip('pip')\n",
    "    out3 = run_with_pip('pip3')\n",
    "\n",
    "    num_pips = len([x for x in [out2, out3] if x is not None])\n",
    "    if num_pips is 0:\n",
    "        return 'pip', out2\n",
    "\n",
    "    if num_pips == 1:\n",
    "        if out2 is not None:\n",
    "            return 'pip', out2\n",
    "        return 'pip3', out3\n",
    "\n",
    "    # num_pips is 2. Return pip3 by default b/c that most likely\n",
    "    # is the one associated with Python 3\n",
    "    return 'pip3', out3\n",
    "\n",
    "\n",
    "def get_env_info():\n",
    "    run_lambda = run\n",
    "    pip_version, pip_list_output = get_pip_packages(run_lambda)\n",
    "\n",
    "    if TORCH_AVAILABLE:\n",
    "        version_str = torch.__version__\n",
    "        debug_mode_str = torch.version.debug\n",
    "        cuda_available_str = torch.cuda.is_available()\n",
    "        cuda_version_str = torch.version.cuda\n",
    "    else:\n",
    "        version_str = debug_mode_str = cuda_available_str = cuda_version_str = 'N/A'\n",
    "\n",
    "    return SystemEnv(\n",
    "        torch_version=version_str,\n",
    "        is_debug_build=debug_mode_str,\n",
    "        python_version='{}.{}'.format(sys.version_info[0], sys.version_info[1]),\n",
    "        is_cuda_available=cuda_available_str,\n",
    "        cuda_compiled_version=cuda_version_str,\n",
    "        cuda_runtime_version=get_running_cuda_version(run_lambda),\n",
    "        nvidia_gpu_models=get_gpu_info(run_lambda),\n",
    "        nvidia_driver_version=get_nvidia_driver_version(run_lambda),\n",
    "        cudnn_version=get_cudnn_version(run_lambda),\n",
    "        pip_version=pip_version,\n",
    "        pip_packages=pip_list_output,\n",
    "        conda_packages=get_conda_packages(run_lambda),\n",
    "        os=get_os(run_lambda),\n",
    "        gcc_version=get_gcc_version(run_lambda),\n",
    "        cmake_version=get_cmake_version(run_lambda),\n",
    "    )\n",
    "\n",
    "env_info_fmt = \"\"\"\n",
    "PyTorch version: {torch_version}\n",
    "Is debug build: {is_debug_build}\n",
    "CUDA used to build PyTorch: {cuda_compiled_version}\n",
    "OS: {os}\n",
    "GCC version: {gcc_version}\n",
    "CMake version: {cmake_version}\n",
    "Python version: {python_version}\n",
    "Is CUDA available: {is_cuda_available}\n",
    "CUDA runtime version: {cuda_runtime_version}\n",
    "GPU models and configuration: {nvidia_gpu_models}\n",
    "Nvidia driver version: {nvidia_driver_version}\n",
    "cuDNN version: {cudnn_version}\n",
    "Versions of relevant libraries:\n",
    "{pip_packages}\n",
    "{conda_packages}\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def pretty_str(envinfo):\n",
    "    def replace_nones(dct, replacement='Could not collect'):\n",
    "        for key in dct.keys():\n",
    "            if dct[key] is not None:\n",
    "                continue\n",
    "            dct[key] = replacement\n",
    "        return dct\n",
    "\n",
    "    def replace_bools(dct, true='Yes', false='No'):\n",
    "        for key in dct.keys():\n",
    "            if dct[key] is True:\n",
    "                dct[key] = true\n",
    "            elif dct[key] is False:\n",
    "                dct[key] = false\n",
    "        return dct\n",
    "\n",
    "    def prepend(text, tag='[prepend]'):\n",
    "        lines = text.split('\\n')\n",
    "        updated_lines = [tag + line for line in lines]\n",
    "        return '\\n'.join(updated_lines)\n",
    "\n",
    "    def replace_if_empty(text, replacement='No relevant packages'):\n",
    "        if text is not None and len(text) == 0:\n",
    "            return replacement\n",
    "        return text\n",
    "\n",
    "    def maybe_start_on_next_line(string):\n",
    "        # If `string` is multiline, prepend a \\n to it.\n",
    "        if string is not None and len(string.split('\\n')) > 1:\n",
    "            return '\\n{}\\n'.format(string)\n",
    "        return string\n",
    "\n",
    "    mutable_dict = envinfo._asdict()\n",
    "\n",
    "    # If nvidia_gpu_models is multiline, start on the next line\n",
    "    mutable_dict['nvidia_gpu_models'] = \\\n",
    "        maybe_start_on_next_line(envinfo.nvidia_gpu_models)\n",
    "\n",
    "    # If the machine doesn't have CUDA, report some fields as 'No CUDA'\n",
    "    dynamic_cuda_fields = [\n",
    "        'cuda_runtime_version',\n",
    "        'nvidia_gpu_models',\n",
    "        'nvidia_driver_version',\n",
    "    ]\n",
    "    all_cuda_fields = dynamic_cuda_fields + ['cudnn_version']\n",
    "    all_dynamic_cuda_fields_missing = all(\n",
    "        mutable_dict[field] is None for field in dynamic_cuda_fields)\n",
    "    if TORCH_AVAILABLE and not torch.cuda.is_available() and all_dynamic_cuda_fields_missing:\n",
    "        for field in all_cuda_fields:\n",
    "            mutable_dict[field] = 'No CUDA'\n",
    "        if envinfo.cuda_compiled_version is None:\n",
    "            mutable_dict['cuda_compiled_version'] = 'None'\n",
    "\n",
    "    # Replace True with Yes, False with No\n",
    "    mutable_dict = replace_bools(mutable_dict)\n",
    "\n",
    "    # Replace all None objects with 'Could not collect'\n",
    "    mutable_dict = replace_nones(mutable_dict)\n",
    "\n",
    "    # If either of these are '', replace with 'No relevant packages'\n",
    "    mutable_dict['pip_packages'] = replace_if_empty(mutable_dict['pip_packages'])\n",
    "    mutable_dict['conda_packages'] = replace_if_empty(mutable_dict['conda_packages'])\n",
    "\n",
    "    # Tag conda and pip packages with a prefix\n",
    "    # If they were previously None, they'll show up as ie '[conda] Could not collect'\n",
    "    if mutable_dict['pip_packages']:\n",
    "        mutable_dict['pip_packages'] = prepend(mutable_dict['pip_packages'],\n",
    "                                               '[{}] '.format(envinfo.pip_version))\n",
    "    if mutable_dict['conda_packages']:\n",
    "        mutable_dict['conda_packages'] = prepend(mutable_dict['conda_packages'],\n",
    "                                                 '[conda] ')\n",
    "    return env_info_fmt.format(**mutable_dict)\n",
    "\n",
    "\n",
    "def get_pretty_env_info():\n",
    "    return pretty_str(get_env_info())\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Collecting environment information...\")\n",
    "    output = get_pretty_env_info()\n",
    "    print(output)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use this code which seems simpler. You might need to run is separate from jupyter notebook though.\n",
    "https://gist.github.com/f0k/63a664160d016a491b2cbea15913d549\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix heatmap visualization as a sanity check\n",
    "\n",
    "if DEBUG >=4:\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.set(font_scale=1.5)\n",
    "\n",
    "    # We need as many as cols as we have features\n",
    "    cols=['1','2','3','4','5','6']\n",
    "\n",
    "    # visualization for cluster number\n",
    "    cluster_number = 2\n",
    "    hm = sns.heatmap(gmm_covariances[cluster_number,:,:],\n",
    "                     cbar=True,\n",
    "                     annot=True,\n",
    "                     square=True,\n",
    "                     fmt='.5f',\n",
    "                     annot_kws={'size': 12},\n",
    "                     yticklabels=cols,\n",
    "                     xticklabels=cols)\n",
    "\n",
    "    plt.title('Covariance matrix of the cluster showing correlation coefficients')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG >=4:\n",
    "    if (h2o4gpu_enabled == True):\n",
    "        from h2o4gpu import metrics\n",
    "    else:\n",
    "        from sklearn import metrics\n",
    "\n",
    "    metrics.silhouette_score(data_scaled, gmm_hash_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data files and concantenate into a dataframe\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "training_data_files_path = r'/home/kc/junk/Training_data_from_public_git/'                     \n",
    "all_files = glob.glob(os.path.join(training_data_files_path, \"*.csv\"))     \n",
    "\n",
    "for file in all_files:\n",
    "    print('Processing: ',file)\n",
    "    commits = pd.read_csv(file)\n",
    "    commits['committed_date'] = \\\n",
    "        commits['committed_date'].astype('str').apply(lambda x: pd.to_datetime(x).tz_convert('GMT'))\n",
    "    commits['authored_date'] = \\\n",
    "        commits['authored_date'].astype('str').apply(lambda x: pd.to_datetime(x).tz_convert('GMT'))\n",
    "    commits.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data files\n",
    "import pandas as pd\n",
    "\n",
    "training_data_files_path = r'/home/kc/Projects/data_files/Training_data_from_public_git/'                     \n",
    "\n",
    "angular_commits = pd.read_csv(training_data_files_path+'angularjs.git.csv')\n",
    "#vscode_commits=pd.read_csv('/home/kc/Projects/data_files/vscode.csv')\n",
    "react_commits=pd.read_csv(training_data_files_path+'react-native.git.csv')\n",
    "\n",
    "#total_commits=tensorflow_commits.append(vscode_commits, ignore_index=True)\n",
    "total_commits=angular_commits.append(react_commits, ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_python_env",
   "language": "python",
   "name": "cuda_python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
