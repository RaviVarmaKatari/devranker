{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DEBUG flag with values from '0' to '5'. Default is '0' which is OFF. \n",
    "# Use this cautiously - we are not validating for this\n",
    "\n",
    "DEBUG = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to check if you have GPU and cuda working\n",
    "# https://colab.research.google.com/notebooks/gpu.ipynb\n",
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cuda - nvidia-smi if you have it installed\n",
    "# Add a flag with Default as False. Don't change this unless you have cuda installed.\n",
    "nvidia_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data files and concantenate into a dataframe\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "training_data_files_path = r'/home/kc/Projects/data_files/Training_data_from_public_git/'                     \n",
    "all_files = glob.glob(os.path.join(training_data_files_path, \"*.csv\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "total_commits = pd.concat(df_from_each_file, ignore_index=True)\n",
    "                             \n",
    "if DEBUG >=2:\n",
    "    print(\"Total No. of rows: \", total_commits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating various features for each modification\n",
    "\n",
    "# Note: We will add a prefix \"feature\" to variables to create the features for the ML model.\n",
    "\n",
    "# From documentation: https://pydriller.readthedocs.io/en/latest/commit.html\n",
    "#     dmm_unit_size (float): DMM metric value for the unit size property.\n",
    "#     dmm_unit_complexity (float): DMM metric value for the unit complexity property.\n",
    "#     dmm_unit_interfacing (float): DMM metric value for the unit interfacing property.\n",
    "\n",
    "# https://pydriller.readthedocs.io/en/latest/modifications.html\n",
    "#     complexity: Cyclomatic Complexity of the file\n",
    "#     changed_methods: subset of _methods_ containing only the changed methods.\n",
    "\n",
    "# Here is more about dmm: https://pydriller.readthedocs.io/en/latest/deltamaintainability.html\n",
    "#     The delta-maintainability metric is the proportion of low-risk change in a commit. \n",
    "#     The resulting value ranges from 0.0 (all changes are risky) to 1.0 (all changes are low risk). \n",
    "#     It rewards making methods better, and penalizes making things worse.\n",
    "\n",
    "# \"total lines changed\" is important but can be very misleading metric of a mod/commit. \n",
    "# We will create a feature which blunts the weight of nloc. From a code review perspective and\n",
    "# general best practices, we can say that 3 commits of 30 lines each is better than a single commit of 90 lines\n",
    "# We should probably increase weightage if the file has been added or deleted. Future versions?\n",
    "total_commits['total_changed'] = total_commits['number_lines_added'] + total_commits['number_lines_removed']\n",
    "total_commits['feature_total_changed'] = (total_commits['total_changed'] ** 0.7)\n",
    "\n",
    "# Feature to account for Methods/functions added and deleted. We propose that writing\n",
    "#     more methods should be exponentially weighted since it makes the code more modular \n",
    "# What happens when a function/method is added and another is deleted. This is a significant\n",
    "#      change which we are currently ignoring. Some future version to accomodate this?\n",
    "# We need to prevent 'gaming' by programmers who may write useless methods to increase score\n",
    "total_commits['n_functions_add_del'] = abs(total_commits['number_functions_before'] - total_commits['number_functions_after'])\n",
    "total_commits['feature_add_del_functions'] = (total_commits['n_functions_add_del'] ** 1.3) * (total_commits['total_changed'] ** 0.5)  \n",
    "# Other comments:\n",
    "# 1. Why are we raising 'n_functions_add_del' to this particular exponent? Here is the thought process:\n",
    "#     We wanted to make sure that the 'avg' mod (i.e. the middle cluster) ended up with features with approx. the same scale.\n",
    "#     This ensures that nloc does not determine the 'value' of the cluster. The rest of the feaures of the mod should be able to\n",
    "#     have a non-trivial impact on the'value' of the cluster.\n",
    "# 2. We will not worry about maintainig scale for the rest of the clusters. We will let that self organise.\n",
    "# 3. We used https://www.desmos.com/calculator to see the curves for the exponents and tweaked them to ensure that the \n",
    "#     'sum of centroids' for the 'avg' or 'middle' cluster was similarly affected by each of the feature centroid. \n",
    "# (Is this a hack? Yep but it sorta plays into our solution as you will see)\n",
    "# Note to self: Write a paper giving a more mathematical description to the solution     \n",
    "\n",
    "\n",
    "# Feature to account for methods which were edited. From experience, we postulate that\n",
    "#    if the effort to change 2 methods is 'x', then the effort to change 4 methods \n",
    "#    slightly more than 2x since there will be more complexity involved. \n",
    "# Here too, the exponents have been selected keeping in mind the 'middle' cluster\n",
    "total_commits['feature_changed_functions'] = (total_commits['number_functions_edited'] ** 1.1) * (total_commits['total_changed'] ** 0.5)  \n",
    "\n",
    "# 1. dmm values are given for the commit. i.e. these features will repeat as many times as\n",
    "#    no. of mods that exist in the commit. To balnce this, we will divide by no. of \n",
    "#    mods in commit which is nothing but the no. of files in the commit.\n",
    "# 2. If zero methods are added/deleted, we need to account for effort for changed methods. \n",
    "#     Lets make surethat the max of either of these features is selected.\n",
    "#total_commits['feature_dmm_size'] = total_commits['dmm_unit_size'] * (1 / total_commits['number_of_mod_files']) * total_commits[['feature_add_del_functions','feature_changed_functions']].max(axis=1) \n",
    "#total_commits['feature_dmm_unit_complexity'] = total_commits['dmm_unit_complexity'] * (1 / total_commits['number_of_mod_files']) * total_commits[['feature_add_del_functions','feature_changed_functions']].max(axis=1)\n",
    "#total_commits['feature_dmm_unit_interfacing'] = total_commits['dmm_unit_interfacing'] * (1 / total_commits['number_of_mod_files']) * total_commits[['feature_add_del_functions','feature_changed_functions']].max(axis=1)\n",
    "#total_commits['feature_dmm_size'] = total_commits['dmm_unit_size'] * (1 / total_commits['number_of_mod_files']) * (((total_commits['n_functions_add_del'] ** 1.3) * (total_commits['total_changed'] ** 0.3)) + ((total_commits['number_functions_edited'] ** 1.1) * (total_commits['total_changed'] ** 0.3))) \n",
    "#total_commits['feature_dmm_unit_complexity'] = total_commits['dmm_unit_complexity'] * (1 / total_commits['number_of_mod_files']) * (((total_commits['n_functions_add_del'] ** 1.3) * (total_commits['total_changed'] ** 0.3)) + ((total_commits['number_functions_edited'] ** 1.1) * (total_commits['total_changed'] ** 0.3))) \n",
    "#total_commits['feature_dmm_unit_interfacing'] = total_commits['dmm_unit_interfacing'] * (1 / total_commits['number_of_mod_files']) * (((total_commits['n_functions_add_del'] ** 1.3) * (total_commits['total_changed'] ** 0.3)) + ((total_commits['number_functions_edited'] ** 1.1) * (total_commits['total_changed'] ** 0.3))) \n",
    "total_commits['feature_dmm_size'] = total_commits['dmm_unit_size'] * (((total_commits['n_functions_add_del'] ** 1.3) * (total_commits['total_changed'] ** 0.3)) + ((total_commits['number_functions_edited'] ** 1.1) * (total_commits['total_changed'] ** 0.3))) \n",
    "total_commits['feature_dmm_unit_complexity'] = total_commits['dmm_unit_complexity'] * (((total_commits['n_functions_add_del'] ** 1.3) * (total_commits['total_changed'] ** 0.3)) + ((total_commits['number_functions_edited'] ** 1.1) * (total_commits['total_changed'] ** 0.3))) \n",
    "total_commits['feature_dmm_unit_interfacing'] = total_commits['dmm_unit_interfacing'] * (((total_commits['n_functions_add_del'] ** 1.3) * (total_commits['total_changed'] ** 0.3)) + ((total_commits['number_functions_edited'] ** 1.1) * (total_commits['total_changed'] ** 0.3))) \n",
    "\n",
    "# We should add a feature to reflect number of lines of comments in the modification. \n",
    "# We should have a feature measring code churn - Code churn can be due to sub-optimal coding or due to too many change requests. Need to distinguish this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Remove outliers. \n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def filter_outliers(data_frame):\n",
    "    \n",
    "    # Calculate z_scores and if zscore is greater than '3', then its an outlier\n",
    "    \n",
    "    # Get non-Outliers: \n",
    "    data_frame_non_outliers = data_frame[(np.abs(stats.zscore(data_frame.select_dtypes(exclude=['object','bool']))) < 3).all(axis=1)]\n",
    "\n",
    "    # Collect outliers\n",
    "    data_frame_outliers = data_frame[~(np.abs(stats.zscore(data_frame.select_dtypes(exclude=['object','bool']))) < 3).all(axis=1)]\n",
    "    \n",
    "    return data_frame_non_outliers, data_frame_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare frames for each file type extension\n",
    "\n",
    "def prepare_frame(total_commits, file_ext):\n",
    "    import numpy as np\n",
    "    \n",
    "    #Filter the mods based on file type extension\n",
    "    file_ext_commits = total_commits[total_commits['file_ext']==file_ext]\n",
    "\n",
    "    ml_commits = file_ext_commits[['hash','Author','feature_total_changed','feature_add_del_functions', \n",
    "                                   'feature_changed_functions', 'feature_dmm_unit_complexity','feature_dmm_size',\n",
    "                                   'feature_dmm_unit_interfacing', 'language_supported']]\n",
    "\n",
    "    # Resetting the frame's index. It is required to retain the integrity of the frame\n",
    "    ml_commits = ml_commits.reset_index().drop(columns = 'index')\n",
    "\n",
    "    # Temporarily dropping text columns for numeric processing\n",
    "    ml_commits_noText = ml_commits.drop(columns = ['Author','hash','language_supported'])\n",
    "\n",
    "    # Explicitely converting fields to numeric types and filling the NaNs with zeros\n",
    "    ml_commits_numeric = ml_commits_noText.apply(pd.to_numeric,errors ='coerce').fillna(0)\n",
    "\n",
    "    # Adding the Author column back to create a 'total' data frame\n",
    "    ml_commits_all_coloumns = ml_commits_numeric.copy()\n",
    "    ml_commits_all_coloumns['Author'] = ml_commits['Author']\n",
    "    ml_commits_all_coloumns['hash'] = ml_commits['hash']\n",
    "    ml_commits_all_coloumns['language_supported'] = ml_commits['language_supported']\n",
    "\n",
    "    return ml_commits_all_coloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale data in the frame\n",
    "\n",
    "def scale_frame(data_frame):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    # Use minMax scaler since this does not distort\n",
    "    # https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
    "    scaler = MinMaxScaler()\n",
    "    data_frame_numeric = data_frame.drop(columns = ['Author','hash'])\n",
    "    scaled_data_frame = scaler.fit_transform(data_frame_numeric)\n",
    "    \n",
    "    return scaled_data_frame, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster(scaled_file_ext_frame, file_ext, file_ext_frame, scaler, k, gmm_models_folder, \n",
    "                   centroids_folder):\n",
    "    \n",
    "    if nvidia_cuda == False:\n",
    "        # Initializing the regular CPU based Gaussian mixture model\n",
    "        from sklearn.mixture import GaussianMixture\n",
    "        mix = GaussianMixture(n_components=k, random_state=42)\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # Learning the Gaussian mixture model from data   \n",
    "    mix.fit(scaled_file_ext_frame)\n",
    "\n",
    "    # Saving the parameters of Gaussian mixture model in a file\n",
    "    import pickle\n",
    "    vfilename = gmm_models_folder+file_ext+'_cpu_gmm_model_pickle.sav'\n",
    "    pickle.dump(mix, open(vfilename, 'wb'))\n",
    "\n",
    "    # Predicting the cluster labels of the data for the Gaussian mixture model\n",
    "    cluster_frame = pd.DataFrame(scaled_file_ext_frame)\n",
    "    gmm_hash_clusters = mix.predict(cluster_frame)\n",
    "\n",
    "    # Collecting the mean of the Gaussian mixture model in 'gmmcentroids'\n",
    "    gmm_centroids = mix.means_\n",
    "    gmm_covariances = mix.covariances_\n",
    "    combinedCentroids = gmm_centroids[gmm_hash_clusters].sum(axis=1)\n",
    "\n",
    "    # Converting the input data series into pan\n",
    "    file_ext_frame['Cluster'] = gmm_hash_clusters\n",
    "    real_centroids = scaler.inverse_transform(gmm_centroids)\n",
    "\n",
    "    # Write these to dataframe\n",
    "    real_centroids_dataFrame = pd.DataFrame(real_centroids, columns=['feature_total_changed',\n",
    "                                              'feature_add_del_functions', 'feature_changed_functions',\n",
    "                                'feature_dmm_unit_complexity','feature_dmm_size','feature_dmm_unit_interfacing'])\n",
    "\n",
    "    # Add a cloumn for summing all centroids \"This is the value of the individual clusters\"\n",
    "    real_centroids_dataFrame['Sum_centroids'] = real_centroids_dataFrame.sum(axis = 1)\n",
    "    \n",
    "    # Save centroids of the clusters to a file for audit\n",
    "    centroid_file = centroids_folder+file_ext+'_cpu_centroids.csv'\n",
    "    real_centroids_dataFrame.to_csv(centroid_file)\n",
    "    \n",
    "    return file_ext_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need a GPU implementation for GMM. Not easy to find or to run. Most are old. \n",
    "# Finally found something which was working. Downloaded it and including this into the notebook.\n",
    "# Note that this code uses diagonal covariance. Full covariance would have been better but we\n",
    "#           can make do with diagonal for now.\n",
    "# https://mg.readthedocs.io/importing-local-python-modules-from-jupyter-notebooks/sys-path-in-notebook/path-notebook.html\n",
    "# https://github.com/ldeecke/gmm-torch\n",
    "# \n",
    "\n",
    "if nvidia_cuda == True:\n",
    "    import os\n",
    "    import sys\n",
    "    sys.path.insert(0, os.path.abspath('/home/kc/Projects/gpu_sklearn_tensor/gmm-torch/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_gpu(scaled_file_ext_frame, file_ext, file_ext_frame, scaler, k, gmm_models_folder, \n",
    "                   centroids_folder):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Instantiate GMM and xg boost models\n",
    "    if nvidia_cuda == True:\n",
    "        # Initializing the GPU enabled Gaussian mixture model\n",
    "        from gmm import GaussianMixture\n",
    "        dimensions = scaled_file_ext_frame.shape[1] # No of 'features'. \n",
    "        mix_gpu = GaussianMixture(k, dimensions)\n",
    "        torch.cuda.set_device('cuda:0')\n",
    "        mix_gpu.cuda()\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    # Convert pandas dataframe/numpy arrray to a tensor\n",
    "    scaled_file_ext_tensor_cpu = torch.tensor(list(scaled_file_ext_frame))\n",
    "    scaled_file_ext_tensor = scaled_file_ext_tensor_cpu.to(device='cuda')\n",
    "    \n",
    "    # Running the Gaussian mixture model from data   \n",
    "    mix_gpu.fit(scaled_file_ext_tensor)\n",
    "\n",
    "    # Saving the parameters of Gaussian mixture model in a file\n",
    "    import pickle\n",
    "    gmm_model_file = gmm_models_folder+file_ext+'_gpu_gmm_model_pickle.sav'\n",
    "    pickle.dump(mix_gpu, open(gmm_model_file, 'wb'))\n",
    "\n",
    "    # Predicting the cluster labels of the data for the Gaussian mixture model\n",
    "    gmm_clusters = mix_gpu.predict(scaled_file_ext_tensor, probs=True)\n",
    "    classlabels = torch.argmax(gmm_clusters, 1)\n",
    "    # Move this to CPU for further processing\n",
    "    if DEBUG >= 3:\n",
    "      print('classlabels ', classlabels)\n",
    "    gmm_hash_clusters = classlabels.to('cpu')\n",
    "    if DEBUG >= 3:\n",
    "      print('gmm_hash_clusters ', gmm_hash_clusters)\n",
    "    \n",
    "    # Collecting the mean of the Gaussian mixture model in 'gmmcentroids'\n",
    "    means_gpu = mix_gpu.mu \n",
    "    if DEBUG >= 3:\n",
    "      print('means_gpu: ', means_gpu)  \n",
    "    vars_gpu = mix_gpu.var \n",
    "    if DEBUG >= 3:\n",
    "      print('vars_gpu: ', vars_gpu)    \n",
    "    means_cpu = means_gpu.to('cpu')\n",
    "    if DEBUG >= 3:\n",
    "      print('means_cpu: ', means_cpu)\n",
    "    vars_cpu = vars_gpu.to('cpu')\n",
    "    if DEBUG >= 3:\n",
    "      print('vars_cpu: ', vars_cpu)\n",
    "    gmm_centroids = means_cpu.numpy()[0,:]\n",
    "\n",
    "    # Converting the input data series into pan\n",
    "    # https://stackoverflow.com/questions/57942487/how-to-convert-torch-tensor-to-pandas-dataframe\n",
    "    #file_ext_frame['Cluster'] = pd.DataFrame(gmm_hash_clusters.numpy())\n",
    "    file_ext_frame['Cluster'] = gmm_hash_clusters.numpy()\n",
    "    #print('allClusters ', gmm_hash_clusters.numpy())\n",
    "    #print('Clusters: ', file_ext_frame['Cluster'])\n",
    "    # Realised that some prediction are Nan. research pointed to this:\n",
    "    #     https://github.com/AlexanderFabisch/gmr/issues/5\n",
    "    #     https://www.researchgate.net/post/What-is-the-way-to-get-rid-off-NaN-values-in-GMM-UBM\n",
    "    #     https://github.com/AlexanderFabisch/gmr/issues/5#issuecomment-312727530\n",
    "    # We should probably increase number of clusters to make sure we are being thorough.\n",
    "    # Let us remove all rows where prediction is NaNs\n",
    "    file_ext_frame.dropna(inplace=True)\n",
    "    \n",
    "    # Now get the 'real value' of centroids by inverse scaling\n",
    "    real_centroids = scaler.inverse_transform(gmm_centroids)\n",
    "\n",
    "    # Write these to dataframe\n",
    "    real_centroids_dataFrame = pd.DataFrame(real_centroids, columns=['feature_total_changed',\n",
    "                                              'feature_add_del_functions', 'feature_changed_functions',\n",
    "                                'feature_dmm_unit_complexity','feature_dmm_size','feature_dmm_unit_interfacing'])\n",
    "\n",
    "    # Add a cloumn for summing all centroids \"This is the value of the individual clusters\"\n",
    "    real_centroids_dataFrame['Sum_centroids'] = real_centroids_dataFrame.sum(axis = 1)\n",
    "    \n",
    "    # Save centroids of the clusters to a file for audit\n",
    "    centroid_file = centroids_folder+file_ext+'_gpu_centroids.csv'\n",
    "    real_centroids_dataFrame.to_csv(centroid_file)\n",
    "    \n",
    "    # Some memory shortage issue. Lets free it up.\n",
    "    #del scaled_file_ext_tensor\n",
    "    #del gmm_hash_clusters\n",
    "    #del means_cpu, gmm_centroids\n",
    "    #del real_centroids_dataFrame\n",
    "    del mix_gpu\n",
    "    torch.cuda.empty_cache()\n",
    "     \n",
    "    return file_ext_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boost_model(file_ext_frame, file_ext, xgboost_models_folder):\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import pickle\n",
    "\n",
    "    # Instantiate GMM and xg boost models\n",
    "    if nvidia_cuda == True:\n",
    "        # GPU has issues which we can ignore: \n",
    "        #  https://medium.com/data-design/xgboost-gpu-performance-on-low-end-gpu-vs-high-end-cpu-a7bc5fcd425b\n",
    "        # Instantiate the XgBoost model with GPU enabled\n",
    "        from xgboost import XGBClassifier\n",
    "        xgboost_model = XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n",
    "    else:\n",
    "        # Instantiate the CPU XgBoost model\n",
    "        from xgboost import XGBClassifier\n",
    "        xgboost_model = XGBClassifier()\n",
    "        # xgboost_model = XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n",
    "        \n",
    "    # Remove text fields before numeric manipulations\n",
    "    file_ext_frame_numeric_xg = file_ext_frame.drop(columns=['Author','hash'])\n",
    "\n",
    "    # Prepare the 'X' and 'Y' for the model\n",
    "    X_file_ext_frame_numeric_xg = file_ext_frame_numeric_xg.drop(columns = ['Cluster'])\n",
    "    Y_file_ext_frame_numeric_xg = file_ext_frame_numeric_xg['Cluster']\n",
    "\n",
    "    # Split the data for 'Training' and 'Testing' datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_file_ext_frame_numeric_xg, Y_file_ext_frame_numeric_xg, random_state=7)\n",
    "\n",
    "\n",
    "    # Training the xgboost classifier\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the class labels of test data for xgboost classifier\n",
    "    y_pred = xgboost_model.predict(X_test)\n",
    "\n",
    "    # check accuracy\n",
    "    if DEBUG >= 1:\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(file_ext+'_size: ', file_ext_frame.shape[0])\n",
    "        print(file_ext+'_accuracy: ', accuracy)\n",
    "    \n",
    "    # Save the model to a file\n",
    "    if nvidia_cuda == True:\n",
    "      filename = xgboost_models_folder+file_ext+'_gpu_xgboost_model.sav'\n",
    "    else:\n",
    "      filename = xgboost_models_folder+file_ext+'_cpu_xgboost_model.sav'\n",
    "    pickle.dump(xgboost_model, open(filename, 'wb'))\n",
    "    \n",
    "    #Free up memory\n",
    "    #del file_ext_frame_numeric_xg\n",
    "    #del X_file_ext_frame_numeric_xg\n",
    "    #del X_train, X_test, y_train, y_test\n",
    "    #del y_pred\n",
    "    del xgboost_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:44:30] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:44:45] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:44:53] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:44:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:44:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:16] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:50] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:47] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:59] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:01] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-f638fb9733aa>:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  file_ext_frame['Cluster'] = gmm_hash_clusters\n",
      "/home/kc/14Jan21_cuda_python_env/lib/python3.9/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "CPU times: user 38min 4s, sys: 5min 34s, total: 43min 39s\n",
      "Wall time: 7min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "# We have to use %%time but NOT %%timeit. %%timeit does multiple runs.\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "# Create a coloumn 'file_ext' which is the file 'type'\n",
    "#total_commits['file_ext'] = total_commits['file_path'].apply(lambda x:pathlib.Path(str(x)).suffix).apply(lambda x:re.split(r\"[^a-zA-Z0-9\\s\\++\\_\\-]\",x)[-1])\n",
    "\n",
    "# For files without any extension, mark 'file_ext' as \"NoExt\" \n",
    "#total_commits.file_ext = total_commits.file_ext.replace(r'^\\s*$', 'NoExt', regex=True)\n",
    "\n",
    "\n",
    "# Prepare a list of all unique file extensions\n",
    "unique_extensions = total_commits['file_ext'].unique()\n",
    "# Print for Debugging \n",
    "if DEBUG >=1:\n",
    "    print('no. of unique_extensions: ', len(unique_extensions))\n",
    "\n",
    "\n",
    "# Folder to save models and centroids\n",
    "gmm_models_folder = '/home/kc/Projects/data_files/sav_files/gmm_sav/'\n",
    "centroids_folder = '/home/kc/Projects/data_files/sav_files/centroids/'\n",
    "xgboost_models_folder = '/home/kc/Projects/data_files/sav_files/xgboost_sav/'\n",
    "        \n",
    "# Remove all previous models \n",
    "#for folders in [gmm_models_folder, centroids_folder, xgboost_models_folder]:\n",
    "#    for file in glob.glob(folders+'*'):\n",
    "#        os.remove(file)\n",
    "\n",
    "# Set number of clusters\n",
    "k=5\n",
    "    \n",
    "# For every file extension: prepare the data frame, create cluster, train xgBoost model and save it.\n",
    "# We should change this to only those extensions supported by lizard/pydriller\n",
    "for file_ext in unique_extensions:\n",
    "    \n",
    "    # Extract data frame for the specific file type extension\n",
    "    file_ext_frame = prepare_frame(total_commits, file_ext)\n",
    "    if DEBUG >= 2:\n",
    "        print('Shape of file_ext_frame:', file_ext_frame.shape, file_ext)\n",
    "\n",
    "    # We seem to have some training data with missing data. Filter them here\n",
    "    if 'language_supported' not in file_ext_frame.columns:\n",
    "        if DEBUG >= 2:\n",
    "          print('ignoring file type: ', file_ext, '(missing column - language_supported)')      \n",
    "        continue\n",
    "    \n",
    "    # We need a good sample size for accurate results. \n",
    "    #    We will ignore'file_ext' if it has less than 50 rows \n",
    "    if file_ext_frame.shape[0] < 50:\n",
    "        if DEBUG >= 2:\n",
    "          print('ignoring file type: ', file_ext, '(Not enough rows)')\n",
    "        continue\n",
    "           \n",
    "    # Check if we support the file_ext\n",
    "    if file_ext_frame['language_supported'].values[0] == False:\n",
    "        if DEBUG >= 2:\n",
    "            print('Language not supported (ignoring):', file_ext)\n",
    "        continue \n",
    "\n",
    "    # Remove outliers from the frame (only if you reasonable amount of data points) \n",
    "    file_ext_frame.drop(columns='language_supported', inplace = True)\n",
    "    file_ext_frame_non_outliers, file_ext_frame_outliers = filter_outliers(file_ext_frame)\n",
    "    if DEBUG >= 2:\n",
    "        print('no. of non-outlier rows: ', file_ext_frame_non_outliers.shape[0])\n",
    "        print('no. of outlier rows: ', file_ext_frame_outliers.shape[0])\n",
    "    \n",
    "    # Scale the data    \n",
    "    # There is a peculiar behaviour here. For lot of file_ext (= 'md', 'html', etc,) All data points are \n",
    "    #     showing up as outliers. This is very funny. We should probably restrict our processing to files\n",
    "    #     supported by 'lizard/pydriller'. \n",
    "    # We are forced to check that we have at least 1 non_outlier.\n",
    "    if file_ext_frame_non_outliers.shape[0] >= 1:\n",
    "        scaled_file_ext_frame, scaler = scale_frame(file_ext_frame_non_outliers)\n",
    "        if DEBUG >= 2:\n",
    "            print('rows in scaled_file_ext_frame: ', scaled_file_ext_frame.shape[0])\n",
    "        \n",
    "        # Create the actual clusters from the data.\n",
    "        if nvidia_cuda == True:\n",
    "            clustered_frame = create_cluster_gpu(scaled_file_ext_frame, file_ext, file_ext_frame_non_outliers, scaler, k, gmm_models_folder, centroids_folder)\n",
    "        else:\n",
    "            clustered_frame = create_cluster(scaled_file_ext_frame, file_ext, file_ext_frame_non_outliers, scaler, k, gmm_models_folder, centroids_folder)\n",
    "\n",
    "        # Train xgboost model for each extension\n",
    "        create_boost_model(clustered_frame, file_ext, xgboost_models_folder)\n",
    "    else:\n",
    "        if DEBUG >= 2:\n",
    "          print('ignoring file type: ', file_ext, '(All rows are outliers)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "14Jan21_cuda_python_env",
   "language": "python",
   "name": "14jan21_cuda_python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
