{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data files\n",
    "import pandas as pd\n",
    "\n",
    "tensorflow_commits = pd.read_csv('/home/kc/Projects/data_files/tensorflow.csv')\n",
    "vscode_commits=pd.read_csv('/home/kc/Projects/data_files/vscode.csv')\n",
    "react_commits=pd.read_csv('/home/kc/Projects/data_files/react-native.csv')\n",
    "\n",
    "total_commits=tensorflow_commits.append(vscode_commits, ignore_index=True)\n",
    "total_commits=total_commits.append(react_commits, ignore_index=True)\n",
    "                             \n",
    "\n",
    "#total_commits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use h2o4gpu if you have it installed\n",
    "\n",
    "# Add a flag with Default as False. Don't change this unless your kernel uses h2o4gpu.\n",
    "h2o4gpu_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating various features for each modification\n",
    "\n",
    "# Total number of lines changed\n",
    "total_commits['total_changed'] = total_commits['lines_added'] + total_commits['lines_removed']\n",
    "\n",
    "# Fraction of lines changed per total numbe of lines in file\n",
    "# We need to account for the fact that new files added with have existing size as '0' and divide by '0' is indeterminate\n",
    "total_commits['size'].loc[total_commits['size'] == 0] = total_commits['total_changed']\n",
    "total_commits['ratio_changed'] = total_commits['total_changed'] / total_commits['size']\n",
    "\n",
    "# Need to weigh the complexity by quantum of change. \n",
    "total_commits['rated_complexity'] = total_commits['ratio_changed'] * total_commits['complexity'] * total_commits['total_changed']\n",
    "\n",
    "# weighing the dmm params by the total changed lines\n",
    "total_commits['total_dmm_size'] = total_commits['total_changed'] * total_commits['dmm_unit_size']\n",
    "total_commits['total_dmm_unit_complexity'] = total_commits['total_changed'] * total_commits['dmm_unit_complexity']\n",
    "total_commits['total_dmm_unit_interfacing'] = total_commits['total_changed'] * total_commits['dmm_unit_interfacing']\n",
    "\n",
    "# We picked the sqrt of no_of_mod_files to reduce weightage of this feature\n",
    "total_commits['scaled_rated_complexity']=total_commits['rated_complexity'] * (total_commits['no._of_mod_files'] ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data. ML requires the data to be converted to numericals\n",
    "#ml_commits = total_commits[['hash','Author', 'no._of_mod_files', 'dmm_unit_size',\n",
    "#       'dmm_unit_complexity', 'dmm_unit_interfacing', 'complexity', 'functions', 'lines_added', 'lines_removed', \n",
    "#       'tokens', 'type']]\n",
    "\n",
    "ml_commits = total_commits[['hash','Author','total_changed','rated_complexity',\n",
    "                            'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity']]\n",
    "\n",
    "# Resetting the frame's index. It is required to retain the integrity of the frame\n",
    "ml_commits = ml_commits.reset_index().drop(columns = 'index')\n",
    "\n",
    "# Temporarily dropping text columns for numeric processing\n",
    "ml_commits_noText = ml_commits.drop(columns = ['Author','hash'])\n",
    "\n",
    "# Explicitely converting fields to numeric types and filling the NaNs with zeros\n",
    "ml_commits_numeric = ml_commits_noText.apply(pd.to_numeric,errors ='coerce').fillna(0)\n",
    "\n",
    "# Adding the Author column back to create a 'total' data frame\n",
    "ml_commits_total = ml_commits_numeric.copy()\n",
    "ml_commits_total['Author'] = ml_commits['Author']\n",
    "ml_commits_total['hash'] = ml_commits['hash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers. \n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Calculate z_scores (and if zscore is greater than '3', then its an outlier) and collect normal subset.\n",
    "ml_commits_nout = ml_commits_total[(np.abs(stats.zscore(ml_commits_total.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "ml_commits_nout.to_csv('/home/kc/junk/totalCommits_nout.csv')\n",
    "\n",
    "# Collect outliers\n",
    "ml_commits_out = ml_commits_total[~(np.abs(stats.zscore(ml_commits_total.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "ml_commits_out.to_csv('/home/kc/junk/totalCommits_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying scaler to regular data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "if (h2o4gpu_enabled == True):\n",
    "    from h2o4gpu.preprocessing import MinMaxScaler\n",
    "else:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "# Use minMax scaler since this does not distort\n",
    "# https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
    "scaler = MinMaxScaler()\n",
    "ml_commits_nout_numeric = ml_commits_nout.drop(columns = ['Author','hash'])\n",
    "data_scaled = scaler.fit_transform(ml_commits_nout_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and figure out how many clusters are optimum\n",
    "# Plot the inertia curve to find the 'elbow'\n",
    "\n",
    "''' Realised that h2o4GPU has not implemented \"Intertia\". Hence we will have to use regular Kmeans library for intertia. \n",
    "Please ignore this cell entirely.\n",
    "\n",
    "\n",
    "import h2o4gpu\n",
    "\n",
    "SSE = []\n",
    "\n",
    "for cluster in range(1,20):\n",
    "    kmeans_ss = h2o4gpu.KMeans(n_gpus=1, n_clusters = cluster, init='k-means++', random_state = 42, backend=h2o4gpu)\n",
    "    %time kmeans_ss.fit(data_scaled)\n",
    "    SSE.append(kmeans_ss.inertia_)\n",
    "    print(kmeans_ss.cluster_centers_)\n",
    "    print(len(SSE))\n",
    "    print(str(SSE))\n",
    "    \n",
    "    # converting the results into a dataframe and plotting them\n",
    "frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and figure out how many clusters are optimum\n",
    "# Plot the inertia curve to find the 'elbow'\n",
    "# You don't need to run this for every iteration. Just uncomment and run whenever you need to\n",
    "\n",
    "# Realised that h2o4GPU has not implemented \"Intertia\". Hence we will have to use regular Kmeans library for intertia.\n",
    "# http://docs.h2o.ai/h2o4gpu/latest-stable/h2o4gpu-py-docs/html/_modules/h2o4gpu/solvers/kmeans.html\n",
    "\n",
    "# Set this to 'True' if you want to plot graph to find elbow\n",
    "find_elbow = False\n",
    "\n",
    "if (find_elbow == True): \n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    SSE = []\n",
    "\n",
    "    for cluster in range(1,20):\n",
    "        %time kmeans_ss = KMeans(n_clusters = cluster, init='k-means++', random_state = 42)\n",
    "        kmeans_ss.fit(data_scaled)\n",
    "        SSE.append(kmeans_ss.inertia_)\n",
    "\n",
    "    # converting the results into a dataframe and plotting them\n",
    "    frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the KMeans object based on number discovered above\n",
    "k = 5\n",
    "\n",
    "if (h2o4gpu_enabled == True):\n",
    "    import h2o4gpu\n",
    "    kmeans = h2o4gpu.KMeans(n_gpus=1, n_clusters = k, init='k-means++', max_iter = 20, random_state = 42, backend=h2o4gpu)\n",
    "else:\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = sklearn.cluster.KMeans(n_clusters = k, init='k-means++', max_iter = 20, random_state = 42)\n",
    "\n",
    "# Creating the model by passing our non-outlier data to Kmeans object\n",
    "trained_model = kmeans.fit(data_scaled)\n",
    "\n",
    "# Retrieving centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Labels of KMeans clusters change for every iteration. \n",
    "# We need to preserve these lables for runs with multiple/incremental input data sets.\n",
    "# Creating a hack to attempt to preserve their identities.\n",
    "# Calculating the arithmetic sum of all values in each centroid. This is done to fix labels for each training iteration\n",
    "# Assumption is that bigger the values in a centroid, higher the original feature values. This assumption is true only for \n",
    "#        data set and our features. We need to definitely confirm/verify this assumption.\n",
    "combinedCentroids = centroids[trained_model.labels_].sum(axis=1)\n",
    "\n",
    "# adding column with combined centroid values to the original dataframe \n",
    "ml_commits_nout['center'] = combinedCentroids\n",
    "#print(combinedCentroids)\n",
    "\n",
    "# Creating a dictionary with combined centroid values and target cluster labels\n",
    "unique_centroids = np.unique(combinedCentroids).tolist()\n",
    "cluster_labels = np.arange(k).tolist()\n",
    "cluster_dict = dict(zip(unique_centroids,cluster_labels))\n",
    "#print(g)\n",
    "ml_commits_nout['fixed_cluster'] = ml_commits_nout['center'].map(cluster_dict)\n",
    "#ml_commits_nout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from numpy import savetxt\n",
    "\n",
    "# Look at the centroids\n",
    "centroids[trained_model.labels_]\n",
    "\n",
    "# Look at the values of inverted scaling of centroids for sanity\n",
    "real_centroids = scaler.inverse_transform(centroids)\n",
    "\n",
    "# Write these to dataframe\n",
    "real_centroids_dataFrame = pd.DataFrame(real_centroids, columns=['total_changed','rated_complexity',\n",
    "                            'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity'])\n",
    "\n",
    "# Add a cloumn for summing all coloumns\n",
    "real_centroids_dataFrame['Sum_centroids'] = real_centroids_dataFrame.sum(axis = 1)\n",
    "\n",
    "#You can write it out as csv if required.\n",
    "real_centroids_dataFrame.to_csv('/home/kc/junk/totalCommits_centroids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting each cluster\n",
    "ml_clustered = ml_commits_nout.groupby(['fixed_cluster'],as_index=True).count()\n",
    "ml_clustered['hash']\n",
    "ml_commits_nout[ml_commits_nout['fixed_cluster']==4].head(5)\n",
    "ml_commits_nout.to_csv('/home/kc/junk/total_commits_clusters_202007281715.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_commits_nout.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "from xgboost import XGBClassifier\n",
    "if (h2o4gpu_enabled == True):\n",
    "    import h2o4gpu as sklearn\n",
    "else:\n",
    "    import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Remove text fields\n",
    "ml_commits_nout_numeric_xg = ml_commits_nout.drop(columns = ['Author','hash','center'])\n",
    "\n",
    "# Prepare the 'X' and 'Y' for the model\n",
    "X_ml_commits_nout_numeric_xg = ml_commits_nout_numeric_xg.drop(columns = ['fixed_cluster'])\n",
    "Y_ml_commits_nout_numeric_xg = ml_commits_nout_numeric_xg['fixed_cluster']\n",
    "\n",
    "# Split the data for 'Training' and 'Testing' datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ml_commits_nout_numeric_xg, Y_ml_commits_nout_numeric_xg, random_state=7)\n",
    "\n",
    "# Instantiate the model and train it\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction on Test set.\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Check accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (h2o4gpu_enabled == True):\n",
    "    from h2o4gpu import metrics\n",
    "else:\n",
    "    from sklearn import metrics\n",
    "\n",
    "metrics.silhouette_score(data_scaled, trained_model.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the XGBoost model\n",
    "import pickle\n",
    "\n",
    "filename = '/home/kc/Projects/data_files/finalized_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "20200724_h2o4gpu",
   "language": "python",
   "name": "20200724_h2o4gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
