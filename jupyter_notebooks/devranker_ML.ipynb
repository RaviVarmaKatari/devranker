{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DEBUG flag with values from '0' to '5'. Default is '0' which is OFF. \n",
    "# Use this cautiously - we are not validating for this\n",
    "\n",
    "DEBUG = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use h2o4gpu if you have it installed\n",
    "# Add a flag with Default as False.\n",
    "h2o4gpu_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data files and concantenate into a dataframe\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "training_data_files_path = r'/home/kc/Projects/data_files/Training_data_from_public_git/'                     \n",
    "all_files = glob.glob(os.path.join(training_data_files_path, \"*.csv\"))     # advisable to use os.path.join as this makes concatenation OS independent\n",
    "\n",
    "df_from_each_file = (pd.read_csv(f) for f in all_files)\n",
    "total_commits = pd.concat(df_from_each_file, ignore_index=True)\n",
    "                             \n",
    "if DEBUG >=2:\n",
    "    print(\"Total No. of rows: \", total_commits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is temporary. We need to remove this cell for the release. i.e. when we move this logic to getData\n",
    "# Create a coloumn 'file_ext' which is the file 'type'\n",
    "total_commits['file_ext'] = total_commits['file_path'].\\\n",
    "                                                    apply(lambda x:pathlib.Path(str(x)).suffix).\\\n",
    "                                                    apply(lambda x:re.split(r\"[^a-zA-Z0-9\\s\\++\\_\\-]\",x)[-1])\n",
    "\n",
    "# For files without any extension, mark 'file_ext' as \"NoExt\" \n",
    "total_commits.file_ext = total_commits.file_ext.replace(r'^\\s*$', 'NoExt', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating various features for each modification\n",
    "\n",
    "# Note: We will add a prefix \"feature\" to variables to create the features for the ML model.\n",
    "\n",
    "# From documentation: https://pydriller.readthedocs.io/en/latest/commit.html\n",
    "#     dmm_unit_size (float): DMM metric value for the unit size property.\n",
    "#     dmm_unit_complexity (float): DMM metric value for the unit complexity property.\n",
    "#     dmm_unit_interfacing (float): DMM metric value for the unit interfacing property.\n",
    "\n",
    "# https://pydriller.readthedocs.io/en/latest/modifications.html\n",
    "#     complexity: Cyclomatic Complexity of the file\n",
    "#     changed_methods: subset of _methods_ containing only the changed methods.\n",
    "\n",
    "# Here is more about dmm: https://pydriller.readthedocs.io/en/latest/deltamaintainability.html\n",
    "#     The delta-maintainability metric is the proportion of low-risk change in a commit. \n",
    "#     The resulting value ranges from 0.0 (all changes are risky) to 1.0 (all changes are low risk). \n",
    "#     It rewards making methods better, and penalizes making things worse.\n",
    "\n",
    "# \"total lines changed\" is important but can be very misleading metric of a commit. \n",
    "# We will create a feature which blunts the weight of nloc\n",
    "# Total number of lines changed\n",
    "total_commits['total_changed'] = total_commits['number_lines_added'] + total_commits['number_lines_removed']\n",
    "total_commits['feature_total_changed'] = (total_commits['total_changed'] ** 0.7)\n",
    "\n",
    "# Fraction of lines changed per total numbe of lines in file\n",
    "# We need to account for the fact that new files added with have existing size as '0'.\n",
    "#           and divide by '0' is indeterminate\n",
    "total_commits['file_number_loc'].loc[total_commits['file_number_loc'] == 0] = total_commits['total_changed']\n",
    "total_commits['ratio_changed'] = total_commits['total_changed'] / total_commits['file_number_loc']\n",
    "# We can change above logic if we use mod.change_type. Maybe in a future version.\n",
    "\n",
    "# 'complexity' is given for the *WHOLE* file. We need to scale/weigh it for only the changed lines.\n",
    "#     Let us weight it by 2 variables:\n",
    "#         \"ratio changed\" AND\n",
    "#         a feature related to 'size' of file i.e. we are making an assumption that larger files are \n",
    "#         more difficult to change but we need to taper this off too. Let us go with cube root of size \n",
    "total_commits['feature_rated_complexity'] = total_commits['ratio_changed'] * total_commits['file_complexity'] * \\\n",
    "                                                (total_commits['total_changed'] ** 0.3)\n",
    "\n",
    "# dmm values are given for the commit. We need to scale them for individual commits.\n",
    "# We should weight this by \"changed_methods\" but we missed mining this. We will add this later.\n",
    "# When adding \"changed methods\", We will do Something like (changed_methods ** 1.5) to reflect importance\n",
    "#       of adding and deleting methods. We can then change to use (total_changed ** 0.3)\n",
    "total_commits['feature_dmm_size'] = (total_commits['total_changed'] ** 0.5) * total_commits['dmm_unit_size']\n",
    "total_commits['feature_dmm_unit_complexity'] = (total_commits['total_changed'] ** 0.5) * \\\n",
    "                                                    total_commits['dmm_unit_complexity']\n",
    "total_commits['feature_dmm_unit_interfacing'] = (total_commits['total_changed'] ** 0.5) * \\\n",
    "                                                    total_commits['dmm_unit_interfacing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Remove outliers. \n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def filter_outliers(data_frame):\n",
    "    \n",
    "    # Calculate z_scores and if zscore is greater than '3', then its an outlier\n",
    "    # https://pbpython.com/pandas_dtypes.html\n",
    "    \n",
    "    # Get non-Outliers: \n",
    "    data_frame_non_outliers = \\\n",
    "    data_frame[(np.abs(stats.zscore(data_frame.select_dtypes(exclude=['object', 'bool'], \\\n",
    "                                                             include=['int64', 'float64']))) < 3).all(axis=1)]\n",
    "\n",
    "    # Collect outliers\n",
    "    data_frame_outliers = \\\n",
    "    data_frame[~(np.abs(stats.zscore(data_frame.select_dtypes(exclude=['object', 'bool'], \\\n",
    "                                                             include=['int64', 'float64']))) < 3).all(axis=1)]\n",
    "    \n",
    "    return data_frame_non_outliers, data_frame_outliers\n",
    "    #return data_frame, data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare frames for each file type extension\n",
    "\n",
    "def prepare_frame(total_commits, file_ext):\n",
    "    \n",
    "    #Filter the mods based on file type extension\n",
    "    file_ext_commits = total_commits[total_commits['file_ext']==file_ext]\n",
    "\n",
    "    ml_commits = file_ext_commits[['hash','Author','feature_total_changed','feature_rated_complexity',\n",
    "                                'feature_dmm_unit_complexity','feature_dmm_size','feature_dmm_unit_interfacing',\n",
    "                                  'language_supported']]\n",
    "\n",
    "    # Resetting the frame's index. It is required to retain the integrity of the frame\n",
    "    ml_commits = ml_commits.reset_index().drop(columns = 'index')\n",
    "\n",
    "    # Temporarily dropping text columns for numeric processing\n",
    "    ml_commits_noText = ml_commits.drop(columns = ['Author','hash','language_supported'])\n",
    "\n",
    "    # Explicitely converting fields to numeric types and filling the NaNs with zeros\n",
    "    ml_commits_numeric = ml_commits_noText.apply(pd.to_numeric,errors ='coerce').fillna(0)\n",
    "\n",
    "    # Adding the Author column back to create a 'total' data frame\n",
    "    ml_commits_all_coloumns = ml_commits_numeric.copy()\n",
    "    ml_commits_all_coloumns['Author'] = ml_commits['Author']\n",
    "    ml_commits_all_coloumns['hash'] = ml_commits['hash']\n",
    "    ml_commits_all_coloumns['language_supported'] = ml_commits['language_supported']\n",
    "    \n",
    "    return ml_commits_all_coloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale data in the frame\n",
    "\n",
    "def scale_frame(data_frame):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    if (h2o4gpu_enabled == True):\n",
    "        from h2o4gpu.preprocessing import MinMaxScaler\n",
    "    else:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    # Use minMax scaler since this does not distort\n",
    "    # https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
    "    scaler = MinMaxScaler()\n",
    "    data_frame_numeric = data_frame.drop(columns = ['Author','hash','language_supported'])\n",
    "    scaled_data_frame = scaler.fit_transform(data_frame_numeric)\n",
    "    \n",
    "    return scaled_data_frame, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster(scaled_file_ext_frame, file_ext, file_ext_frame, scaler, k, gmm_models_folder, \n",
    "                   centroids_folder):\n",
    "    \n",
    "    # We are currently using GMM from sklearn. We need to get the GPU version of GMM.\n",
    "    # https://pypi.org/project/pycave/\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "\n",
    "    # Initializing the Gaussian mixture model \n",
    "    mix = GaussianMixture(n_components=k, random_state=42)\n",
    "\n",
    "    # Learning the Gaussian mixture model from data   \n",
    "    mix.fit(scaled_file_ext_frame)\n",
    "\n",
    "    # Saving the parameters of Gaussian mixture model in a file\n",
    "    import pickle\n",
    "    vfilename = gmm_models_folder+file_ext+'_gmm_model_pickle.sav'\n",
    "    pickle.dump(mix, open(vfilename, 'wb'))\n",
    "\n",
    "    # Predicting the cluster labels of the data for the Gaussian mixture model\n",
    "    cluster_frame = pd.DataFrame(scaled_file_ext_frame)\n",
    "    gmm_hash_clusters = mix.predict(cluster_frame)\n",
    "\n",
    "    # Collecting the mean of the Gaussian mixture model in 'gmmcentroids'\n",
    "    gmm_centroids = mix.means_\n",
    "    gmm_covariances = mix.covariances_\n",
    "    combinedCentroids = gmm_centroids[gmm_hash_clusters].sum(axis=1)\n",
    "\n",
    "    # Converting the input data series into pan\n",
    "    file_ext_frame['Cluster'] = gmm_hash_clusters\n",
    "    real_centroids = scaler.inverse_transform(gmm_centroids)\n",
    "\n",
    "    # Write these to dataframe\n",
    "    real_centroids_dataFrame = pd.DataFrame(real_centroids, columns=['feature_total_changed',\n",
    "                                                                     'feature_rated_complexity',\n",
    "                                'feature_dmm_unit_complexity','feature_dmm_size','feature_dmm_unit_interfacing'])\n",
    "\n",
    "    # Add a cloumn for summing all centroids \"This is the value of the individual clusters\"\n",
    "    real_centroids_dataFrame['Sum_centroids'] = real_centroids_dataFrame.sum(axis = 1)\n",
    "    \n",
    "    # Save centroids of the clusters to a file for audit\n",
    "    centroid_file = centroids_folder+file_ext+'centroids.csv'\n",
    "    real_centroids_dataFrame.to_csv(centroid_file)\n",
    "    \n",
    "    return file_ext_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boost_model(file_ext_frame, file_ext, xgboost_models_folder):\n",
    "    \n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    if (h2o4gpu_enabled == True):\n",
    "        import h2o4gpu as sklearn\n",
    "        # We assume that if h2o4gpu is enabled then, GPU is available and we can use xgboost on GPU.\n",
    "        # Instantiate the xgboost model with relevant params\n",
    "        # https://gist.github.com/shreyasbapat/89c6d6e09ff3f763e21ea68f98d74f84\n",
    "        # https://xgboost.readthedocs.io/en/latest/gpu/index.html\n",
    "        xgboost_model = XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n",
    "    else:\n",
    "        import sklearn\n",
    "        xgboost_model = XGBClassifier()\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import pickle\n",
    "\n",
    "    # Remove text fields before numeric manipulations\n",
    "    file_ext_frame_numeric_xg = file_ext_frame.drop(columns=['Author','hash','language_supported'])\n",
    "\n",
    "    # Prepare the 'X' and 'Y' for the model\n",
    "    X_file_ext_frame_numeric_xg = file_ext_frame_numeric_xg.drop(columns = ['Cluster'])\n",
    "    Y_file_ext_frame_numeric_xg = file_ext_frame_numeric_xg['Cluster']\n",
    "\n",
    "    # Split the data for 'Training' and 'Testing' datasets\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "            train_test_split(X_file_ext_frame_numeric_xg, Y_file_ext_frame_numeric_xg, random_state=7)\n",
    "\n",
    "\n",
    "    # Training the xgboost classifier\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the class labels of test data for xgboost classifier\n",
    "    y_pred = xgboost_model.predict(X_test)\n",
    "\n",
    "    # check accuracy\n",
    "    if DEBUG >= 1:\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(file_ext+'_size: ', file_ext_frame.shape[0])\n",
    "        print(file_ext+'_accuracy: ', accuracy)\n",
    "    \n",
    "    # Save the model to a file\n",
    "    filename = xgboost_models_folder+file_ext+'_xgboost_model.sav'\n",
    "    pickle.dump(xgboost_model, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "# Print for Debugging \n",
    "if DEBUG >=1:\n",
    "    print('No. of file extensions: ', len(total_commits['file_ext'].unique()))\n",
    "\n",
    "# Prepare a list of all unique file extensions\n",
    "unique_extensions = total_commits['file_ext'].unique()\n",
    "\n",
    "# Folder to save models and centroids\n",
    "gmm_models_folder = '/home/kc/Projects/data_files/sav_files/gmm_sav/'\n",
    "centroids_folder = '/home/kc/Projects/data_files/sav_files/centroids/'\n",
    "xgboost_models_folder = '/home/kc/Projects/data_files/sav_files/xgboost_sav/'\n",
    "        \n",
    "# Remove all previous models \n",
    "#for folders in [gmm_models_folder, centroids_folder, xgboost_models_folder]:\n",
    "#    for file in glob.glob(folders+'*'):\n",
    "#        os.remove(file)\n",
    "\n",
    "for file_ext in unique_extensions:\n",
    "    \n",
    "    # Set number of clusters\n",
    "    k=5\n",
    "    \n",
    "    # Extract data frame for the specific file type extension\n",
    "    file_ext_frame = prepare_frame(total_commits, file_ext)\n",
    "   \n",
    "    # We need a good sample size for accurate results. \n",
    "    #    We will ignore'file_ext' if it has less than 50 rows \n",
    "    if file_ext_frame.shape[0] < 50:\n",
    "        print('ignoring file type: ', file_ext, '(Not enough rows)')\n",
    "        continue\n",
    "        \n",
    "    # If file_ext is not supported by 'lizard', then ignore 'file_ext' and proceed to next\n",
    "    if file_ext_frame.at[0,'language_supported'] == False:\n",
    "        if DEBUG >= 2:\n",
    "            print('Ignoring unsupported file extention: ', file_ext)\n",
    "        continue\n",
    "        \n",
    "    # Remove outliers from the frame (only if you reasonable amount of data points) \n",
    "    file_ext_frame_non_outliers, file_ext_frame_outliers = \\\n",
    "                                    filter_outliers(file_ext_frame)\n",
    "    if DEBUG >= 2:\n",
    "        print('file_ext: ', file_ext, ', no. of non-outlier rows: ', file_ext_frame_non_outliers.shape[0])\n",
    "        print('file_ext: ', file_ext, ', no. of outlier rows: ', file_ext_frame_outliers.shape[0])\n",
    "    \n",
    "    # Scale the data    \n",
    "    # There is a peculiar behaviour here. For lot of file_ext (= 'md', 'html', etc,) All data points are \n",
    "    #     showing up as outliers. This is very funny. We should probably restrict our processing to files\n",
    "    #     supported by 'lizard/pydriller'. \n",
    "    # We are forced to check that we have at least 1 non_outlier.\n",
    "    if file_ext_frame_non_outliers.shape[0] >= 1:\n",
    "        scaled_file_ext_frame, scaler = scale_frame(file_ext_frame_non_outliers)\n",
    "        if DEBUG >= 2:\n",
    "            print('rows in scaled_file_ext_frame: ', scaled_file_ext_frame.shape[0])\n",
    "        \n",
    "        # Create the actual clusters from the data.\n",
    "        clustered_frame = create_cluster(scaled_file_ext_frame, file_ext, file_ext_frame_non_outliers, scaler, \n",
    "                                            k, gmm_models_folder, centroids_folder)\n",
    "\n",
    "        # Train xgboost model for each extension\n",
    "        create_boost_model(clustered_frame, file_ext, xgboost_models_folder)\n",
    "    else:\n",
    "        if DEBUG >= 2:\n",
    "            print('ignoring file type: ', file_ext, '(All rows are outliers)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "20200724_h2o4gpu",
   "language": "python",
   "name": "20200724_h2o4gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
