{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data files\n",
    "import pandas as pd\n",
    "\n",
    "total_commits=pd.read_csv('/home/kc/Projects/data_files/vscode.csv')\n",
    "#total_commits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating various features for each modification\n",
    "\n",
    "total_commits['total_changed'] = total_commits['lines_added']+total_commits['lines_removed']\n",
    "total_commits['size'].loc[total_commits['size'] == 0] = total_commits['total_changed']\n",
    "total_commits['ratio_changed'] = total_commits['total_changed']/total_commits['size']\n",
    "total_commits['rated_complexity'] = total_commits['ratio_changed']*total_commits['complexity']\n",
    "total_commits['total_dmm_size'] = total_commits['total_changed']*total_commits['dmm_unit_size']\n",
    "total_commits['total_dmm_unit_complexity'] = total_commits['total_changed']*total_commits['dmm_unit_complexity']\n",
    "total_commits['total_dmm_unit_interfacing'] = total_commits['total_changed']*total_commits['dmm_unit_interfacing']\n",
    "total_commits['scaled_rated_complexity']=total_commits['rated_complexity']*total_commits['no._of_mod_files']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data. ML requires the data to be converted to numericals\n",
    "#ml_commits = total_commits[['hash','Author', 'no._of_mod_files', 'dmm_unit_size',\n",
    "#       'dmm_unit_complexity', 'dmm_unit_interfacing', 'complexity', 'functions', 'lines_added', 'lines_removed', \n",
    "#       'tokens', 'type']]\n",
    "\n",
    "ml_commits = total_commits[['hash','Author','total_changed','ratio_changed','rated_complexity',\n",
    "                            'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity']]\n",
    "\n",
    "# Resetting the frame's index. It is required to retain the integrity of the frame\n",
    "ml_commits = ml_commits.reset_index().drop(columns = 'index')\n",
    "\n",
    "# Temporarily dropping text columns for numeric processing\n",
    "ml_commits_noText = ml_commits.drop(columns = ['Author','hash'])\n",
    "\n",
    "# Explicitely converting fields to numeric types and filling the NaNs with zeros\n",
    "ml_commits_numeric = ml_commits_noText.apply(pd.to_numeric,errors ='coerce').fillna(0)\n",
    "\n",
    "# Adding the Author column back to create a 'total' data frame\n",
    "ml_commits_total = ml_commits_numeric.copy()\n",
    "ml_commits_total['Author'] = ml_commits['Author']\n",
    "ml_commits_total['hash'] = ml_commits['hash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers. \n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Calculate z_scores (and if zscore is greater than '3', then its an outlier) and collect normal subset.\n",
    "ml_commits_nout = ml_commits_total[(np.abs(stats.zscore(ml_commits_total.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "ml_commits_nout.to_csv('/home/kc/junk/vscode_nout.csv')\n",
    "\n",
    "# Collect outliers\n",
    "ml_commits_out = ml_commits_total[~(np.abs(stats.zscore(ml_commits_total.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "ml_commits_out.to_csv('/home/kc/junk/vscode_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying scaler to regular data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Use minMax scaler since this does not distort\n",
    "# https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
    "scaler = MinMaxScaler()\n",
    "ml_commits_nout_numeric = ml_commits_nout.drop(columns = ['Author','hash'])\n",
    "data_scaled = scaler.fit_transform(ml_commits_nout_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and figure out how many clusters are optimum\n",
    "# Plot the inertia curve to find the 'elbow'\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "SSE = []\n",
    "\n",
    "for cluster in range(1,20):\n",
    "    kmeans_ss = KMeans(n_clusters = cluster, init='k-means++', random_state = 42)\n",
    "    kmeans_ss.fit(data_scaled)\n",
    "    SSE.append(kmeans_ss.inertia_)\n",
    "\n",
    "    # converting the results into a dataframe and plotting them\n",
    "frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the KMeans object based on number discovered above\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters = k, init='k-means++', max_iter = 20, random_state = 42)\n",
    "\n",
    "# Creating the model by passing our non-outlier data to Kmeans object\n",
    "trained_model = kmeans.fit(data_scaled)\n",
    "\n",
    "# Retrieving centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "# Labels of KMeans clusters change for every iteration. \n",
    "# We need to preserve these lables for runs with multiple/incremental input data sets.\n",
    "# Creating a hack to attempt to preserve their identities.\n",
    "# Calculating the arithmetic sum of all values in each centroid. This is done to fix labels for each training iteration\n",
    "# Assumption is that bigger the values in a centroid, higher the original feature values. This assumption is true only for \n",
    "#        data set and our features. We need to definitely confirm/verify this assumption.\n",
    "combinedCentroids = centroids[trained_model.labels_].sum(axis=1)\n",
    "\n",
    "# adding column with combined centroid values to the original dataframe \n",
    "ml_commits_nout['center'] = combinedCentroids\n",
    "#print(combinedCentroids)\n",
    "\n",
    "# Creating a dictionary with combined centroid values and target cluster labels\n",
    "unique_centroids = np.unique(combinedCentroids).tolist()\n",
    "cluster_labels = np.arange(k).tolist()\n",
    "cluster_dict = dict(zip(unique_centroids,cluster_labels))\n",
    "#print(g)\n",
    "ml_commits_nout['fixed_cluster'] = ml_commits_nout['center'].map(cluster_dict)\n",
    "ml_commits_nout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_clustered = ml_commits_nout.groupby(['fixed_cluster'],as_index=True).count()\n",
    "ml_clustered['hash']\n",
    "ml_commits_nout[ml_commits_nout['fixed_cluster']==4].head(5)\n",
    "ml_commits_nout.to_csv('/home/kc/junk/react_clusters_202007212010.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h2o4gpu",
   "language": "python",
   "name": "h2o4gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
