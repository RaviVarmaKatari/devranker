{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DEBUG flag with values from '0' to '5'. Default is '0' which is OFF. \n",
    "# Use this cautiously - we are not validating for this\n",
    "\n",
    "DEBUG = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data files\n",
    "import pandas as pd\n",
    "\n",
    "tensorflow_commits = pd.read_csv('/home/kc/Projects/data_files/tensorflow.csv')\n",
    "vscode_commits=pd.read_csv('/home/kc/Projects/data_files/vscode.csv')\n",
    "react_commits=pd.read_csv('/home/kc/Projects/data_files/react-native.csv')\n",
    "\n",
    "total_commits=tensorflow_commits.append(vscode_commits, ignore_index=True)\n",
    "total_commits=total_commits.append(react_commits, ignore_index=True)\n",
    "                             \n",
    "\n",
    "if DEBUG >=1:\n",
    "    print(total_commits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use h2o4gpu if you have it installed\n",
    "\n",
    "# Add a flag with Default as False. Don't change this unless your kernel uses h2o4gpu.\n",
    "h2o4gpu_enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating various features for each modification\n",
    "\n",
    "# Total number of lines changed\n",
    "total_commits['total_changed'] = total_commits['lines_added'] + total_commits['lines_removed']\n",
    "\n",
    "# Fraction of lines changed per total numbe of lines in file\n",
    "# We need to account for the fact that new files added with have existing size as '0' and divide by '0' is indeterminate\n",
    "total_commits['size'].loc[total_commits['size'] == 0] = total_commits['total_changed']\n",
    "total_commits['ratio_changed'] = total_commits['total_changed'] / total_commits['size']\n",
    "\n",
    "# Need to weigh the complexity by quantum of change. \n",
    "total_commits['rated_complexity'] = total_commits['ratio_changed'] * total_commits['complexity'] * total_commits['total_changed']\n",
    "\n",
    "# weighing the dmm params by the total changed lines\n",
    "total_commits['total_dmm_size'] = total_commits['total_changed'] * total_commits['dmm_unit_size']\n",
    "total_commits['total_dmm_unit_complexity'] = total_commits['total_changed'] * total_commits['dmm_unit_complexity']\n",
    "total_commits['total_dmm_unit_interfacing'] = total_commits['total_changed'] * total_commits['dmm_unit_interfacing']\n",
    "\n",
    "# We picked the sqrt of no_of_mod_files to reduce weightage of this feature\n",
    "total_commits['scaled_rated_complexity']=total_commits['rated_complexity'] * (total_commits['no._of_mod_files'] ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Remove outliers. \n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def filter_outliers(data_frame):\n",
    "    \n",
    "    # Calculate z_scores and if zscore is greater than '3', then its an outlier\n",
    "    \n",
    "    # Get non-Outliers: \n",
    "    data_frame_non_outliers = data_frame[(np.abs(stats.zscore(data_frame.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "\n",
    "    # Collect outliers\n",
    "    data_frame_outliers = data_frame[~(np.abs(stats.zscore(data_frame.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "    \n",
    "    return data_frame_non_outliers, data_frame_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare frames for each file type extension\n",
    "\n",
    "def prepare_frame(total_commits, file_ext):\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    #Filter the mods based on file type extension\n",
    "    file_ext_commits = total_commits[total_commits['file_ext']==file_ext]\n",
    "\n",
    "    ml_commits = file_ext_commits[['hash','Author','total_changed','rated_complexity',\n",
    "                                'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity']]\n",
    "\n",
    "    # Resetting the frame's index. It is required to retain the integrity of the frame\n",
    "    ml_commits = ml_commits.reset_index().drop(columns = 'index')\n",
    "\n",
    "    # Temporarily dropping text columns for numeric processing\n",
    "    ml_commits_noText = ml_commits.drop(columns = ['Author','hash'])\n",
    "\n",
    "    # Explicitely converting fields to numeric types and filling the NaNs with zeros\n",
    "    ml_commits_numeric = ml_commits_noText.apply(pd.to_numeric,errors ='coerce').fillna(0)\n",
    "\n",
    "    # Adding the Author column back to create a 'total' data frame\n",
    "    ml_commits_all_coloumns = ml_commits_numeric.copy()\n",
    "    ml_commits_all_coloumns['Author'] = ml_commits['Author']\n",
    "    ml_commits_all_coloumns['hash'] = ml_commits['hash']\n",
    "    \n",
    "    return ml_commits_all_coloumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scale data in the frame\n",
    "\n",
    "def scale_frame(data_frame):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    if (h2o4gpu_enabled == True):\n",
    "        from h2o4gpu.preprocessing import MinMaxScaler\n",
    "    else:\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    # Use minMax scaler since this does not distort\n",
    "    # https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
    "    scaler = MinMaxScaler()\n",
    "    data_frame_numeric = data_frame.drop(columns = ['Author','hash'])\n",
    "    scaled_data_frame = scaler.fit_transform(data_frame_numeric)\n",
    "    \n",
    "    return scaled_data_frame, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster(scaled_file_ext_frame, file_ext, file_ext_frame, scaler, k):\n",
    "    \n",
    "    # We are currently using GMM from sklearn. We need to get the GPU version of GMM.\n",
    "    # https://pypi.org/project/pycave/\n",
    "    from sklearn.mixture import GaussianMixture\n",
    "\n",
    "    # Initializing the Gaussian mixture model \n",
    "    mix = GaussianMixture(n_components=k, random_state=42)\n",
    "\n",
    "    # Learning the Gaussian mixture model from data   \n",
    "    mix.fit(scaled_file_ext_frame)\n",
    "\n",
    "    # Saving the parameters of Gaussian mixture model in a file\n",
    "    import pickle\n",
    "    vfilename = '/home/kc/Projects/data_files/sav_files/'+file_ext+'_gmm_pickle.sav'\n",
    "    pickle.dump(mix, open(vfilename, 'wb'))\n",
    "\n",
    "    # Predicting the cluster labels of the data for the Gaussian mixture model\n",
    "    cluster_frame = pd.DataFrame(scaled_file_ext_frame)\n",
    "    gmm_hash_clusters = mix.predict(cluster_frame)\n",
    "\n",
    "    # Collecting the mean of the Gaussian mixture model in 'gmmcentroids'\n",
    "    gmm_centroids = mix.means_\n",
    "    gmm_covariances = mix.covariances_\n",
    "    combinedCentroids = gmm_centroids[gmm_hash_clusters].sum(axis=1)\n",
    "\n",
    "    # Converting the input data series into pan\n",
    "    file_ext_frame['Cluster'] = gmm_hash_clusters\n",
    "    real_centroids = scaler.inverse_transform(gmm_centroids)\n",
    "\n",
    "    # Write these to dataframe\n",
    "    real_centroids_dataFrame = pd.DataFrame(real_centroids, columns=['total_changed','rated_complexity',\n",
    "                                'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity'])\n",
    "\n",
    "    # Add a cloumn for summing all centroids \"This is the value of the individual clusters\"\n",
    "    real_centroids_dataFrame['Sum_centroids'] = real_centroids_dataFrame.sum(axis = 1)\n",
    "    \n",
    "    # Save centroids of the clusters to a file for audit\n",
    "    centroid_file = '/home/kc/Projects/data_files/sav_files/'+file_ext+'centroids.csv'\n",
    "    real_centroids_dataFrame.to_csv(centroid_file)\n",
    "    \n",
    "    return file_ext_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_boost_model(file_ext_frame, file_ext, folder):\n",
    "    \n",
    "    from xgboost import XGBClassifier\n",
    "    \n",
    "    if (h2o4gpu_enabled == True):\n",
    "        import h2o4gpu as sklearn\n",
    "        # We assume that if h2o4gpu is enabled then, GPU is available and we can use xgboost on GPU.\n",
    "        # Instantiate the xgboost model with relevant params\n",
    "        # https://gist.github.com/shreyasbapat/89c6d6e09ff3f763e21ea68f98d74f84\n",
    "        # https://xgboost.readthedocs.io/en/latest/gpu/index.html\n",
    "        xgboost_model = XGBClassifier(tree_method='gpu_hist', gpu_id=0)\n",
    "    else:\n",
    "        import sklearn\n",
    "        xgboost_model = XGBClassifier()\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    import pickle\n",
    "\n",
    "    # Remove text fields before numeric manipulations\n",
    "    file_ext_frame_numeric_xg = file_ext_frame.drop(columns=['Author','hash'])\n",
    "\n",
    "    # Prepare the 'X' and 'Y' for the model\n",
    "    X_file_ext_frame_numeric_xg = file_ext_frame_numeric_xg.drop(columns = ['Cluster'])\n",
    "    Y_file_ext_frame_numeric_xg = file_ext_frame_numeric_xg['Cluster']\n",
    "\n",
    "    # Split the data for 'Training' and 'Testing' datasets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_file_ext_frame_numeric_xg, Y_file_ext_frame_numeric_xg, random_state=7)\n",
    "\n",
    "\n",
    "    # Training the xgboost classifier\n",
    "    xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predicting the class labels of test data for xgboost classifier\n",
    "    y_pred = xgboost_model.predict(X_test)\n",
    "\n",
    "    # check accuracy\n",
    "    if DEBUG >= 1:\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(file_ext+'_size: ', file_ext_frame.shape[0])\n",
    "        print(file_ext+'_accuracy: ', accuracy)\n",
    "    \n",
    "    # Save the model to a file\n",
    "    filename = folder+file_ext+'_finalized_model.sav'\n",
    "    pickle.dump(xgboost_model, open(filename, 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import glob\n",
    "\n",
    "\n",
    "# Create a coloumn 'file_ext' which is the file 'type'\n",
    "total_commits['file_ext'] = total_commits['file_path'].apply(lambda x:pathlib.Path(str(x)).suffix).apply(lambda x:re.split(r\"[^a-zA-Z0-9\\s\\++\\_\\-]\",x)[-1])\n",
    "\n",
    "# For files without any extension, mark 'file_ext' as \"NoExt\" \n",
    "total_commits.file_ext = total_commits.file_ext.replace(r'^\\s*$', 'NoExt', regex=True)\n",
    "\n",
    "# Print for Debugging \n",
    "if DEBUG >=1:\n",
    "    print(len(total_commits['file_ext'].unique()))\n",
    "\n",
    "# Prepare a list of all unique file extensions\n",
    "unique_extensions = total_commits['file_ext'].unique()\n",
    "\n",
    "# What does the below line do ? Remove all previous models? \n",
    "pickled_files = glob.glob('/home/kc/Projects/data_files/sav_files/*.sav')\n",
    "for f in pickled_files:\n",
    "    os.remove(f)\n",
    "\n",
    "# For every file extension: prepare the data frame, create cluster, train xgBoost model and save it.\n",
    "# We should change this to only those extensions supported by lizard/pydriller\n",
    "for file_ext in unique_extensions:\n",
    "    \n",
    "    # Set number of clusters\n",
    "    k=5\n",
    "    \n",
    "    # Extract data frame for the specific file type extension\n",
    "    file_ext_frame = prepare_frame(total_commits, file_ext)\n",
    "    \n",
    "    # Remove outliers from the frame (only if you reasonable amount of data points) \n",
    "    if file_ext_frame.shape[0] > 10:\n",
    "        file_ext_frame_non_outliers, file_ext_frame_outliers = filter_outliers(file_ext_frame)\n",
    "    else :\n",
    "        file_ext_frame_non_outliers = file_ext_frame\n",
    "        #file_ext_frame_outliers = NULL\n",
    "\n",
    "    # We need at least 5 data points to ensure that we get at least 5 clusters\n",
    "    # Get the number of rows\n",
    "    count_row = file_ext_frame_non_outliers.shape[0]\n",
    "    \n",
    "    # Scale the data if you have more than one row\n",
    "    if count_row>1:\n",
    "        scaled_file_ext_frame, scaler = scale_frame(file_ext_frame_non_outliers)\n",
    "        \n",
    "        # If we have less than 5 rows, we cannot have 5 clusters, we have to reduce clusters\n",
    "        #      to handle this boundary case.\n",
    "        if count_row<5:\n",
    "            k = count_row\n",
    "         \n",
    "        # Create the actual clusters from the data.\n",
    "        clustered_frame = create_cluster(scaled_file_ext_frame, file_ext, file_ext_frame_non_outliers, scaler, k)\n",
    "        \n",
    "        # Train xgboost model for each extension\n",
    "        create_boost_model(clustered_frame, file_ext, '/home/kc/Projects/data_files/sav_files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix heatmap visualization as a sanity check\n",
    "\n",
    "if DEBUG >=4:\n",
    "\n",
    "    import seaborn as sns\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.set(font_scale=1.5)\n",
    "\n",
    "    # We need as many as cols as we have features\n",
    "    cols=['1','2','3','4','5','6']\n",
    "\n",
    "    # visualization for cluster number\n",
    "    cluster_number = 2\n",
    "    hm = sns.heatmap(gmm_covariances[cluster_number,:,:],\n",
    "                     cbar=True,\n",
    "                     annot=True,\n",
    "                     square=True,\n",
    "                     fmt='.5f',\n",
    "                     annot_kws={'size': 12},\n",
    "                     yticklabels=cols,\n",
    "                     xticklabels=cols)\n",
    "\n",
    "    plt.title('Covariance matrix of the cluster showing correlation coefficients')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG >=4:\n",
    "    if (h2o4gpu_enabled == True):\n",
    "        from h2o4gpu import metrics\n",
    "    else:\n",
    "        from sklearn import metrics\n",
    "\n",
    "    metrics.silhouette_score(data_scaled, gmm_hash_clusters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
