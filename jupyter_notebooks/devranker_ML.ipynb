{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data files\n",
    "import pandas as pd\n",
    "\n",
    "tensorflow_commits = pd.read_csv('C:/Users/aveli/Downloads/tensorflow.csv')\n",
    "vscode_commits=pd.read_csv('C:/Users/aveli/Downloads/vscode.csv')\n",
    "react_commits=pd.read_csv('C:/Users/aveli/Downloads/react-native.csv')\n",
    "\n",
    "total_commits=tensorflow_commits.append(vscode_commits, ignore_index=True)\n",
    "total_commits=total_commits.append(react_commits, ignore_index=True)\n",
    "                             \n",
    "\n",
    "#total_commits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use h2o4gpu if you have it installed\n",
    "\n",
    "# Add a flag with Default as False. Don't change this unless your kernel uses h2o4gpu.\n",
    "h2o4gpu_enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating various features for each modification\n",
    "\n",
    "# Total number of lines changed\n",
    "total_commits['total_changed'] = total_commits['lines_added'] + total_commits['lines_removed']\n",
    "\n",
    "# Fraction of lines changed per total numbe of lines in file\n",
    "# We need to account for the fact that new files added with have existing size as '0' and divide by '0' is indeterminate\n",
    "total_commits['size'].loc[total_commits['size'] == 0] = total_commits['total_changed']\n",
    "total_commits['ratio_changed'] = total_commits['total_changed'] / total_commits['size']\n",
    "\n",
    "# Need to weigh the complexity by quantum of change. \n",
    "total_commits['rated_complexity'] = total_commits['ratio_changed'] * total_commits['complexity'] * total_commits['total_changed']\n",
    "\n",
    "# weighing the dmm params by the total changed lines\n",
    "total_commits['total_dmm_size'] = total_commits['total_changed'] * total_commits['dmm_unit_size']\n",
    "total_commits['total_dmm_unit_complexity'] = total_commits['total_changed'] * total_commits['dmm_unit_complexity']\n",
    "total_commits['total_dmm_unit_interfacing'] = total_commits['total_changed'] * total_commits['dmm_unit_interfacing']\n",
    "\n",
    "# We picked the sqrt of no_of_mod_files to reduce weightage of this feature\n",
    "total_commits['scaled_rated_complexity']=total_commits['rated_complexity'] * (total_commits['no._of_mod_files'] ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data. ML requires the data to be converted to numericals\n",
    "#ml_commits = total_commits[['hash','Author', 'no._of_mod_files', 'dmm_unit_size',\n",
    "#       'dmm_unit_complexity', 'dmm_unit_interfacing', 'complexity', 'functions', 'lines_added', 'lines_removed', \n",
    "#       'tokens', 'type']]\n",
    "\n",
    "ml_commits = total_commits[['hash','Author','total_changed','rated_complexity',\n",
    "                            'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity']]\n",
    "\n",
    "# Resetting the frame's index. It is required to retain the integrity of the frame\n",
    "ml_commits = ml_commits.reset_index().drop(columns = 'index')\n",
    "\n",
    "# Temporarily dropping text columns for numeric processing\n",
    "ml_commits_noText = ml_commits.drop(columns = ['Author','hash'])\n",
    "\n",
    "# Explicitely converting fields to numeric types and filling the NaNs with zeros\n",
    "ml_commits_numeric = ml_commits_noText.apply(pd.to_numeric,errors ='coerce').fillna(0)\n",
    "\n",
    "# Adding the Author column back to create a 'total' data frame\n",
    "ml_commits_total = ml_commits_numeric.copy()\n",
    "ml_commits_total['Author'] = ml_commits['Author']\n",
    "ml_commits_total['hash'] = ml_commits['hash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers. \n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# Calculate z_scores (and if zscore is greater than '3', then its an outlier) and collect normal subset.\n",
    "ml_commits_nout = ml_commits_total[(np.abs(stats.zscore(ml_commits_total.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "ml_commits_nout.to_csv('C:/Users/aveli/Downloads/totalCommits_nout.csv')\n",
    "\n",
    "# Collect outliers\n",
    "ml_commits_out = ml_commits_total[~(np.abs(stats.zscore(ml_commits_total.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "ml_commits_out.to_csv('C:/Users/aveli/Downloads/totalCommits_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying scaler to regular data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "if (h2o4gpu_enabled == True):\n",
    "    from h2o4gpu.preprocessing import MinMaxScaler\n",
    "else:\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "# Use minMax scaler since this does not distort\n",
    "# https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02\n",
    "scaler = MinMaxScaler()\n",
    "ml_commits_nout_numeric = ml_commits_nout.drop(columns = ['Author','hash'])\n",
    "data_scaled = scaler.fit_transform(ml_commits_nout_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and figure out how many clusters are optimum\n",
    "# Plot the inertia curve to find the 'elbow'\n",
    "\n",
    "''' Realised that h2o4GPU has not implemented \"Intertia\". Hence we will have to use regular Kmeans library for intertia. \n",
    "Please ignore this cell entirely.\n",
    "\n",
    "\n",
    "import h2o4gpu\n",
    "\n",
    "SSE = []\n",
    "\n",
    "for cluster in range(1,20):\n",
    "    kmeans_ss = h2o4gpu.KMeans(n_gpus=1, n_clusters = cluster, init='k-means++', random_state = 42, backend=h2o4gpu)\n",
    "    %time kmeans_ss.fit(data_scaled)\n",
    "    SSE.append(kmeans_ss.inertia_)\n",
    "    print(kmeans_ss.cluster_centers_)\n",
    "    print(len(SSE))\n",
    "    print(str(SSE))\n",
    "    \n",
    "    # converting the results into a dataframe and plotting them\n",
    "frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try and figure out how many clusters are optimum\n",
    "# Plot the inertia curve to find the 'elbow'\n",
    "# You don't need to run this for every iteration. Just uncomment and run whenever you need to\n",
    "\n",
    "# Realised that h2o4GPU has not implemented \"Intertia\". Hence we will have to use regular Kmeans library for intertia.\n",
    "# http://docs.h2o.ai/h2o4gpu/latest-stable/h2o4gpu-py-docs/html/_modules/h2o4gpu/solvers/kmeans.html\n",
    "\n",
    "# Set this to 'True' if you want to plot graph to find elbow\n",
    "find_elbow = False\n",
    "\n",
    "if (find_elbow == True): \n",
    "    from sklearn.cluster import KMeans\n",
    "\n",
    "    SSE = []\n",
    "\n",
    "    for cluster in range(1,20):\n",
    "        %time kmeans_ss = KMeans(n_clusters = cluster, init='k-means++', random_state = 42)\n",
    "        kmeans_ss.fit(data_scaled)\n",
    "        SSE.append(kmeans_ss.inertia_)\n",
    "\n",
    "    # converting the results into a dataframe and plotting them\n",
    "    frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the KMeans object based on number discovered above\n",
    "k = 5\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Initializing the Gaussian mixture model \n",
    "mix = GaussianMixture(n_components=k,random_state=42)\n",
    "\n",
    "# Learning the Gaussian mixture model from data   \n",
    "mix.fit(data_scaled)\n",
    "\n",
    "# Saving the parameters of Gaussian mixture model in a file\n",
    "import pickle\n",
    "vfilename = 'C:/Users/aveli/Downloads/gmm_cluster_model.sav'\n",
    "pickle.dump(mix, open(vfilename, 'wb'))\n",
    "\n",
    "# Predicting the cluster labels of the data for the Gaussian mixture model\n",
    "cluster_frame = pd.DataFrame(data_scaled)\n",
    "gmm_hash_clusters = mix.predict(cluster_frame)\n",
    "\n",
    "# Collecting the mean of the Gaussian mixture model in 'gmmcentroids'\n",
    "gmm_centroids = mix.means_\n",
    "gmm_covariances = mix.covariances_\n",
    "combinedCentroids = gmm_centroids[gmm_hash_clusters].sum(axis=1)\n",
    "\n",
    "# Converting the input data series into pan\n",
    "ml_commits_nout['Cluster'] = gmm_hash_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Covariance matrix heatmap visualization as a sanity check\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# We need as many as cols as we have features\n",
    "cols=['1','2','3','4','5','6']\n",
    "\n",
    "# visualization for cluster number\n",
    "cluster_number = 2\n",
    "hm = sns.heatmap(gmm_covariances[cluster_number,:,:],\n",
    "                 cbar=True,\n",
    "                 annot=True,\n",
    "                 square=True,\n",
    "                 fmt='.5f',\n",
    "                 annot_kws={'size': 12},\n",
    "                 yticklabels=cols,\n",
    "                 xticklabels=cols)\n",
    "\n",
    "plt.title('Covariance matrix of the cluster showing correlation coefficients')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the values of inverted scaling of centroids for sanity\n",
    "real_centroids = scaler.inverse_transform(gmm_centroids)\n",
    "\n",
    "# Write these to dataframe\n",
    "real_centroids_dataFrame = pd.DataFrame(real_centroids, columns=['total_changed','rated_complexity',\n",
    "                            'total_dmm_unit_complexity','total_dmm_size','total_dmm_unit_interfacing', 'scaled_rated_complexity'])\n",
    "\n",
    "# Add a cloumn for summing all coloumns\n",
    "real_centroids_dataFrame['Sum_centroids'] = real_centroids_dataFrame.sum(axis = 1)\n",
    "\n",
    "#You can write it out as csv if required.\n",
    "real_centroids_dataFrame.to_csv('C:/Users/aveli/Downloads/totalCommits_centroids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "from xgboost import XGBClassifier\n",
    "if (h2o4gpu_enabled == True):\n",
    "    import h2o4gpu as sklearn\n",
    "else:\n",
    "    import sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Remove text fields before numeric manipulations\n",
    "ml_commits_nout_numeric_xg = ml_commits_nout.drop(columns=['Author','hash'])\n",
    "\n",
    "# Prepare the 'X' and 'Y' for the model\n",
    "X_ml_commits_nout_numeric_xg = ml_commits_nout_numeric_xg.drop(columns = ['Cluster'])\n",
    "Y_ml_commits_nout_numeric_xg = ml_commits_nout_numeric_xg['Cluster']\n",
    "\n",
    "# Split the data for 'Training' and 'Testing' datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ml_commits_nout_numeric_xg, Y_ml_commits_nout_numeric_xg, random_state=7)\n",
    "\n",
    "# Instantiate the xgboost model\n",
    "xgboost_model = XGBClassifier()\n",
    "\n",
    "# Training the xgboost classifier\n",
    "xgboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the class labels of test data for xgboost classifier\n",
    "y_pred = xgboost_model.predict(X_test)\n",
    " \n",
    "# check accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('accuracy', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (h2o4gpu_enabled == True):\n",
    "    from h2o4gpu import metrics\n",
    "else:\n",
    "    from sklearn import metrics\n",
    "\n",
    "metrics.silhouette_score(data_scaled, gmm_hash_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle the XGBoost model\n",
    "import pickle\n",
    "\n",
    "# Saving the trained model in a file\n",
    "filename = 'C:/Users/aveli/Downloads/finalized_model.sav'\n",
    "pickle.dump(xgboost_model, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
