{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "import git\n",
    "import os\n",
    "from git import Repo\n",
    "import sys\n",
    "\n",
    "def create_components(git_url,esurl,localdir):\n",
    "    # if no url supplied for Elastic, assume the localhost\n",
    "    if esurl =='':\n",
    "        try:\n",
    "            es = elasticsearch.Elasticsearch(['http://localhost:9200/'])\n",
    "        except:\n",
    "            print('Elasticsearch not running at localhost:9200')\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        try:\n",
    "            es = elasticsearch.Elasticsearch([esurl])\n",
    "        except:\n",
    "            print('Elasticsearch not running at the given URL. For default localhost, do not provide the argument')\n",
    "            sys.exit(1)\n",
    "        \n",
    "    # Get the default commit index name\n",
    "    es_index_raw = str.split(git_url,'/')[-1].split('.')[0]+'_'+'index'\n",
    "    # Get the default blame index name\n",
    "    es_blame_index_raw = str.split(git_url,'/')[-1].split('.')[0]+'_'+'blame'+'_'+'index'\n",
    "    es_index = es_index_raw.lower()\n",
    "    es_blame_index = es_blame_index_raw.lower()\n",
    "    # Create elasticsearch instance\n",
    "    \n",
    "    # If local Repo path is not supplied, create default path in '/tmp'\n",
    "    if localdir == '':\n",
    "        if sys.platform == 'linux':\n",
    "            local_dir ='/tmp/'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "        else:\n",
    "            local_dir ='C:\\\\Downloads'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "    else:\n",
    "        local_dir = localdir+'/'+str.split(git_url,'/')[-1].split('.')[0]\n",
    "    # Check if the local Repo already exists\n",
    "    if os.path.isdir(local_dir):\n",
    "        # Load the local Repo\n",
    "        try:\n",
    "            repo = git.Repo(local_dir)   \n",
    "        # Get the latest commit object in the local Repo\n",
    "            local_commit = repo.commit()     \n",
    "        except:\n",
    "            print('No valid Repo found at the location. If unsure, remove the directory and try without local dir argument')\n",
    "            sys.exit(1)\n",
    "                       # latest local commit \n",
    "        \n",
    "        # Get the latest commit object in the remote Repo\n",
    "        remote = git.remote.Remote(repo, 'origin')      # remote repo\n",
    "        info = remote.fetch()[0]                        # fetch changes\n",
    "        remote_commit = info.commit  \n",
    "        \n",
    "        # If latest commit in local and remote differ refresh the local Repo\n",
    "        if (local_commit.hexsha == remote_commit.hexsha ):\n",
    "            print('No changes in the Repo...')\n",
    "        else:    \n",
    "            repo = git.Repo(local_dir) \n",
    "            o = repo.remotes.origin\n",
    "            o.pull()\n",
    "            # Analyse and store additional commit data\n",
    "            store_commit_data(local_dir,es,es_index,es_blame_index,local_commit.hexsha,remote_commit.hexsha )\n",
    "    else:\n",
    "        # If no local Repo exists, clone the Repo\n",
    "        try:\n",
    "            if sys.platform == 'linux':\n",
    "                git.Git(localdir).clone(git_url)\n",
    "            else:\n",
    "                git.Git('C:\\\\Downloads').clone(git_url)\n",
    "        except:\n",
    "            print('Not able to clone the Repo. If there is a non Git directory with the  same name, delete it and then try')\n",
    "            sys.exit(1)\n",
    "        # Delete the elastic indices, if exist\n",
    "        es.indices.delete(index=es_index, ignore=[400, 404])\n",
    "        es.indices.delete(index=es_blame_index, ignore=[400, 404])\n",
    "        # Create new elastic indices\n",
    "        es.indices.create(es_index)\n",
    "        es.indices.create(es_blame_index)\n",
    "        # Call the function to store the necessary commit data\n",
    "        store_commit_data(local_dir,es,es_index,es_blame_index,'None','None')\n",
    "\n",
    "    return es,es_index,es_blame_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process individual mod in commit\n",
    "\n",
    "def process_mod():\n",
    "    #Building Commit tuples\n",
    "            commit_data = {'hash':commit.hash,'Author':commit.author.name,'Email':commit.author.email,\n",
    "                               'message':commit.msg,'authored_date':commit.author_date,\n",
    "                               'Committer':commit.committer.name,'committed_date':commit.committer_date,\n",
    "                               'no._of_branches':len(commit.branches),'merge_commit?':commit.merge,\n",
    "                               'no._of_mod_files':len(commit.modifications),'dmm_unit_size':commit.dmm_unit_size,\n",
    "                               'dmm_unit_complexity':commit.dmm_unit_complexity,'dmm_unit_interfacing':commit.dmm_unit_interfacing,\n",
    "                               'file_name':mod.filename, 'file_path':mod.new_path, 'complexity': mod.complexity, 'functions': len(mod.methods),\n",
    "                               'lines_added':mod.added,'lines_removed': mod.removed,'loc':mod.nloc,'size': 0 if mod.source_code is None else len(mod.source_code.splitlines()),'tokens':mod.token_count\n",
    "                                       \n",
    "                                        }\n",
    "            # loading each commit tuple into the list\n",
    "            doclist.append(commit_data)\n",
    "            # Each modification object contains diff_parsed tag with \"added\" and \"deleted\" properties. \n",
    "            alines = mod.diff_parsed['added']\n",
    "            # \"added\" property is a tuple list with line number and actual line text. List of text lines is extracted\n",
    "            addedlines = [x[1] if len(alines)>0 else 'None' for x in alines ]\n",
    "                    \n",
    "            # Each modification object contains diff_parsed tag with \"added\" and \"deleted\" properties. \n",
    "            blines = mod.diff_parsed['deleted']\n",
    "            # \"deleted\" property is a tuple list with line number and actual line text. List of text lines is extracted\n",
    "            deletedlines = [x[1] if len(blines)>0 else 'None' for x in blines]\n",
    "            count = 0\n",
    "                    \n",
    "            # Absolute path of the file in the cloned repo. This is required to validate that the file has not been deleted in the subsequent commits\n",
    "            newfilepath = local_dir+'/'+str(mod.new_path)\n",
    "            # For bug fix commits, retrieving the blame data. Using Regex on Commit messages to identify bug fix commits\n",
    "            if len(re.findall(r\"\\bbug\\b|\\bBug\\b|\\bFix\\b|\\bfix\\b\",commit.msg))>0 and os.path.isfile(newfilepath):# & len(addedlines)>0:\n",
    "                # Running Git Blame on each added line.\n",
    "                for eachline in addedlines:\n",
    "                    repo_blame = repo.blame(commit.hash,mod.new_path,eachline)\n",
    "                    # Git Blame of a line can produce multiple records with each record representing a past modification\n",
    "                    for blame_record in repo_blame:\n",
    "                        # Git Blame produces duplicate records (Don't know why). Attempt to ignore duplicated by comparting the current record with the previous record\n",
    "                        # Also Git Blame produces record of the same commit hash, which can be ignored\n",
    "                        prev_record = ''\n",
    "                        if str(commit.hash) !=str(blame_record[0]) and (str(blame_record[0]) != prev_record):\n",
    "                        # Building Blame tuple for each Blame record\n",
    "                            blame_doc = {'orig_hash':commit.hash,'blame_hash':str(blame_record[0]),\n",
    "                                            'file':mod.new_path}    \n",
    "                            # Loading blame data into the list\n",
    "                            blamelist.append(blame_doc)\n",
    "                            prev_record = blame_record[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def process_commit(commit, repo, doclist, blamelist, local_dir):\n",
    "    \n",
    "    #Threads to Process modificationa. Threads inherit variables from parent - repo, doclist etc\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "        executor.map(process_mod, commit.modifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import pandas as pd\n",
    "import git\n",
    "import re\n",
    "from pydriller import RepositoryMining\n",
    "from elasticsearch import helpers\n",
    "from git import Repo\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "def store_commit_data(local_dir,es,es_index,es_blame_index,local_commit,remote_commit):\n",
    "    \n",
    "    repo = Repo(local_dir)\n",
    "    # Creating empty lists for carrying commit data\n",
    "    doclist = []\n",
    "    blamelist =[]\n",
    "    # Init multiprocessing pool\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    \n",
    "    # If the Repo has just been cloned, the program will traverse the whole Repo\n",
    "    if(local_commit == 'None'):\n",
    "        # using PyDriller's API.\n",
    "        #for commit in RepositoryMining(local_dir).traverse_commits():\n",
    "        #       process_commit(commit, repo, doclist, blamelist)\n",
    "        [pool.apply_async(process_commit(commit, repo, doclist, blamelist, local_dir)) for commit in RepositoryMining(local_dir).traverse_commits()]\n",
    "        \n",
    "    else:\n",
    "        #if Repo has been refreshed, only delta commits are processed\n",
    "        #for commit in RepositoryMining(local_dir,from_commit = local_commit, to_commit = remote_commit ).traverse_commits():\n",
    "        #    process_commit(commit, repo, doclist, blamelist)\n",
    "        [pool.apply_async(process_commit(commit, repo, doclist, blamelist, local_dir)) for commit in RepositoryMining(local_dir).traverse_commits()]\n",
    "\n",
    "    # Close Multiprocessing pool\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    #pool.clear()\n",
    "    \n",
    "    # using elasticsearch.py's helper tools to bulk load into elasticsearch's commit index           \n",
    "    helpers.bulk(es,doclist,index=es_index,doc_type ='commit_data',request_timeout = 2000)\n",
    "    # Since Git Blame produces duplicate data, getting only unique records\n",
    "    blamelist_fil = [i for n, i in enumerate(blamelist) if i not in blamelist[n + 1:]]\n",
    "    # using elasticsearch.py's helper tools to bulk load into elasticsearch's blame index\n",
    "    helpers.bulk(es,blamelist_fil,index=es_blame_index,doc_type ='blame',request_timeout = 2000)\n",
    "    # Very important to explicitly refresh the Elastic indices as they are not automatically done.\n",
    "    es.indices.refresh([es_blame_index,es_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kc\n",
      "/home/kc/cg_Repos\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Home_address = str(Path.home())\n",
    "print(Home_address)\n",
    "localdir = Home_address + '/cg_Repos'\n",
    "print(localdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling the base method to process and load the commit data. See \"create_components\" method for detail\n",
    "#localdir = '/home/vamsi'\n",
    "#p1 = create_components('https://github.com/R-Knowsys/elasticray.git','',localdir)\n",
    "#p2 = create_components('https://github.com/chaoss/grimoirelab.git','',localdir)\n",
    "#p3 = create_components('https://github.com/microsoft/vscode.git','','')\n",
    "p4 = create_components('https://github.com/tensorflow/tensorflow.git','',localdir)    \n",
    "#p5 = create_components('https://github.com/facebook/react-native.git','','')\n",
    "#p6 = create_components('https://github.com/kcramakrishna/Digital_Assistant_Client.git','',localdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import pandas as pd\n",
    "\n",
    "def get_user_data(es_instance,commit_index,blame_index):\n",
    "        # Assigning Elastic instance, Commit Elastic Index and Blame Elastic Index to variables\n",
    "        es = es_instance\n",
    "        es_ma_index = commit_index\n",
    "        es_bl_index = blame_index\n",
    "        # Using Elasticsearch DSL function to get the data of Commit index\n",
    "        blame_es_data = Search(using=es, index=es_bl_index)\n",
    "        # Loading data into a dictionary\n",
    "        blame_dict = [hit.to_dict() for hit in blame_es_data.scan()]\n",
    "        # Using Elasticsearch DSL function to get the data of Blame index\n",
    "        commit_es_data = Search(using=es, index=es_ma_index)\n",
    "        # Loading data into a dictionary\n",
    "        commit_dict = [hit.to_dict() for hit in commit_es_data.scan()]\n",
    "        # Creating pandas dataframe for commit data\n",
    "        commit_frame = pd.DataFrame(commit_dict)\n",
    "        # Creating pandas dataframe for blame data\n",
    "        blame_frame = pd.DataFrame(blame_dict)\n",
    "        # Getting the blame row count. If the frmae is empty, it means all the records are clean\n",
    "        blame_count = blame_frame.shape[0]\n",
    "        \n",
    "        if blame_count>0:\n",
    "            # Adding a column to Blame frame indicating that the row represents a Buggy commit\n",
    "            blame_frame['type'] = 'Buggy'\n",
    "            # Combining Commit frmae with Blame frame. An additional column called 'type' gets added to the Commit frame.\n",
    "            comb_frame = pd.merge(commit_frame,blame_frame,how='left',left_on = ['hash','file_path'],right_on = ['blame_hash','file'])\n",
    "        else:\n",
    "            # If the Blame frame is empty, no need to merge.\n",
    "            comb_frame=commit_frame\n",
    "        # When merging happnes and 'type' column gets added to the main Commit frame, The rows that are not part of Blame frame are filled with 'Nan'.\n",
    "        # Here, all the NaNs fro 'type' column are replaced with 'Clean' label.\n",
    "        # Effectively, Each commit file (one Commit can contain more than one file) is categorised as either Buggy or Clean.\n",
    "        comb_frame['type'] = comb_frame['type'].fillna('Clean')\n",
    "        \n",
    "        # Cleaning and retaining the required columns\n",
    "        comb_frame_refined = comb_frame[['hash', 'Author','Committer', 'Email', 'message',                               'committed_date', 'no._of_branches', 'merge_commit?',\n",
    "                                        'no._of_mod_files', 'dmm_unit_size', 'dmm_unit_complexity','dmm_unit_interfacing',\n",
    "                                        'file_path', 'complexity','functions', 'lines_added', 'lines_removed', 'size', 'tokens',\n",
    "                                        'type']]\n",
    "        # Commit hash raw value is very long. Cutting the value into first ten chars \n",
    "        # Assumption is that the first ten chars rednder necessary uniqueness. May need to revisit later\n",
    "        comb_frame_refined['hash'] = comb_frame_refined['hash'].str.slice(0,10)\n",
    "\n",
    "        # Changing the type from string to Data. Used Pacific time zone. Heard the pacific coast is beautiful\n",
    "        commit_frame['committed_date'] = commit_frame['committed_date'].astype('str').apply(lambda x: pd.to_datetime(x).tz_convert('US/Pacific'))\n",
    "        \n",
    "        # Sorting the frame by committes date\n",
    "        comb_frame_refined = comb_frame_refined.sort_values('committed_date', ascending=False)\n",
    "        \n",
    "        #------ Attempting to score developers with a simple/simplistic calculation -----/\n",
    "        \n",
    "        # Replacing all NaN fields with zero\n",
    "        comb_frame_refined = comb_frame_refined.fillna(0)\n",
    "        # Adding commit count (actually commit file count) for each other column\n",
    "        comb_frame_refined['commit_count'] = comb_frame_refined.groupby('Author').hash.transform('nunique')\n",
    "        # Adding up three dmm values creating a new column\n",
    "        comb_frame_refined['riskfreescore'] = comb_frame_refined['dmm_unit_size']+comb_frame_refined['dmm_unit_complexity']+comb_frame_refined['dmm_unit_interfacing']\n",
    "        # Adding up \"Lines Added\" and \"Lines Removed\" and creating a new column\n",
    "        comb_frame_refined['lineschanged'] = comb_frame_refined['lines_added']+comb_frame_refined['lines_removed']\n",
    "        # Simple calculation to come up with a score for each row\n",
    "        comb_frame_refined['score'] = comb_frame_refined.apply(lambda x:((x.riskfreescore)*10+(x.lineschanged)+(x.complexity)*10)/10,axis =1)\n",
    "        # Filtering out the necessary columns and creating a new frame\n",
    "        dev_score = comb_frame_refined[['Author','commit_count','score','type']]\n",
    "        # Score is scaled based on the type of commit file\n",
    "        dev_score['scaled_score'] = dev_score.apply(lambda x: x.score if x.type == 'Clean' else x.score/2,axis=1)\n",
    "        # Retaining the necessary columns\n",
    "        dev_score_refined = dev_score[['Author','commit_count','scaled_score']]\n",
    "        # Calculating the Average score for the Author. A simple arithmetic mean.\n",
    "        dev_score_refined['average_score'] = dev_score_refined.groupby('Author').scaled_score.transform('mean').round()\n",
    "        dev_score_final = dev_score_refined[['Author','commit_count','average_score']].drop_duplicates().sort_values('average_score', ascending=False).reset_index().drop(columns = 'index') \n",
    "        return dev_score_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import elasticsearch\n",
    "from elasticsearch_dsl import Search,Q\n",
    "import pandas as pd\n",
    "\n",
    "def get_latest_commits(es_instance,commit_index,blame_index):\n",
    "        # Assigning Elastic instance, Commit Elastic Index and Blame Elastic Index to variables\n",
    "        es = es_instance\n",
    "        es_ma_index = commit_index\n",
    "        es_bl_index = blame_index\n",
    "        # Using Elasticsearch DSL function to get the data of Commit index\n",
    "        blame_es_data = Search(using=es, index=es_bl_index)\n",
    "        # Loading data into a dictionary\n",
    "        blame_dict = [hit.to_dict() for hit in blame_es_data.scan()]\n",
    "        # Using Elasticsearch DSL function to get the data of Blame index\n",
    "        commit_es_data = Search(using=es, index=es_ma_index)\n",
    "        # Loading data into a dictionary\n",
    "        commit_dict = [hit.to_dict() for hit in commit_es_data.scan()]\n",
    "        # Creating pandas dataframe for commit data\n",
    "        commit_frame = pd.DataFrame(commit_dict)\n",
    "        # Creating pandas dataframe for blame data\n",
    "        blame_frame = pd.DataFrame(blame_dict)\n",
    "        # Getting the blame row count. If the frmae is empty, it means all the records are clean\n",
    "        blame_count = blame_frame.shape[0]\n",
    "        \n",
    "        if blame_count>0:\n",
    "            # Adding a column to Blame frame indicating that the row represents a Buggy commit\n",
    "            blame_frame['type'] = 'Buggy'\n",
    "            # Combining Commit frmae with Blame frame. An additional column called 'type' gets added to the Commit frame.\n",
    "            comb_frame = pd.merge(commit_frame,blame_frame,how='left',left_on = ['hash','file_path'],right_on = ['blame_hash','file'])\n",
    "        else:\n",
    "            # If the Blame frame is empty, no need to merge.\n",
    "            comb_frame=commit_frame\n",
    "        # When merging happnes and 'type' column gets added to the main Commit frame, The rows that are not part of Blame frame are filled with 'Nan'.\n",
    "        # Here, all the NaNs fro 'type' column are replaced with 'Clean' label.\n",
    "        # Effectively, Each commit file (one Commit can contain more than one file) is categorised as either Buggy or Clean.\n",
    "        comb_frame['type'] = comb_frame['type'].fillna('Clean')\n",
    "        \n",
    "        # Cleaning and retaining the required columns\n",
    "        comb_frame_refined = comb_frame[['hash', 'Author','Committer', 'Email', 'message',                               'committed_date', 'no._of_branches', 'merge_commit?',\n",
    "                                        'no._of_mod_files', 'dmm_unit_size', 'dmm_unit_complexity','dmm_unit_interfacing',\n",
    "                                        'file_path', 'complexity','functions', 'lines_added', 'lines_removed', 'size', 'tokens',\n",
    "                                        'type']]\n",
    "        # Commit hash raw value is very long. Cutting the value into first ten chars \n",
    "        # Assumption is that the first ten chars rednder necessary uniqueness. May need to revisit later\n",
    "        comb_frame_refined['hash'] = comb_frame_refined['hash'].str.slice(0,10)\n",
    "\n",
    "        # Changing the type from string to Data. Used Pacific time zone. Heard the pacific coast is beautiful\n",
    "        commit_frame['committed_date'] = commit_frame['committed_date'].astype('str').apply(lambda x: pd.to_datetime(x).tz_convert('US/Pacific'))\n",
    "        # Sorting the frame by committes date\n",
    "        comb_frame_refined = comb_frame_refined.sort_values('committed_date', ascending=False)\n",
    "        \n",
    "        return comb_frame_refined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/Projects/devranker-jupyter/devranking/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-00644f9984d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_latest_commits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#get_latest_commits(p6[0],p6[1],p6[2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-5b737a4dc67e>\u001b[0m in \u001b[0;36mget_latest_commits\u001b[0;34m(es_instance, commit_index, blame_index)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Here, all the NaNs fro 'type' column are replaced with 'Clean' label.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Effectively, Each commit file (one Commit can contain more than one file) is categorised as either Buggy or Clean.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mcomb_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomb_frame\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Clean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Cleaning and retaining the required columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/devranker-jupyter/devranking/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/devranker-jupyter/devranking/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'type'"
     ]
    }
   ],
   "source": [
    "get_latest_commits(p1[0],p1[1],p1[2])\n",
    "#get_latest_commits(p6[0],p6[1],p6[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following cells contain the code catering for K-Means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing data for few Git Repos. Look into the above method definitions for detail\n",
    "#p1 = create_components('https://github.com/R-Knowsys/elasticray.git','','')\n",
    "p2 = create_components('https://github.com/chaoss/grimoirelab.git','','')\n",
    "#p3 = create_components('https://github.com/microsoft/vscode.git','','')\n",
    "p4 = create_components('https://github.com/tensorflow/tensorflow.git','','')    \n",
    "#p5 = create_components('https://github.com/facebook/react-native.git','','')\n",
    "\n",
    "# Getting the commit data as pandas frames\n",
    "#p1_commits = get_latest_commits(p1[0],p1[1],p1[2])\n",
    "p2_commits = get_latest_commits(p2[0],p2[1],p2[2])\n",
    "p4_commits = get_latest_commits(p4[0],p4[1],p4[2])\n",
    "\n",
    "# Combining commit data from two Git Repos. Add as many Repos as you like\n",
    "total_commits = p2_commits.append(p4_commits)\n",
    "print(total_commits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data. ML requires the data to be converted to numericals\n",
    "ml_commits = total_commits[['Author', 'no._of_mod_files', 'dmm_unit_size',\n",
    "       'dmm_unit_complexity', 'dmm_unit_interfacing', 'complexity', 'functions', 'lines_added', 'lines_removed', \n",
    "       'tokens', 'type']]\n",
    "# Assign 15 to Clean row and 5 to buggy row\n",
    "ml_commits['type'] = ml_commits['type'].apply(lambda x:15 if x=='Clean' else 5)\n",
    "# Resetting the frame's index. It is required to retain the integrity of the frame\n",
    "ml_commits = ml_commits.reset_index().drop(columns = 'index')\n",
    "# Author column needs to be dropped before converting the all the fields into numeric types\n",
    "ml_commits_na = ml_commits.drop(columns = ['Author'])\n",
    "# Converting the fields to numeric types, filling the NaNs with zeros\n",
    "ml_commits_numeric =ml_commits_na.apply(pd.to_numeric,errors ='coerce').fillna(0)\n",
    "# Adding teh Author column back\n",
    "ml_commits_numeric['Author']=ml_commits['Author']\n",
    "ml_commits_numeric_na = ml_commits_numeric.drop(columns=['Author'])\n",
    "\n",
    "print(ml_commits_numeric.shape)\n",
    "print(ml_commits_numeric_na.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to remove outliers. May not be required\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "#df = ml_commits_numeric\n",
    "#df1 = df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n",
    "ml_commits_nout = ml_commits_numeric[(np.abs(stats.zscore(ml_commits_numeric.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "ml_commits_mode = ml_commits_nout.select_dtypes(exclude='object').mode()\n",
    "ml_commits_out = ml_commits_numeric[~(np.abs(stats.zscore(ml_commits_numeric.select_dtypes(exclude='object'))) < 3).all(axis=1)]\n",
    "#print(ml_commits_nout.shape)\n",
    "#print(ml_commits_numeric.shape)\n",
    "index = 0\n",
    "'''\n",
    "while index<ml_commits_mode.size-1:\n",
    "    \n",
    "    ml_commits_out.iloc[:,index] = ml_commits_mode.iloc[0,index]\n",
    "    index +=1\n",
    "'''\n",
    "#df3.iloc[:,0] = df2.iloc[0,0]\n",
    "ml_commits_all = ml_commits_nout.append(ml_commits_out).reset_index()\n",
    "ml_commits_fil = ml_commits_all[['Author','complexity','lines_added','type']]\n",
    "\n",
    "ml_commits_all_na = ml_commits_all.drop(columns = ['Author'])\n",
    "ml_commits_fil_na = ml_commits_fil.drop(columns = ['Author'])\n",
    "print(ml_commits_all_na.shape)\n",
    "print(ml_commits_all.shape)\n",
    "ml_commits_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimentionality Reduction before training with Kmeans.\n",
    "# Dimentionality Reduction is the technique used to identify the most influential features for machine learning\n",
    "# The input data in this case contains just ten features. Hence, this may not be required\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assigning the input frame to a simpler variable\n",
    "df = ml_commits_numeric_na\n",
    "# Creating the instance of Standard Scaler\n",
    "scaler = StandardScaler()\n",
    "# Transforming the input data using Standard Scaler\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "# Creating the instance of PCA. PCA is a popular tool for the purpose of Dimentionality Reduction\n",
    "pca = PCA()\n",
    "# Processing the scaled data using PCA\n",
    "pca.fit(data_scaled)\n",
    "# Printing the variance ratio for each feature. FYI, each column of the frame represents a feature\n",
    "pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the variance ratios for each feature\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (12,8))\n",
    "plt.plot(range(1,11),pca.explained_variance_ratio_.cumsum(),marker = 'o',linestyle = '--')\n",
    "plt.title('Explained Variance by Component')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulatve Explained Variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above plot, it is clear that first three features have major impact on variance. And the last five have no imapct at all.\n",
    "# Hence, we can try Kmeans with first three features\n",
    "pca = PCA(n_components = 3)\n",
    "pca.fit(data_scaled)\n",
    "scores_pca = pca.transform(data_scaled)\n",
    "print(scores_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inertia calculation provides insights into required number of Clusters\n",
    "\n",
    "SSE = []\n",
    "\n",
    "for cluster in range(1,20):\n",
    "    kmeans_pca = KMeans(n_clusters = cluster, init='k-means++',random_state = 42)\n",
    "    kmeans_pca.fit(scores_pca)\n",
    "    SSE.append(kmeans_pca.inertia_)\n",
    "\n",
    "    # converting the results into a dataframe and plotting them\n",
    "frame = pd.DataFrame({'Cluster':range(1,20), 'SSE':SSE})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(frame['Cluster'], frame['SSE'], marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "df = ml_commits_numeric_na\n",
    "scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "\n",
    "data_normalized = normalizer.fit_transform(df)\n",
    "'''\n",
    "pd.DataFrame(data_scaled).describe()\n",
    "kmeans = KMeans(n_clusters=2, init='k-means++')\n",
    "kmeans.fit(data_scaled)\n",
    "kmeans.inertia_\n",
    "'''\n",
    "kmeans = KMeans(n_clusters = 5, init='random',max_iter = 10)\n",
    "s = kmeans.fit(data_scaled)\n",
    "p = kmeans.fit(data_normalized)\n",
    "q = kmeans.fit(scores_pca)\n",
    "s_centroids = s.cluster_centers_\n",
    "p_centroids = p.cluster_centers_\n",
    "q_centroids = q.cluster_centers_\n",
    "#pred = kmeans.predict(data_scaled)\n",
    "print(s.labels_.astype(float))\n",
    "print(p.labels_.astype(float))\n",
    "print(q.labels_.astype(float))\n",
    "print(s_centroids)\n",
    "print(p_centroids)\n",
    "print(q_centroids)\n",
    "print(data_scaled.shape)\n",
    "print(data_normalized.shape)\n",
    "print(scores_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the input frame with PCA processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "df = ml_commits_numeric_na\n",
    "scaler = StandardScaler()\n",
    "normalizer = Normalizer()\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "\n",
    "\n",
    "pca = PCA(n_components = 3)\n",
    "pca.fit(data_scaled)\n",
    "scores_pca = pca.transform(data_scaled)\n",
    "kmeans = KMeans(n_clusters = 5, init='random',max_iter = 10)\n",
    "q = kmeans.fit(scores_pca)\n",
    "\n",
    "plt.scatter(scores_pca[:,0], scores_pca[:,2], s=50, alpha=0.5)\n",
    "plt.scatter(q_centroids[:, 0], q_centroids[:, 2], c='red', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the normalized input frame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "df = ml_commits_numeric_na\n",
    "\n",
    "normalizer = Normalizer()\n",
    "data_normalized = normalizer.fit_transform(df)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 5, init='random',max_iter = 10)\n",
    "p = kmeans.fit(data_normalized)\n",
    "\n",
    "\n",
    "plt.scatter(data_normalized[:,0], data_normalized[:,1], s=50, alpha=0.5)\n",
    "plt.scatter(p_centroids[:, 0], p_centroids[:, 1], c='red', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the scaled input frame\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "df = ml_commits_numeric_na\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data_scaled = scaler.fit_transform(df)\n",
    "\n",
    "kmeans = KMeans(n_clusters = 5, init='random',max_iter = 10)\n",
    "s = kmeans.fit(data_scaled)\n",
    "\n",
    "plt.scatter(data_scaled[:,0], data_scaled[:,1], s=50, alpha=0.5)\n",
    "plt.scatter(s_centroids[:, 0], s_centroids[:,1], c='red', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to try out various input models\n",
    "# Includes measurement of quality of Clustering through silhouette_scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# X can be assigned to any input frame.\n",
    "X = data_normalized\n",
    "# If you want to try out Scaled frame, replace the above line with the following\n",
    "# X = data_scaled\n",
    "# For PCA frame, replace with the following line\n",
    "# X = scores_pca\n",
    "range_n_clusters = [2, 3, 4, 5, 6,8]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10,max_iter = 50,init = 'random')\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values =  sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    # In the below line, two features, X[:,4] and X[:,6] are selected for plotting\n",
    "    # Those features can be replaced with any other two features.\n",
    "    # Keep in mind that, the frame contains ten features, from X[:,0] to X[:,9]\n",
    "    # In the case of PCA, there are only three features selected after the dimentionality reduction.\n",
    "    # Hence, if the X is assigned to scores_pca, you can select from X[:,0] to X[:2]\n",
    "    ax2.scatter(X[:, 4], X[:, 6], marker='.', s=150, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    # The centers should have same positions as features. \n",
    "    ax2.scatter(centers[:,4], centers[:,6], marker='o',\n",
    "                c=\"red\", alpha=1, s=300, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[4], c[6], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to try out various input models\n",
    "# This is same as the above except that the input is limited to just two features\n",
    "# The puprose here is to understand how cluster behaves if we have same number of training features as plotted\n",
    "# Includes measurement of quality of Clustering through silhouette_scores\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "print(__doc__)\n",
    "\n",
    "# X can be assigned to any input frame.\n",
    "X = data_normalized[:,[3,5]]\n",
    "# If you want to try out Scaled frame, replace the above line with the following\n",
    "# X = data_scaled[:,[3,5]]\n",
    "# For PCA frame, replace with the following line\n",
    "# X = scores_pca[:,[0,1]]\n",
    "range_n_clusters = [2, 3, 4, 5, 6,8]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10,max_iter = 50,init = 'random')\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values =  sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    # Here, we can only plot X[:, 0], X[:, 1] as the training was done on just two features\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=150, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    # The centers should have same positions as features. \n",
    "    ax2.scatter(centers[:,0], centers[:,1], marker='o',\n",
    "                c=\"red\", alpha=1, s=300, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Predicting the Cluster for individual Authors\n",
    "kmeans = KMeans(n_clusters = 5, init='random',max_iter = 10)\n",
    "# Using data_scaled. This can be replaced with data_normalized or scores_pca\n",
    "s = kmeans.fit(data_scaled)\n",
    "# Converting the input data series into pandas Dataframe\n",
    "cluster_frame = pd.DataFrame(data_scaled)\n",
    "# Adding Author column to the frame. Since the frame indices match, we can use the base input columns\n",
    "cluster_frame['Author'] = ml_commits_numeric['Author']\n",
    "# Calculating mean of each column values grouped by the Author\n",
    "clust_auth_frame = cluster_frame.groupby(['Author'],as_index=True).mean()#.reset_index()#.drop(columns = 'index')\n",
    "\n",
    "# predicting the cluster for each author. Keep in mind that the above step creates Authors as index of the input frame\n",
    "auth_pred_scores = kmeans.predict(clust_auth_frame)\n",
    "\n",
    "# Adding the predicted cluster as a column\n",
    "clust_auth_frame['cluster'] = auth_pred_scores\n",
    "\n",
    "clust_auth_frame[['cluster']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "data_scaled = scaler.fit_transform(ml_commits_all_na)\n",
    "num_cols = len(scores_pca[0,:])\n",
    "for i in range(num_cols):\n",
    "    col = scores_pca[:,i]\n",
    "    col_stats = ss.describe(col)\n",
    "    print(col_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "devranking",
   "language": "python",
   "name": "devranking"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
